{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloudvolume import CloudVolume\n",
    "from meshparty import skeletonize, trimesh_io\n",
    "from caveclient import CAVEclient\n",
    "import trimesh\n",
    "import numpy as np\n",
    "import datetime\n",
    "import networkx as nx\n",
    "from scipy.sparse import identity\n",
    "from scipy.spatial import distance_matrix\n",
    "import scipy \n",
    "from tqdm import tqdm\n",
    "# import aws\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pyembree\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.spatial as spatial\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "orphans = pd.read_csv(\"/Users/sheeltanna/Desktop/AGT_REPO/campfire/GT_30_Orphans_Spring_2023 - Sheet1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_array(x):\n",
    "    res = list(map(str.strip, x.split('; ')))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "orphans['endpoints'] = orphans['endpoints'].map(lambda x: list(map(str.strip, x.split('; '))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert from string list to 2-d array\n",
    "def convert_to_array(row):\n",
    "    count = 0\n",
    "    result = []\n",
    "    for endpoint in row[\"endpoints\"]:\n",
    "        # endpoint = tuple(map(int, endpoint.split(', ')))\n",
    "        endpoint = eval(endpoint)\n",
    "        # print()\n",
    "        # print(endpoint)\n",
    "        # print(type(endpoint))\n",
    "        if(count == 0):\n",
    "            result = np.array(endpoint)\n",
    "            count = count + 1\n",
    "        else:\n",
    "            result = np.vstack((result, np.array(endpoint)))\n",
    "            count = count + 1\n",
    "            #result = np.concatenate(result, list(tuple))\n",
    "    #check if there was only 1 point, convert to 2-d array:\n",
    "    # if(type(result) == list):\n",
    "        \n",
    "    if(count == 1 and result.size != 0):\n",
    "        result = result.reshape(1,3)\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "orphans[\"real_endpoints\"] = orphans.apply(convert_to_array, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [[402547, 228831, 23991], [402958, 229220, 235...\n",
       "1     [[401602, 224623, 23991], [405273, 226312, 236...\n",
       "2     [[403332, 227545, 24398], [403566, 227614, 244...\n",
       "3     [[77262, 113088, 20442], [79178, 109113, 20400...\n",
       "4     [[100423, 143992, 21653], [100583, 144194, 217...\n",
       "5      [[71894, 146181, 20442], [71498, 146290, 20403]]\n",
       "6       [[77575, 107807, 21133], [77000, 99565, 21215]]\n",
       "7     [[79785, 135709, 21133], [84176, 127514, 21582...\n",
       "8       [[81198, 106660, 21133], [88834, 99501, 20479]]\n",
       "9      [[75409, 116393, 21135], [74841, 116076, 21217]]\n",
       "10    [[351150, 143681, 15176], [348147, 144970, 148...\n",
       "11    [[344600, 142349, 16939], [344592, 142335, 169...\n",
       "12    [[366719, 138890, 16417], [367885, 138975, 165...\n",
       "13    [[353411, 147608, 15177], [352173, 148914, 148...\n",
       "14    [[346497, 82778, 27090], [339051, 74578, 26769...\n",
       "15     [[351839, 91058, 27284], [349567, 97065, 26976]]\n",
       "16    [[357525, 81584, 25828], [358640, 81180, 25734...\n",
       "17     [[347271, 87374, 25828], [347862, 89018, 24930]]\n",
       "18    [[111700, 241689, 22797], [110188, 241117, 228...\n",
       "19    [[116186, 247468, 23275], [114619, 246629, 235...\n",
       "20    [[103037, 175591, 18434], [99978, 172371, 1883...\n",
       "21    [[313505, 161103, 18580], [314477, 162881, 187...\n",
       "22     [[307899, 98712, 19067], [309700, 87653, 19669]]\n",
       "23    [[286807, 94503, 18046], [291939, 86875, 18287...\n",
       "24    [[104063, 174940, 18432], [104232, 162719, 200...\n",
       "25    [[299555, 102903, 17377], [299986, 100770, 172...\n",
       "26    [[280639, 88256, 18965], [280377, 87163, 19077...\n",
       "27     [[84884, 179330, 16386], [84527, 179768, 16234]]\n",
       "28    [[125626, 172825, 17140], [125668, 172840, 171...\n",
       "29    [[299386, 144111, 17853], [308152, 142095, 172...\n",
       "Name: real_endpoints, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orphans[\"real_endpoints\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TIP FINDER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_process_mesh(root_id):\n",
    "    datastack_name = \"minnie65_phase3_v1\"\n",
    "    client = CAVEclient(datastack_name)\n",
    "    vol = CloudVolume(\n",
    "        client.info.segmentation_source(),\n",
    "        use_https=True,\n",
    "        progress=False,\n",
    "        bounded=False,\n",
    "        fill_missing=True,\n",
    "        secrets={\"token\": client.auth.token}\n",
    "    )\n",
    "    print(\"Downloading Mesh\")\n",
    "    mesh = vol.mesh.get(str(root_id))[root_id]\n",
    "    mesh_obj = trimesh.Trimesh(np.divide(mesh.vertices, np.array([1,1,1])), mesh.faces)\n",
    "    print(\"Vertices: \", mesh.vertices.shape[0])\n",
    "\n",
    "    if mesh_obj.volume > 4000000000000:\n",
    "        print(\"TOO BIG, SKIPPING\")\n",
    "        #queue_url_endpoints = sqs.get_or_create_queue(\"root_ids_functional_dlqueue\")\n",
    "\n",
    "        #entries=sqs.construct_rootid_entries([root_id])\n",
    "\n",
    "        #sqs.send_batch(queue_url_endpoints, entries)\n",
    "\n",
    "        return None\n",
    "    trimesh.repair.fix_normals(mesh_obj)\n",
    "    mesh_obj.fill_holes()\n",
    "\n",
    "    return mesh_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soma(soma_id:str):\n",
    "    cave_client = CAVEclient('minnie65_phase3_v1')\n",
    "    soma = cave_client.materialize.query_table(\n",
    "        \"nucleus_neuron_svm\",\n",
    "        filter_equal_dict={'id':soma_id}\n",
    "    )\n",
    "    return soma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mesh_ccs(mesh_obj):\n",
    "    print(\"Processing CC's\")\n",
    "    ccs_graph = trimesh.graph.connected_components(mesh_obj.edges)\n",
    "    ccs_len = [len(c) for c in ccs_graph]\n",
    "\n",
    "    # Subselect the parts of the mesh that are not inside one another \n",
    "    # the other components are an artifact of the soma seg and small unfilled sections\n",
    "    largest_component = ccs_graph[np.argmax(ccs_len)]\n",
    "    largest_component_remap = np.arange(ccs_graph[np.argmax(ccs_len)].shape[0])\n",
    "    face_dict = {largest_component[i]:largest_component_remap[i] for i in range(largest_component.shape[0])}\n",
    "\n",
    "    new_faces_mask = np.isin(mesh_obj.faces, list(face_dict.keys()))\n",
    "    new_faces_mask = new_faces_mask[:, 0]*new_faces_mask[:, 1]*new_faces_mask[:, 2]\n",
    "\n",
    "    new_faces = np.vectorize(face_dict.get)(mesh_obj.faces[new_faces_mask])\n",
    "    new_faces = new_faces[new_faces[:, 0] != None]\n",
    "    largest_component_mesh = trimesh.Trimesh(mesh_obj.vertices[largest_component], new_faces)\n",
    "\n",
    "    all_ids = set(largest_component)\n",
    "    encapsulated_ids = []\n",
    "\n",
    "    for i in range(1, len(ccs_graph)):\n",
    "        n_con = largest_component_mesh.contains(mesh_obj.vertices[ccs_graph[i]])\n",
    "        if np.sum(n_con) / n_con.shape[0] == 0 and n_con.shape[0] > 50:\n",
    "            all_ids.update(ccs_graph[i])\n",
    "        else:\n",
    "            if len(ccs_graph[i]) < 1000:\n",
    "                encapsulated_ids.append((np.mean(mesh_obj.vertices[ccs_graph[i]], axis=0)/[4,4,40], len(ccs_graph[i])))\n",
    "            \n",
    "    all_component = np.array(list(ccs_graph[np.argmax(ccs_len)]))\n",
    "    all_component_remap = np.arange(all_component.shape[0])\n",
    "    face_dict = {all_component[i]:all_component_remap[i] for i in range(all_component.shape[0])}\n",
    "    new_faces_mask = np.isin(mesh_obj.faces, list(face_dict.keys()))\n",
    "    new_faces_mask = new_faces_mask[:, 0]*new_faces_mask[:, 1]*new_faces_mask[:, 2]\n",
    "\n",
    "    new_faces = np.vectorize(face_dict.get)(mesh_obj.faces[new_faces_mask])\n",
    "    new_faces[new_faces[:, 0] != None]\n",
    "    \n",
    "    largest_component_mesh = trimesh.Trimesh(mesh_obj.vertices[all_component], new_faces)\n",
    "    \n",
    "    mesh_obj = largest_component_mesh\n",
    "    return mesh_obj, encapsulated_ids, np.max(ccs_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_defects(mesh_obj, a=.75):\n",
    "    bad_edges = trimesh.grouping.group_rows(\n",
    "        mesh_obj.edges_sorted, require_count=1)\n",
    "    bad_edges_ind = mesh_obj.edges[bad_edges]\n",
    "    sparse_edges = mesh_obj.edges_sparse\n",
    "    xs = list(bad_edges_ind[:, 0]) + list(bad_edges_ind[:, 1]) \n",
    "    ys = list(bad_edges_ind[:, 1]) + list(bad_edges_ind[:, 0])\n",
    "    vs = [1]*bad_edges_ind.shape[0]*2\n",
    "    bad_inds = scipy.sparse.coo_matrix((vs, (xs, ys)), shape=(mesh_obj.vertices.shape[0], mesh_obj.vertices.shape[0]))\n",
    "    # Make it symmetrical and add identity so each integrates from itself too, then subtract singleton edges\n",
    "    # I noticed that the number of asymmetrical edges vs the number of single edges I find from group rows\n",
    "    # Are close but different. Haven't looked into that yet. Also removing edges 1 hop away from single edges to remove bias towards\n",
    "    # Holes in the mesh that are caused by mesh construction errors as opposed to segmentation errors\n",
    "    sparse_edges = mesh_obj.edges_sparse + mesh_obj.edges_sparse.T + identity(mesh_obj.edges_sparse.shape[0]) - sparse_edges.multiply(bad_inds) - bad_inds\n",
    "    degs = mesh_obj.vertex_degree + 1\n",
    "\n",
    "    # N_iter is a smoothing parameter here. The loop below smooths the vertex error about the mesh to get more consistent connected regions\n",
    "    n_iter = 2\n",
    "    angle_sum = np.array(abs(mesh_obj.face_angles_sparse).sum(axis=1)).flatten()\n",
    "    defs = (2 * np.pi) - angle_sum\n",
    "\n",
    "    abs_defs = np.abs(defs)\n",
    "    abs_defs_i = abs_defs.copy()\n",
    "    for i in range(n_iter):\n",
    "        abs_defs_i = sparse_edges.dot(abs_defs_i) / degs\n",
    "    \n",
    "    verts_select = np.argwhere((abs_defs_i > a))# & (abs_defs < 2.5))\n",
    "\n",
    "    edges_mask = np.isin(mesh_obj.edges, verts_select)\n",
    "    edges_mask[bad_edges] = False\n",
    "    edges_select = edges_mask[:, 0] * edges_mask[:, 1]\n",
    "    edges_select = mesh_obj.edges[edges_select]\n",
    "\n",
    "    G = nx.from_edgelist(edges_select)#f_edge_sub)\n",
    "\n",
    "    ccs = nx.connected_components(G)\n",
    "    subgraphs = [G.subgraph(cc).copy() for cc in ccs]\n",
    "\n",
    "    lens = []\n",
    "    lengths = []\n",
    "    for i in tqdm(range(len(subgraphs))):\n",
    "        ns = np.array(list(subgraphs[i].nodes()))\n",
    "    #     ns = ns[abs_defs[ns ]]\n",
    "        l = len(ns)\n",
    "        if l > 20 and l < 5000:\n",
    "            lens.append(ns)\n",
    "            lengths.append(l)\n",
    "    all_nodes = set()\n",
    "    for l in lens:\n",
    "        all_nodes.update(l)\n",
    "    all_nodes = np.array(list(all_nodes))\n",
    "    # sharp_pts = mesh_obj.vertices[all_nodes]\n",
    "    centers = np.array([np.mean(mesh_obj.vertices[list(ppts)],axis=0) for ppts in lens])\n",
    "\n",
    "    return centers, lens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_endpoints(mesh_obj, skel_mp):\n",
    "    # Process the skeleton to get the endpoints\n",
    "    interior_cc_mask = set()\n",
    "    el = nx.from_edgelist(skel_mp.edges)\n",
    "    comps = list(nx.connected_components(el))\n",
    "    for c in comps:\n",
    "        if len(c) < 100:\n",
    "            n_con = mesh_obj.contains(skel_mp.vertices[list(c)])\n",
    "            if np.sum(n_con) / n_con.shape[0] > .10:\n",
    "                interior_cc_mask.update(list(c))\n",
    "    # Process the skeleton to get the endpoints\n",
    "    edges = skel_mp.edges.copy()\n",
    "\n",
    "    edge_mask = ~np.isin(edges, interior_cc_mask)\n",
    "    edge_mask = edge_mask[:, 0] + edge_mask[:, 1]\n",
    "    edges = edges[edge_mask]\n",
    "    edges_flat  = edges.flatten()\n",
    "    edge_bins = np.bincount(edges_flat) \n",
    "\n",
    "    eps = np.squeeze(np.argwhere(edge_bins==1))\n",
    "    eps_nm = skel_mp.vertices[eps]\n",
    "\n",
    "    eps_comp = distance_matrix(eps_nm, eps_nm)\n",
    "    eps_comp[eps_comp == 0] = np.inf\n",
    "    eps_thresh = np.argwhere(~(np.min(eps_comp, axis=0) < 3000))\n",
    "\n",
    "    eps = np.squeeze(eps[eps_thresh])\n",
    "    eps_nm = np.squeeze(eps_nm[eps_thresh])\n",
    "    return eps, eps_nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mesh_errors(mesh_obj, centers, eps, eps_nm, lens, skel_mp):\n",
    "    print(\"Processing mesh errors\")\n",
    "    path_to_root_dict = {}\n",
    "    for ep in eps:\n",
    "        path_to_root_dict[ep] = skel_mp.path_to_root(ep)\n",
    "        \n",
    "    dists_defects = np.zeros(centers.shape[0])\n",
    "    sizes = np.zeros(centers.shape[0])\n",
    "    mesh_map = skel_mp.mesh_to_skel_map\n",
    "    closest_skel_pts = mesh_map[[l[0] for l in lens]]\n",
    "\n",
    "    # print(centers, eps_nm)\n",
    "\n",
    "    dist_matrix = distance_matrix(centers, eps_nm)\n",
    "    ct = 0\n",
    "\n",
    "    closest_tip = np.zeros((centers.shape[0]))\n",
    "\n",
    "    for center in tqdm(centers):\n",
    "    #     skel_pts_dists = np.linalg.norm(skel_mp.vertices - center, axis=1)\n",
    "    #     ep_pts_dists = np.linalg.norm(eps_nm - center, axis=1)\n",
    "        \n",
    "        closest_skel_pt = closest_skel_pts[ct]\n",
    "        min_ep = np.inf\n",
    "        eps_hit = []\n",
    "        for j, ep in enumerate(eps):\n",
    "            if closest_skel_pt in path_to_root_dict[ep]:\n",
    "                eps_hit.append(j)\n",
    "        if len(eps_hit) == 0:\n",
    "            dists_defects[ct] = np.inf\n",
    "            sizes[ct] = np.inf\n",
    "            ct+=1\n",
    "            continue\n",
    "        \n",
    "        dists = dist_matrix[ct, eps_hit]\n",
    "    #     print(dists, eps_hit, center / [4,4,40])\n",
    "        \n",
    "        amin = np.argmin(dists)\n",
    "        tip_hit = eps_hit[amin]\n",
    "        min_dist = dists[amin]\n",
    "        \n",
    "        closest_tip[ct] = tip_hit\n",
    "    #     print(np.argmin(ep_pts_dists), ep_found, eps_nm[np.argmin(ep_pts_dists)]/[4,4,40], eps_nm[j]/[4,4,40], center/[4,4,40])\n",
    "        dists_defects[ct] = min_dist\n",
    "        sizes[ct] = len(lens[ct])\n",
    "        ct+=1\n",
    "    dists_defects_sub = dists_defects[dists_defects < np.inf]\n",
    "    sizes_sub = sizes[dists_defects < np.inf]\n",
    "    centers_sub = centers[dists_defects < np.inf]\n",
    "    tips_hit_sub = closest_tip[dists_defects < np.inf]\n",
    "    closest_skel_pts_sub = closest_skel_pts[dists_defects < np.inf]\n",
    "    inds_sub = np.arange(centers.shape[0])[dists_defects < np.inf]\n",
    "\n",
    "\n",
    "    # Also ranking each component based on its PCA- if the first component is big enough, the points are mostly linear\n",
    "    # These point sets seem to be less likely to be true errors\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca_vec = np.zeros(inds_sub.shape[0])\n",
    "    for i in range(inds_sub.shape[0]):\n",
    "        pca = PCA()#n_components=2)\n",
    "        pca.fit(mesh_obj.vertices[lens[inds_sub[i]]])\n",
    "\n",
    "        pca_vec[i] = pca.explained_variance_ratio_[0]\n",
    "\n",
    "    dists_defects_sub[dists_defects_sub < 4000] = 100\n",
    "    dists_defects_norm = dists_defects_sub #/ np.max(dists_defects_sub)\n",
    "    ranks_ep = sizes_sub / dists_defects_norm * (1-pca_vec)\n",
    "    ranks = sizes_sub**2 * (1-pca_vec)\n",
    "\n",
    "    #ranks_ep_errors_filt = ranks_ep[ranks_ep > .1]\n",
    "    centers_ep_send_errors = centers_sub[np.argsort(ranks_ep)][::-1][:20]\n",
    "    final_mask_eps = np.full(centers_ep_send_errors.shape[0], True)\n",
    "    tips_hit_send_ep = tips_hit_sub[np.argsort(ranks_ep)][::-1][:20]\n",
    "    uns, nums = np.unique(tips_hit_send_ep, return_counts=True)\n",
    "\n",
    "    for un, num in zip(uns, nums):\n",
    "        if num > 1:\n",
    "            final_mask_eps[np.argwhere(tips_hit_send_ep == un)[1:]] = False\n",
    "    centers_errors_ep = centers_ep_send_errors[final_mask_eps]\n",
    "    centers_errors = centers_sub[np.argsort(ranks)[::-1]][:20]\n",
    "    return centers_errors, centers_errors_ep, ranks, ranks_ep, path_to_root_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mesh_facets(mesh_obj, skel_mp, eps, path_to_root_dict, eps_nm):\n",
    "    print(\"Processing facets\")\n",
    "    #can possibly change param here\n",
    "    locs = np.argwhere(mesh_obj.facets_area > 5000)\n",
    "\n",
    "    mesh_map = skel_mp.mesh_to_skel_map\n",
    "    mesh_coords = mesh_obj.vertices[mesh_obj.faces]\n",
    "    mean_locs = []\n",
    "    mesh_ind = []\n",
    "    fs = []\n",
    "    for l in tqdm(locs):\n",
    "        fs.append(np.sum(mesh_obj.facets_area[l]))\n",
    "        fc = mesh_obj.facets[l[0]]\n",
    "        vert_locs = mesh_coords[fc]\n",
    "        mean_locs.append(np.mean(vert_locs[:, 0], axis=0))\n",
    "        mesh_ind.append(fc[0])\n",
    "    mesh_ind = mesh_obj.faces[mesh_ind][:, 0]\n",
    "    mean_locs = np.array(mean_locs)\n",
    "    dists_defects_facets = np.zeros(mean_locs.shape[0])\n",
    "    mesh_map_facets = skel_mp.mesh_to_skel_map\n",
    "    closest_skel_pts_facets = mesh_map[[m for m in mesh_ind]]\n",
    "    dist_matrix_facets = distance_matrix(mean_locs, eps_nm)\n",
    "    ct = 0\n",
    "\n",
    "    closest_tip_facets = np.zeros((mean_locs.shape[0]))\n",
    "\n",
    "    for center in tqdm(mean_locs):\n",
    "\n",
    "        closest_skel_pt = closest_skel_pts_facets[ct]\n",
    "        eps_hit = []\n",
    "        for j, ep in enumerate(eps):\n",
    "            if closest_skel_pt in path_to_root_dict[ep]:\n",
    "                eps_hit.append(j)\n",
    "        if len(eps_hit) == 0:\n",
    "            dists_defects_facets[ct] = np.inf\n",
    "            ct+=1\n",
    "            continue\n",
    "        \n",
    "        dists = dist_matrix_facets[ct, eps_hit]\n",
    "        \n",
    "        amin = np.argmin(dists)\n",
    "        tip_hit = eps_hit[amin]\n",
    "        min_dist = dists[amin]\n",
    "        \n",
    "        closest_tip_facets[ct] = tip_hit\n",
    "        dists_defects_facets[ct] = min_dist\n",
    "        ct+=1\n",
    "    dists_defects_sub_facets = dists_defects_facets[dists_defects_facets < np.inf]\n",
    "    sizes_sub_facets = np.array(fs)[dists_defects_facets < np.inf]\n",
    "    mean_locs_facets = mean_locs[dists_defects_facets < np.inf]\n",
    "    \n",
    "    tips_hit_sub_facets = closest_tip_facets[dists_defects_facets < np.inf]\n",
    "    closest_skel_pts_sub_facets = closest_skel_pts_facets[dists_defects_facets < np.inf]\n",
    "    inds_sub_facets = np.arange(mean_locs.shape[0])[dists_defects_facets < np.inf]\n",
    "    \n",
    "    ranks_ep_facets = sizes_sub_facets**2 / dists_defects_sub_facets\n",
    "    #ranks_ep_facets_filt = ranks_ep_facets[ranks_ep_facets > 2e7]\n",
    "    mean_locs_send_facets = mean_locs_facets[np.argsort(ranks_ep_facets)][::-1][:20]\n",
    "    final_mask_facets = np.full(mean_locs_send_facets.shape[0], True)\n",
    "    tips_hit_send_facets = tips_hit_sub_facets[np.argsort(ranks_ep_facets)][::-1][:20]\n",
    "    uns, nums = np.unique(tips_hit_send_facets, return_counts=True)\n",
    "\n",
    "    for un, num in zip(uns, nums):\n",
    "        if num > 1:\n",
    "            final_mask_facets[np.argwhere(tips_hit_send_facets == un)[1:]] = False\n",
    "    facets_send_final = mean_locs_send_facets[final_mask_facets] / [4,4,40]\n",
    "    return facets_send_final, ranks_ep_facets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def error_locs_defects(root_id, soma_id = None, soma_table=None, center_collapse=True):\n",
    "    #print(\"START\", root_id)\n",
    "\n",
    "    mesh_obj = get_and_process_mesh(root_id)\n",
    "    if mesh_obj is None:\n",
    "        return None\n",
    "    # SKELETONIZE - if we are just looking for general errors, not errors at endpoints, this can be skipped\n",
    "    try:\n",
    "        if soma_table==None:\n",
    "            soma_table = get_soma(str(soma_id))\n",
    "        if soma_table[soma_table.id == soma_id].shape[0] > 0:\n",
    "            center = np.array(soma_table[soma_table.id == soma_id].pt_position)[0] * [4,4,40]\n",
    "        else:\n",
    "            center=None\n",
    "    except:\n",
    "        center = None\n",
    "    print(\"Subselecting largest connected component of mesh\")\n",
    "    mesh_obj, encapsulated_ids, max_verts = process_mesh_ccs(mesh_obj)\n",
    "    \n",
    "\n",
    "    skel_mp = skeletonize.skeletonize_mesh(trimesh_io.Mesh(mesh_obj.vertices, \n",
    "                                            mesh_obj.faces),\n",
    "                                            invalidation_d=3000,\n",
    "                                            shape_function='cone',\n",
    "                                            collapse_function='branch',\n",
    "#                                             soma_radius = soma_radius,\n",
    "                                            soma_pt=center,\n",
    "                                            smooth_neighborhood=5,\n",
    "                                            cc_vertex_thresh=max_verts - 10\n",
    "#                                                     collapse_params = {'dynamic_threshold':True}\n",
    "                                            )\n",
    "    print(\"Skel done\")\n",
    "\n",
    "    # find edges that only occur once..  might be faster to find these in the sparse matrix..\n",
    "    centers, lens = process_defects(mesh_obj)\n",
    "    eps, eps_nm = process_endpoints(mesh_obj, skel_mp)\n",
    "\n",
    "    if len(centers) !=0:\n",
    "        centers_errors, centers_errors_ep, ranks, ranks_ep, path_to_root_dict = process_mesh_errors(mesh_obj, centers, eps, eps_nm, lens, skel_mp)\n",
    "        ranks_return = np.squeeze(ranks[np.argsort(ranks)[::-1]][:20])\n",
    "        ranks_ep_return = np.squeeze(ranks_ep[np.argsort(ranks_ep)][::-1][:20])\n",
    "    else:\n",
    "        # Assign placeholder values for each of the variables above.\n",
    "        centers_errors = np.zeros ((1,3))\n",
    "        centers_errors_ep = np.zeros ((1,3))\n",
    "        ranks = np.zeros ((1))\n",
    "        ranks_ep = np.zeros((1, 3))\n",
    "        path_to_root_dict = {}\n",
    "        for ep in eps:\n",
    "            path_to_root_dict[ep] = skel_mp.path_to_root(ep)\n",
    "\n",
    "        ranks_return = 0\n",
    "        ranks_ep_return = 0\n",
    "\n",
    "\n",
    "    #if len(centers_errors.shape) > 1 and centers_errors.shape[0] > 0 and len(centers_errors_ep.shape) > 1 and len(centers_errors_ep.shape[0] > 0):\n",
    "    #    centers_errors = centers_errors[np.min(distance_matrix(centers_errors, centers_errors_ep), axis=1)>1000]\n",
    "    facets_send_final, ranks_ep_facets = process_mesh_facets(mesh_obj, skel_mp, eps, path_to_root_dict, eps_nm)\n",
    "\n",
    "    errors_send = centers_errors / [4,4,40]\n",
    "    errors_tips_send = centers_errors_ep / [4,4,40]\n",
    "    encapsulated_centers = [e[0] for e in encapsulated_ids]\n",
    "    encapsulated_lens = [e[1] for e in encapsulated_ids]\n",
    "    sorted_encapsulated_send = np.array(encapsulated_centers)[np.argsort(encapsulated_lens)][::-1]\n",
    "\n",
    "\n",
    "\n",
    "    return sorted_encapsulated_send, facets_send_final, errors_send, errors_tips_send, np.squeeze(ranks_ep_facets[[np.argsort(ranks_ep_facets)][::-1][:20]]), ranks_return, ranks_ep_return\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING TIP GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_endpoints(row):\n",
    "    seg_id = row[\"seg_id\"]\n",
    "    sorted_encapsulated_send, facets_send_final, errors_send, errors_tips_send, dummy, dummy2, dummy3 = error_locs_defects(seg_id)\n",
    "    #facets_send_final = facets_send_final[facets_send_final != [0,0,0]]\n",
    "    #errors_tips_send = errors_tips_send[errors_tips_send != [0,0,0]]\n",
    "    together = np.vstack((facets_send_final, errors_tips_send))\n",
    "    #should alter \"together\" if not set together equal to this line\n",
    "    # print(together)\n",
    "    # new = together[together != [0,0,0]]\n",
    "    mask=np.sum(together,axis=1)\n",
    "    together = together[mask > 0]\n",
    "    return together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_endpoints(dataframe) :\n",
    "    dataframe[\"endpoints_generated\"] = dataframe.apply(find_endpoints, axis = 1)\n",
    "    return dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACCURACY FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_eps_acc(gt_endpoints, pred_endpoints, threshold):\n",
    "    # Calculate distances\n",
    "    dist_matrix = np.array(spatial.distance.cdist(gt_endpoints, pred_endpoints, metric = 'euclidean'))\n",
    "\n",
    "    # Apply threshold\n",
    "    dist_matrix[dist_matrix > threshold] = 0\n",
    "\n",
    "    # Calculating accuracy\n",
    "    valid_eps = np.count_nonzero(dist_matrix, axis = 1)\n",
    "    accuracy = np.count_nonzero(valid_eps) / len(gt_endpoints)\n",
    "\n",
    "    #If more than one valid endpoint found for a single ground truth endpoint, add the other valid endpoints to extra_valid_pairs\n",
    "    # extra_valid_pairs = []\n",
    "    # [[extra_valid_pairs.append([gt_endpoints[i], pred_endpoints[index]]) \\\n",
    "    #     for index, j in enumerate(dist_matrix[i]) if j != np.min(dist_matrix[i][dist_matrix[i] != 0]) if j != 0] \\\n",
    "    #         for i in valid_eps if i > 1]\n",
    "\n",
    "    # return accuracy, extra_valid_pairs\n",
    "    return accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Mesh\n",
      "Warning: deduplication not currently supported for this layer's variable layered draco meshes\n",
      "Vertices:  7993\n",
      "Subselecting largest connected component of mesh\n",
      "Processing CC's\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7992/7992 [00:00<00:00, 1295943.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skel done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 146772.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing facets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 537/537 [00:00<00:00, 34412.11it/s]\n",
      "100%|██████████| 537/537 [00:00<00:00, 58160.96it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb Cell 23\u001b[0m in \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m## TESTING ON THE 30 NEWLY LABELLED ORPHANS\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m orphans \u001b[39m=\u001b[39m generate_endpoints(orphans)\n",
      "\u001b[1;32m/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb Cell 23\u001b[0m in \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_endpoints\u001b[39m(dataframe) :\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     dataframe[\u001b[39m\"\u001b[39m\u001b[39mendpoints_generated\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataframe\u001b[39m.\u001b[39;49mapply(find_endpoints, axis \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m dataframe\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/frame.py:9555\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9544\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9546\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[1;32m   9547\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   9548\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9553\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[1;32m   9554\u001b[0m )\n\u001b[0;32m-> 9555\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/apply.py:746\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[1;32m    744\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[0;32m--> 746\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/apply.py:873\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 873\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[1;32m    875\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[1;32m    876\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/apply.py:889\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    887\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[1;32m    888\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[1;32m    890\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    891\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    892\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    893\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32m/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb Cell 23\u001b[0m in \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_endpoints\u001b[39m(row):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     seg_id \u001b[39m=\u001b[39m row[\u001b[39m\"\u001b[39m\u001b[39mseg_id\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     sorted_encapsulated_send, facets_send_final, errors_send, errors_tips_send, dummy, dummy2, dummy3 \u001b[39m=\u001b[39m error_locs_defects(seg_id)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m#facets_send_final = facets_send_final[facets_send_final != [0,0,0]]\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m#errors_tips_send = errors_tips_send[errors_tips_send != [0,0,0]]\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb#X31sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     together \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack((facets_send_final, errors_tips_send))\n",
      "\u001b[1;32m/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb Cell 23\u001b[0m in \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39merror_locs_defects\u001b[39m(root_id, soma_id \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, soma_table\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, center_collapse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m#print(\"START\", root_id)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     mesh_obj \u001b[39m=\u001b[39m get_and_process_mesh(root_id)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mif\u001b[39;00m mesh_obj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb#X31sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb Cell 23\u001b[0m in \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m datastack_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mminnie65_phase3_v1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m client \u001b[39m=\u001b[39m CAVEclient(datastack_name)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m vol \u001b[39m=\u001b[39m CloudVolume(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     client\u001b[39m.\u001b[39;49minfo\u001b[39m.\u001b[39;49msegmentation_source(),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb#X31sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     use_https\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     progress\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb#X31sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     bounded\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb#X31sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     fill_missing\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb#X31sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     secrets\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mtoken\u001b[39;49m\u001b[39m\"\u001b[39;49m: client\u001b[39m.\u001b[39;49mauth\u001b[39m.\u001b[39;49mtoken}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb#X31sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb#X31sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDownloading Mesh\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sheeltanna/Desktop/AGT_REPO/campfire/copy_of_work.ipynb#X31sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m mesh \u001b[39m=\u001b[39m vol\u001b[39m.\u001b[39mmesh\u001b[39m.\u001b[39mget(\u001b[39mstr\u001b[39m(root_id))[root_id]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/cloudvolume/cloudvolume.py:233\u001b[0m, in \u001b[0;36mCloudVolume.__new__\u001b[0;34m(cls, cloudpath, mip, bounded, autocrop, fill_missing, cache, compress_cache, cdn_cache, progress, info, provenance, compress, compress_level, non_aligned_writes, parallel, delete_black_uploads, background_color, green_threads, use_https, max_redirects, mesh_dir, skel_dir, agglomerate, secrets, spatial_index_db, lru_bytes)\u001b[0m\n\u001b[1;32m    231\u001b[0m path \u001b[39m=\u001b[39m strict_extract(cloudpath)\n\u001b[1;32m    232\u001b[0m \u001b[39mif\u001b[39;00m path\u001b[39m.\u001b[39mformat \u001b[39min\u001b[39;00m REGISTERED_PLUGINS:\n\u001b[0;32m--> 233\u001b[0m   \u001b[39mreturn\u001b[39;00m REGISTERED_PLUGINS[path\u001b[39m.\u001b[39;49mformat](\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    234\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m   \u001b[39mraise\u001b[39;00m UnsupportedFormatError(\n\u001b[1;32m    236\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mUnknown format \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(path\u001b[39m.\u001b[39mformat)\n\u001b[1;32m    237\u001b[0m   )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/cloudvolume/datasource/graphene/__init__.py:54\u001b[0m, in \u001b[0;36mcreate_graphene\u001b[0;34m(cloudpath, mip, bounded, autocrop, fill_missing, cache, compress_cache, cdn_cache, progress, info, provenance, compress, parallel, delete_black_uploads, background_color, green_threads, use_https, mesh_dir, skel_dir, agglomerate, secrets, spatial_index_db, lru_bytes, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmkcache\u001b[39m(cloudpath):\n\u001b[1;32m     48\u001b[0m   \u001b[39mreturn\u001b[39;00m CacheService(\n\u001b[1;32m     49\u001b[0m     cloudpath\u001b[39m=\u001b[39mget_cache_path(cache, cloudpath),\n\u001b[1;32m     50\u001b[0m     enabled\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(cache),\n\u001b[1;32m     51\u001b[0m     config\u001b[39m=\u001b[39mconfig,\n\u001b[1;32m     52\u001b[0m     compress\u001b[39m=\u001b[39mcompress_cache,\n\u001b[1;32m     53\u001b[0m   )\n\u001b[0;32m---> 54\u001b[0m meta \u001b[39m=\u001b[39m GrapheneMetadata(\n\u001b[1;32m     55\u001b[0m   cloudpath, config\u001b[39m=\u001b[39;49mconfig, cache\u001b[39m=\u001b[39;49mmkcache(cloudpath),\n\u001b[1;32m     56\u001b[0m   info\u001b[39m=\u001b[39;49minfo, provenance\u001b[39m=\u001b[39;49mprovenance, \n\u001b[1;32m     57\u001b[0m   use_https\u001b[39m=\u001b[39;49muse_https, agglomerate\u001b[39m=\u001b[39;49magglomerate,\n\u001b[1;32m     58\u001b[0m   auth_token\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49msecrets,\n\u001b[1;32m     59\u001b[0m )\n\u001b[1;32m     60\u001b[0m \u001b[39m# Resetting the cache is necessary because\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39m# graphene retrieves a data_dir from the info file\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39m# that reflects the real cache location.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m cache_service \u001b[39m=\u001b[39m mkcache(meta\u001b[39m.\u001b[39mcloudpath) \n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/cloudvolume/datasource/graphene/metadata.py:95\u001b[0m, in \u001b[0;36mGrapheneMetadata.__init__\u001b[0;34m(self, cloudpath, use_https, use_auth, auth_token, agglomerate, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauth_header \u001b[39m=\u001b[39m {\n\u001b[1;32m     92\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mAuthorization\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mBearer \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_token(auth_token)\n\u001b[1;32m     93\u001b[0m   }\n\u001b[1;32m     94\u001b[0m kwargs[\u001b[39m'\u001b[39m\u001b[39muse_https\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mbool\u001b[39m(use_https)\n\u001b[0;32m---> 95\u001b[0m \u001b[39msuper\u001b[39;49m(GrapheneMetadata, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(cloudpath, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     97\u001b[0m version \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mserver_path\u001b[39m.\u001b[39mversion\n\u001b[1;32m     98\u001b[0m \u001b[39mif\u001b[39;00m version \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtable\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/cloudvolume/datasource/precomputed/metadata.py:63\u001b[0m, in \u001b[0;36mPrecomputedMetadata.__init__\u001b[0;34m(self, cloudpath, config, cache, info, provenance, max_redirects, use_https)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mif\u001b[39;00m provenance \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprovenance \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrefresh_provenance()\n\u001b[1;32m     64\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache\u001b[39m.\u001b[39menabled:\n\u001b[1;32m     65\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache\u001b[39m.\u001b[39mcheck_provenance_validity()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/cloudvolume/datasource/precomputed/metadata.py:330\u001b[0m, in \u001b[0;36mPrecomputedMetadata.refresh_provenance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprovenance \u001b[39m=\u001b[39m DataLayerProvenance(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprov)\n\u001b[1;32m    328\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprovenance\n\u001b[0;32m--> 330\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprovenance \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfetch_provenance()\n\u001b[1;32m    331\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache:\n\u001b[1;32m    332\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache\u001b[39m.\u001b[39mmaybe_cache_provenance()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/cloudvolume/datasource/precomputed/metadata.py:362\u001b[0m, in \u001b[0;36mPrecomputedMetadata.fetch_provenance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[39mRefresh the current provenance file from primary storage (e.g. the cloud)\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[39mwithout reference to cache. The cache will not be updated.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39mReturns: dict\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    361\u001b[0m cf \u001b[39m=\u001b[39m CloudFiles(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcloudpath, secrets\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39msecrets)\n\u001b[0;32m--> 362\u001b[0m provfile \u001b[39m=\u001b[39m cf\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mprovenance\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    363\u001b[0m \u001b[39mif\u001b[39;00m provfile:\n\u001b[1;32m    364\u001b[0m   provfile \u001b[39m=\u001b[39m provfile\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/cloudfiles/cloudfiles.py:98\u001b[0m, in \u001b[0;36mparallelize.<locals>.decor.<locals>.inner_decor\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m parallel \u001b[39m=\u001b[39m nvl(parallel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel, \u001b[39m1\u001b[39m)\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m parallel \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 98\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    100\u001b[0m progress \u001b[39m=\u001b[39m params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mprogress\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    101\u001b[0m params[\u001b[39m\"\u001b[39m\u001b[39mprogress\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/cloudfiles/cloudfiles.py:404\u001b[0m, in \u001b[0;36mCloudFiles.get\u001b[0;34m(self, paths, total, raw, progress, parallel, return_dict, raise_errors)\u001b[0m\n\u001b[1;32m    401\u001b[0m total \u001b[39m=\u001b[39m totalfn(paths, total)\n\u001b[1;32m    403\u001b[0m \u001b[39mif\u001b[39;00m total \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 404\u001b[0m   ret \u001b[39m=\u001b[39m download(first(paths))\n\u001b[1;32m    405\u001b[0m   \u001b[39mif\u001b[39;00m return_dict:\n\u001b[1;32m    406\u001b[0m     \u001b[39mreturn\u001b[39;00m { ret[\u001b[39m\"\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m\"\u001b[39m]: ret[\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m] }\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/cloudfiles/cloudfiles.py:374\u001b[0m, in \u001b[0;36mCloudFiles.get.<locals>.download\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    373\u001b[0m   \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection() \u001b[39mas\u001b[39;00m conn:\n\u001b[0;32m--> 374\u001b[0m     content, encoding, server_hash, server_hash_type \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mget_file(path, start\u001b[39m=\u001b[39;49mstart, end\u001b[39m=\u001b[39;49mend)\n\u001b[1;32m    376\u001b[0m   \u001b[39m# md5s don't match for partial reads\u001b[39;00m\n\u001b[1;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m start \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m end \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py:432\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    431\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 432\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    434\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    436\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/concurrent/futures/_base.py:388\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__get_result\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    387\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m--> 388\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    389\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    390\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/cloudfiles/interfaces.py:633\u001b[0m, in \u001b[0;36mHttpInterface.get_file\u001b[0;34m(self, file_path, start, end)\u001b[0m\n\u001b[1;32m    631\u001b[0m   resp \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(key, headers\u001b[39m=\u001b[39mheaders)\n\u001b[1;32m    632\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 633\u001b[0m   resp \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(key)\n\u001b[1;32m    634\u001b[0m \u001b[39mif\u001b[39;00m resp\u001b[39m.\u001b[39mstatus_code \u001b[39min\u001b[39;00m (\u001b[39m404\u001b[39m, \u001b[39m403\u001b[39m):\n\u001b[1;32m    635\u001b[0m   \u001b[39mreturn\u001b[39;00m (\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/requests/sessions.py:745\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stream:\n\u001b[0;32m--> 745\u001b[0m     r\u001b[39m.\u001b[39;49mcontent\n\u001b[1;32m    747\u001b[0m \u001b[39mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/requests/models.py:899\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    898\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 899\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content \u001b[39m=\u001b[39m \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter_content(CONTENT_CHUNK_SIZE)) \u001b[39mor\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    901\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content_consumed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    902\u001b[0m \u001b[39m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[39m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/urllib3/response.py:627\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[0;32m--> 627\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[1;32m    629\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[1;32m    630\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/urllib3/response.py:566\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    563\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    565\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 566\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    568\u001b[0m         flush_decoder \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/urllib3/response.py:532\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    530\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    531\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/http/client.py:454\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    452\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    453\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 454\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    455\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[1;32m    456\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    457\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/http/client.py:498\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    493\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[1;32m    495\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 498\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    499\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[1;32m    500\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    501\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    502\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## TESTING ON THE 30 NEWLY LABELLED ORPHANS\n",
    "\n",
    "orphans = generate_endpoints(orphans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply function to entire df \n",
    "count = 0\n",
    "acc_array = []\n",
    "for index, row in orphans.iterrows():\n",
    "    if (type(row[\"real_endpoints\"])== list and type(row[\"endpoints_generated\"]) == list):\n",
    "        acc = 1\n",
    "        print(\"both empty\")\n",
    "    elif(type(row[\"endpoints_generated\"]) == list and type(row[\"real_endpoints\"]) != list):\n",
    "        acc = 0\n",
    "        print(\"no endpoints generated, but endpoints exist\")\n",
    "    else:\n",
    "        count = count + 1\n",
    "        # print(\"predicted,\", row[\"endpoints_generated\"])\n",
    "        # print()\n",
    "        # print(\"real,\", row[\"real_endpoints\"])\n",
    "        # print()\n",
    "        # print(\"seg_id\", row[\"seg_id\"])\n",
    "        # print()\n",
    "        # print(type(row[\"real_endpoints\"]))\n",
    "\n",
    "        #print(str(count) + ':')\n",
    "        acc = pred_eps_acc(row[\"real_endpoints\"], row[\"endpoints_generated\"], 200)\n",
    "        acc_array.append(acc)\n",
    "        if(acc == 1):\n",
    "            print(row[\"seg_id\"])\n",
    "        #print(acc)\n",
    "        # print(\"NEXT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_array = np.array(acc_array)\n",
    "print(acc_array)\n",
    "print()\n",
    "print(len(acc_array[acc_array == 1.0]))\n",
    "\n",
    "\n",
    "###SEVENTY PERCENT OF ORPHANS HAD SOME ENDPOINT FOUND\n",
    "\n",
    "\n",
    "# print(len(acc_array))\n",
    "# print(acc_array)\n",
    "# print(acc_array[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###all perfect scores\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--Possiby search the 6 perfect ones dependent on task \n",
    "--changing invalidation_d resulted in the biggest change in accuracies \n",
    "--eps_nm? \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINE TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# indices 2, 5 11\n",
    "\n",
    "print(orphans[\"seg_id\"][2])\n",
    "print(orphans[\"endpoints_generated\"][2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES FOR SMALL PROCESSES: \n",
    "\n",
    "1. index 2 was very small process with two clear flat ends. Skeleton did not reach one flat end and resulted in zero eps_nm and 1 error location which was rightfully filtered out. Changing threshold for eps_nm (not sure if we shoudl do this) found 2 endpoints, but these locations are not correct. They represent the ends of the skeleton. Increasing inval_d did not find the correct endpoints (as exepcted). Decreasing invalidation_d from 5000 to 1500 found one of the flat regions, but endpoint filtered out before getting facets. Endpoint not found if eps_nm threshold returned to 3000. \n",
    "\n",
    "2. index 5 also somewhat small, but has 2 flat ends. Skeleton did reach both ends (somewhat), but resulted in zero eps_nm with threshold of 3000. Changed eps_nm threshold to 1500 (not sure if we should do this) and found 2 endpoints (ends of the skeleton). Left invalidation_d quite low at 1500. AFter filtering and scaling, the two endpoints in ep_nm were gone. We should focus on translation from eps_nm to facets. \n",
    "\n",
    "3. Index 11 was VERY VERY small and flat. Resonable that skeleton wasn't albe to be processed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_obj = get_and_process_mesh(864691135909994000)\n",
    "print(\"Processing CC's\")\n",
    "ccs_graph = trimesh.graph.connected_components(mesh_obj.edges)\n",
    "ccs_len = [len(c) for c in ccs_graph]\n",
    "\n",
    "# Subselect the parts of the mesh that are not inside one another \n",
    "# the other components are an artifact of the soma seg and small unfilled sections\n",
    "largest_component = ccs_graph[np.argmax(ccs_len)]\n",
    "largest_component_remap = np.arange(ccs_graph[np.argmax(ccs_len)].shape[0])\n",
    "face_dict = {largest_component[i]:largest_component_remap[i] for i in range(largest_component.shape[0])}\n",
    "\n",
    "new_faces_mask = np.isin(mesh_obj.faces, list(face_dict.keys()))\n",
    "new_faces_mask = new_faces_mask[:, 0]*new_faces_mask[:, 1]*new_faces_mask[:, 2]\n",
    "\n",
    "new_faces = np.vectorize(face_dict.get)(mesh_obj.faces[new_faces_mask])\n",
    "new_faces = new_faces[new_faces[:, 0] != None]\n",
    "largest_component_mesh = trimesh.Trimesh(mesh_obj.vertices[largest_component], new_faces)\n",
    "\n",
    "all_ids = set(largest_component)\n",
    "encapsulated_ids = []\n",
    "\n",
    "for i in range(1, len(ccs_graph)):\n",
    "    n_con = largest_component_mesh.contains(mesh_obj.vertices[ccs_graph[i]])\n",
    "    if np.sum(n_con) / n_con.shape[0] == 0 and n_con.shape[0] > 50:\n",
    "        all_ids.update(ccs_graph[i])\n",
    "    else:\n",
    "        if len(ccs_graph[i]) < 1000:\n",
    "            encapsulated_ids.append((np.mean(mesh_obj.vertices[ccs_graph[i]], axis=0)/[4,4,40], len(ccs_graph[i])))\n",
    "        \n",
    "all_component = np.array(list(ccs_graph[np.argmax(ccs_len)]))\n",
    "all_component_remap = np.arange(all_component.shape[0])\n",
    "face_dict = {all_component[i]:all_component_remap[i] for i in range(all_component.shape[0])}\n",
    "new_faces_mask = np.isin(mesh_obj.faces, list(face_dict.keys()))\n",
    "new_faces_mask = new_faces_mask[:, 0]*new_faces_mask[:, 1]*new_faces_mask[:, 2]\n",
    "\n",
    "new_faces = np.vectorize(face_dict.get)(mesh_obj.faces[new_faces_mask])\n",
    "new_faces[new_faces[:, 0] != None]\n",
    "\n",
    "largest_component_mesh = trimesh.Trimesh(mesh_obj.vertices[all_component], new_faces)\n",
    "\n",
    "mesh_obj = largest_component_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skel_mp = skeletonize.skeletonize_mesh(trimesh_io.Mesh(mesh_obj.vertices, \n",
    "                                            mesh_obj.faces),\n",
    "                                            invalidation_d=5000,\n",
    "                                            shape_function='cone',\n",
    "                                            collapse_function='branch',\n",
    "                                            #soma_radius = soma_radius,\n",
    "                                            #soma_pt=center,\n",
    "                                            smooth_neighborhood=5,\n",
    "                                            #cc_vertex_thresh=max_verts - 10\n",
    "                                            #collapse_params = {'dynamic_threshold':True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meshparty import trimesh_io, trimesh_vtk, skeletonize, mesh_filters\n",
    "skel_actor = trimesh_vtk.skeleton_actor(skel_mp,\n",
    "                   edge_property=None,\n",
    "                   vertex_property=None,\n",
    "                   vertex_data=None,\n",
    "                   normalize_property=True,\n",
    "                   color=(1, 0, 1),\n",
    "                   line_width=5,\n",
    "                   opacity=0.7,\n",
    "                   lut_map=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.75\n",
    "bad_edges = trimesh.grouping.group_rows(\n",
    "mesh_obj.edges_sorted, require_count=1)\n",
    "bad_edges_ind = mesh_obj.edges[bad_edges]\n",
    "sparse_edges = mesh_obj.edges_sparse\n",
    "xs = list(bad_edges_ind[:, 0]) + list(bad_edges_ind[:, 1]) \n",
    "ys = list(bad_edges_ind[:, 1]) + list(bad_edges_ind[:, 0])\n",
    "vs = [1]*bad_edges_ind.shape[0]*2\n",
    "bad_inds = scipy.sparse.coo_matrix((vs, (xs, ys)), shape=(mesh_obj.vertices.shape[0], mesh_obj.vertices.shape[0]))\n",
    "# Make it symmetrical and add identity so each integrates from itself too, then subtract singleton edges\n",
    "# I noticed that the number of asymmetrical edges vs the number of single edges I find from group rows\n",
    "# Are close but different. Haven't looked into that yet. Also removing edges 1 hop away from single edges to remove bias towards\n",
    "# Holes in the mesh that are caused by mesh construction errors as opposed to segmentation errors\n",
    "sparse_edges = mesh_obj.edges_sparse + mesh_obj.edges_sparse.T + identity(mesh_obj.edges_sparse.shape[0]) - sparse_edges.multiply(bad_inds) - bad_inds\n",
    "degs = mesh_obj.vertex_degree + 1\n",
    "\n",
    "# N_iter is a smoothing parameter here. The loop below smooths the vertex error about the mesh to get more consistent connected regions\n",
    "n_iter = 2\n",
    "angle_sum = np.array(abs(mesh_obj.face_angles_sparse).sum(axis=1)).flatten()\n",
    "defs = (2 * np.pi) - angle_sum\n",
    "\n",
    "abs_defs = np.abs(defs)\n",
    "abs_defs_i = abs_defs.copy()\n",
    "for i in range(n_iter):\n",
    "    abs_defs_i = sparse_edges.dot(abs_defs_i) / degs\n",
    "\n",
    "verts_select = np.argwhere((abs_defs_i > a))# & (abs_defs < 2.5))\n",
    "\n",
    "edges_mask = np.isin(mesh_obj.edges, verts_select)\n",
    "edges_mask[bad_edges] = False\n",
    "edges_select = edges_mask[:, 0] * edges_mask[:, 1]\n",
    "edges_select = mesh_obj.edges[edges_select]\n",
    "\n",
    "G = nx.from_edgelist(edges_select)#f_edge_sub)\n",
    "\n",
    "ccs = nx.connected_components(G)\n",
    "subgraphs = [G.subgraph(cc).copy() for cc in ccs]\n",
    "\n",
    "lens = []\n",
    "lengths = []\n",
    "for i in tqdm(range(len(subgraphs))):\n",
    "    ns = np.array(list(subgraphs[i].nodes()))\n",
    "#     ns = ns[abs_defs[ns ]]\n",
    "    l = len(ns)\n",
    "    if l > 20 and l < 5000:\n",
    "        lens.append(ns)\n",
    "        lengths.append(l)\n",
    "all_nodes = set()\n",
    "for l in lens:\n",
    "    all_nodes.update(l)\n",
    "all_nodes = np.array(list(all_nodes))\n",
    "# sharp_pts = mesh_obj.vertices[all_nodes]\n",
    "centers = np.array([np.mean(mesh_obj.vertices[list(ppts)],axis=0) for ppts in lens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interior_cc_mask = set()\n",
    "el = nx.from_edgelist(skel_mp.edges)\n",
    "comps = list(nx.connected_components(el))\n",
    "for c in comps:\n",
    "    if len(c) < 100:\n",
    "        n_con = mesh_obj.contains(skel_mp.vertices[list(c)])\n",
    "        if np.sum(n_con) / n_con.shape[0] > .10:\n",
    "            interior_cc_mask.update(list(c))\n",
    "# Process the skeleton to get the endpoints\n",
    "edges = skel_mp.edges.copy()\n",
    "\n",
    "edge_mask = ~np.isin(edges, interior_cc_mask)\n",
    "edge_mask = edge_mask[:, 0] + edge_mask[:, 1]\n",
    "edges = edges[edge_mask]\n",
    "edges_flat  = edges.flatten()\n",
    "edge_bins = np.bincount(edges_flat) \n",
    "\n",
    "eps = np.squeeze(np.argwhere(edge_bins==1))\n",
    "eps_nm = skel_mp.vertices[eps]\n",
    "print(eps_nm)\n",
    "\n",
    "eps_comp = distance_matrix(eps_nm, eps_nm)\n",
    "eps_comp[eps_comp == 0] = np.inf\n",
    "eps_thresh = np.argwhere(~(np.min(eps_comp, axis=0) < 1500))\n",
    "\n",
    "eps = np.squeeze(eps[eps_thresh])\n",
    "eps_nm = np.squeeze(eps_nm[eps_thresh])\n",
    "print()\n",
    "print(eps_nm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VISUALIZING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALZING HERE\n",
    "syn_actor = trimesh_vtk.point_cloud_actor(eps_nm, size=100, color=(0.0, 0.0, 0.9))\n",
    "syn_actor2 = trimesh_vtk.point_cloud_actor(centers, size=100, color=(0.9, 0.2, 0.9))\n",
    "mesh_actor = trimesh_vtk.mesh_actor(mesh_obj, opacity=1, color=(0.7, 0.7, 0.7))\n",
    "trimesh_vtk.render_actors([mesh_actor, skel_actor, syn_actor2, syn_actor])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BAD AND GOOD PROCESS \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "###PROCESS THAT it DOESN'T DO WELL ON\n",
    "Image('/Users/sheeltanna/Desktop/AGT_REPO/campfire/Small_Orphan_Image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the predicted endpoints \n",
    "sorted_encapsulated_send, facets_send_final, errors_send, errors_tips_send, dummy, dummy2, dummy3 = error_locs_defects(864691134330810585)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the accuracy + seeing the predicted vs real endpoints\n",
    "together = np.vstack((facets_send_final, errors_tips_send))\n",
    "    #should alter \"together\" if not set together equal to this line\n",
    "    # print(together)\n",
    "    # new = together[together != [0,0,0]]\n",
    "mask=np.sum(together,axis=1)\n",
    "together = together[mask > 0]\n",
    "print(\"predicted\")\n",
    "print(together)\n",
    "print()\n",
    "print(\"real\")\n",
    "print(orphans[\"real_endpoints\"][1])\n",
    "print()\n",
    "print(\"accurcy\")\n",
    "acc = pred_eps_acc(orphans[\"real_endpoints\"][1], together, 200)\n",
    "print(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"before scaling of endpoints\")\n",
    "print(eps_nm)\n",
    "print()\n",
    "print(\"after scaling of endpoints\")\n",
    "print(facets_send_final)\n",
    "print()\n",
    "print(\"before scaling of errors\")\n",
    "print(centers)\n",
    "print()\n",
    "print(\"after scaling of errors\")\n",
    "print(errors_tips_send)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
