{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "data = pd.read_csv(\"automatedSplit_tasks.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting data into tasks chronologically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unique task ID (seg_id+ID from above dataFrame)\n",
    "data['unique_task_id'] = data.apply(lambda row: int(str(row.seg_id) + str(row.ID)), axis=1)\n",
    "\n",
    "# Group each unique task ID together\n",
    "data = data.sort_values(\"unique_task_id\")\n",
    "\n",
    "# Convert the opened column to datetime object\n",
    "data.opened = pd.to_datetime(data.opened).values.astype(np.int64)\n",
    "\n",
    "# Group data by unique task ids\n",
    "grouped_data = data.groupby(\"unique_task_id\")\n",
    "\n",
    "# For each task, create a new column with average opened time\n",
    "data['avg_opened_time'] = data.apply(lambda row: grouped_data.get_group(row.unique_task_id).opened.mean(), axis=1)\n",
    "\n",
    "# Sort the tasks in the data df chronologically\n",
    "data = data.sort_values(\"avg_opened_time\")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering data to only include assignees that have completed all tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the data by unique_task_id\n",
    "grouped_data = data.groupby(\"unique_task_id\")\n",
    "\n",
    "# Creating a dataframe that stores the assignees for each unique task\n",
    "task_assignees = pd.DataFrame(\n",
    "    columns=['unique_task_id', 'num_assignees', 'assignees'])\n",
    "\n",
    "for group_name, group in grouped_data:\n",
    "    task_assignees.loc[len(task_assignees.index)] = [group_name, len(pd.unique(group.assignee_masked)), pd.unique(group.assignee_masked)]\n",
    "\n",
    "task_assignees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping data by each assignee\n",
    "# Through doing this, we saw that assignee 0 and the expert proofreaders did not complete all 90 tasks.Next step is to filter out assignee 0 and the expert proofreaders.\n",
    "\n",
    "grouped_by_assignees = data.groupby('assignee_masked')\n",
    "assignees_tasks = pd.DataFrame(columns = ['assignee_name', 'tasks', 'num_tasks'])\n",
    "\n",
    "for group_name, group in grouped_by_assignees:\n",
    "    assignees_tasks.loc[len(assignees_tasks.index)] = [group_name, list(grouped_by_assignees.get_group(\n",
    "        group_name).unique_task_id), len(list(grouped_by_assignees.get_group(group_name).unique_task_id))]\n",
    "\n",
    "assignees_tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the above cell, we saw that assignee 0 and the expert proofreaders did not complete all 90 tasks.Next step is to filter out assignee 0 and the expert proofreaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Filtering out all asignees that do not have the maximum number of tasks completed\n",
    "# should filter out assignee 0 and expert proofreaders (100,101,102,103,104,105,106)\n",
    "assignees_to_keep = list(assignees_tasks[assignees_tasks.num_tasks == assignees_tasks.num_tasks.max()].assignee_name)\n",
    "data = data[data['assignee_masked'].isin(assignees_to_keep)]\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into 3 sets of equal number of tasks, chronologically\n",
    "num_tasks = len(pd.unique(data.unique_task_id))\n",
    "\n",
    "tasks_for_set_1 = list(pd.unique(data.unique_task_id))[0:int(num_tasks/3)]\n",
    "tasks_for_set_2 = list(pd.unique(data.unique_task_id))[int(num_tasks/3): 2* int(num_tasks/3)]\n",
    "tasks_for_set_3 = list(pd.unique(data.unique_task_id))[2 * int(num_tasks/3):]\n",
    "\n",
    "data_set_1_of_3 = data[data['unique_task_id'].isin(tasks_for_set_1)]\n",
    "data_set_2_of_3 = data[data['unique_task_id'].isin(tasks_for_set_2)]\n",
    "data_set_3_of_3 = data[data['unique_task_id'].isin(tasks_for_set_3)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the full data df and the equi-length chronologically split dfs to CSVs\n",
    "\n",
    "data.to_csv('./filtered_data/dataset_full_filteredasignees.csv')\n",
    "data_set_1_of_3.to_csv('./filtered_data/dataset_1_of_3_filteredasignees.csv')\n",
    "data_set_2_of_3.to_csv('./filtered_data/dataset_2_of_3_filteredasignees.csv')\n",
    "data_set_3_of_3.to_csv('./filtered_data/dataset_3_of_3_filteredasignees.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
