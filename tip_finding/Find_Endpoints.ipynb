{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloudvolume import CloudVolume\n",
    "from meshparty import skeletonize, trimesh_io\n",
    "from caveclient import CAVEclient\n",
    "import trimesh\n",
    "import numpy as np\n",
    "import datetime\n",
    "import networkx as nx\n",
    "from scipy.sparse import identity\n",
    "from scipy.spatial import distance_matrix\n",
    "import scipy \n",
    "from tqdm import tqdm\n",
    "# import aws\n",
    "import pandas as pd\n",
    "import pyembree\n",
    "#import aws.sqs as sqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_process_mesh(root_id):\n",
    "    datastack_name = \"minnie65_phase3_v1\"\n",
    "    client = CAVEclient(datastack_name)\n",
    "    vol = CloudVolume(\n",
    "        client.info.segmentation_source(),\n",
    "        use_https=True,\n",
    "        progress=False,\n",
    "        bounded=False,\n",
    "        fill_missing=True,\n",
    "        secrets={\"token\": client.auth.token}\n",
    "    )\n",
    "    print(\"Downloading Mesh\")\n",
    "    mesh = vol.mesh.get(str(root_id))[root_id]\n",
    "    mesh_obj = trimesh.Trimesh(np.divide(mesh.vertices, np.array([1,1,1])), mesh.faces)\n",
    "    print(\"Vertices: \", mesh.vertices.shape[0])\n",
    "\n",
    "    if mesh_obj.volume > 4000000000000:\n",
    "        print(\"TOO BIG, SKIPPING\")\n",
    "        #queue_url_endpoints = sqs.get_or_create_queue(\"root_ids_functional_dlqueue\")\n",
    "\n",
    "        #entries=sqs.construct_rootid_entries([root_id])\n",
    "\n",
    "        #sqs.send_batch(queue_url_endpoints, entries)\n",
    "\n",
    "        return None\n",
    "    trimesh.repair.fix_normals(mesh_obj)\n",
    "    mesh_obj.fill_holes()\n",
    "\n",
    "    return mesh_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soma(soma_id:str):\n",
    "    cave_client = CAVEclient('minnie65_phase3_v1')\n",
    "    soma = cave_client.materialize.query_table(\n",
    "        \"nucleus_neuron_svm\",\n",
    "        filter_equal_dict={'id':soma_id}\n",
    "    )\n",
    "    return soma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mesh_ccs(mesh_obj):\n",
    "    print(\"Processing CC's\")\n",
    "    ccs_graph = trimesh.graph.connected_components(mesh_obj.edges)\n",
    "    ccs_len = [len(c) for c in ccs_graph]\n",
    "\n",
    "    # Subselect the parts of the mesh that are not inside one another \n",
    "    # the other components are an artifact of the soma seg and small unfilled sections\n",
    "    largest_component = ccs_graph[np.argmax(ccs_len)]\n",
    "    largest_component_remap = np.arange(ccs_graph[np.argmax(ccs_len)].shape[0])\n",
    "    face_dict = {largest_component[i]:largest_component_remap[i] for i in range(largest_component.shape[0])}\n",
    "\n",
    "    new_faces_mask = np.isin(mesh_obj.faces, list(face_dict.keys()))\n",
    "    new_faces_mask = new_faces_mask[:, 0]*new_faces_mask[:, 1]*new_faces_mask[:, 2]\n",
    "\n",
    "    new_faces = np.vectorize(face_dict.get)(mesh_obj.faces[new_faces_mask])\n",
    "    new_faces = new_faces[new_faces[:, 0] != None]\n",
    "    largest_component_mesh = trimesh.Trimesh(mesh_obj.vertices[largest_component], new_faces)\n",
    "\n",
    "    all_ids = set(largest_component)\n",
    "    encapsulated_ids = []\n",
    "\n",
    "    for i in range(1, len(ccs_graph)):\n",
    "        n_con = largest_component_mesh.contains(mesh_obj.vertices[ccs_graph[i]])\n",
    "        if np.sum(n_con) / n_con.shape[0] == 0 and n_con.shape[0] > 50:\n",
    "            all_ids.update(ccs_graph[i])\n",
    "        else:\n",
    "            if len(ccs_graph[i]) < 1000:\n",
    "                encapsulated_ids.append((np.mean(mesh_obj.vertices[ccs_graph[i]], axis=0)/[4,4,40], len(ccs_graph[i])))\n",
    "            \n",
    "    all_component = np.array(list(ccs_graph[np.argmax(ccs_len)]))\n",
    "    all_component_remap = np.arange(all_component.shape[0])\n",
    "    face_dict = {all_component[i]:all_component_remap[i] for i in range(all_component.shape[0])}\n",
    "    new_faces_mask = np.isin(mesh_obj.faces, list(face_dict.keys()))\n",
    "    new_faces_mask = new_faces_mask[:, 0]*new_faces_mask[:, 1]*new_faces_mask[:, 2]\n",
    "\n",
    "    new_faces = np.vectorize(face_dict.get)(mesh_obj.faces[new_faces_mask])\n",
    "    new_faces[new_faces[:, 0] != None]\n",
    "    \n",
    "    largest_component_mesh = trimesh.Trimesh(mesh_obj.vertices[all_component], new_faces)\n",
    "\n",
    "    mesh_obj = largest_component_mesh\n",
    "    return mesh_obj, encapsulated_ids, np.max(ccs_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_defects(mesh_obj, a=.75):\n",
    "    bad_edges = trimesh.grouping.group_rows(\n",
    "        mesh_obj.edges_sorted, require_count=1)\n",
    "    bad_edges_ind = mesh_obj.edges[bad_edges]\n",
    "    sparse_edges = mesh_obj.edges_sparse\n",
    "    xs = list(bad_edges_ind[:, 0]) + list(bad_edges_ind[:, 1]) \n",
    "    ys = list(bad_edges_ind[:, 1]) + list(bad_edges_ind[:, 0])\n",
    "    vs = [1]*bad_edges_ind.shape[0]*2\n",
    "    bad_inds = scipy.sparse.coo_matrix((vs, (xs, ys)), shape=(mesh_obj.vertices.shape[0], mesh_obj.vertices.shape[0]))\n",
    "    # Make it symmetrical and add identity so each integrates from itself too, then subtract singleton edges\n",
    "    # I noticed that the number of asymmetrical edges vs the number of single edges I find from group rows\n",
    "    # Are close but different. Haven't looked into that yet. Also removing edges 1 hop away from single edges to remove bias towards\n",
    "    # Holes in the mesh that are caused by mesh construction errors as opposed to segmentation errors\n",
    "    sparse_edges = mesh_obj.edges_sparse + mesh_obj.edges_sparse.T + identity(mesh_obj.edges_sparse.shape[0]) - sparse_edges.multiply(bad_inds) - bad_inds\n",
    "    degs = mesh_obj.vertex_degree + 1\n",
    "\n",
    "    # N_iter is a smoothing parameter here. The loop below smooths the vertex error about the mesh to get more consistent connected regions\n",
    "    n_iter = 2\n",
    "    angle_sum = np.array(abs(mesh_obj.face_angles_sparse).sum(axis=1)).flatten()\n",
    "    defs = (2 * np.pi) - angle_sum\n",
    "\n",
    "    abs_defs = np.abs(defs)\n",
    "    abs_defs_i = abs_defs.copy()\n",
    "    for i in range(n_iter):\n",
    "        abs_defs_i = sparse_edges.dot(abs_defs_i) / degs\n",
    "    \n",
    "    verts_select = np.argwhere((abs_defs_i > a))# & (abs_defs < 2.5))\n",
    "\n",
    "    edges_mask = np.isin(mesh_obj.edges, verts_select)\n",
    "    edges_mask[bad_edges] = False\n",
    "    edges_select = edges_mask[:, 0] * edges_mask[:, 1]\n",
    "    edges_select = mesh_obj.edges[edges_select]\n",
    "\n",
    "    G = nx.from_edgelist(edges_select)#f_edge_sub)\n",
    "\n",
    "    ccs = nx.connected_components(G)\n",
    "    subgraphs = [G.subgraph(cc).copy() for cc in ccs]\n",
    "\n",
    "    lens = []\n",
    "    lengths = []\n",
    "    for i in tqdm(range(len(subgraphs))):\n",
    "        ns = np.array(list(subgraphs[i].nodes()))\n",
    "    #     ns = ns[abs_defs[ns ]]\n",
    "        l = len(ns)\n",
    "        if l > 20 and l < 5000:\n",
    "            lens.append(ns)\n",
    "            lengths.append(l)\n",
    "    all_nodes = set()\n",
    "    for l in lens:\n",
    "        all_nodes.update(l)\n",
    "    all_nodes = np.array(list(all_nodes))\n",
    "    # sharp_pts = mesh_obj.vertices[all_nodes]\n",
    "    centers = np.array([np.mean(mesh_obj.vertices[list(ppts)],axis=0) for ppts in lens])\n",
    "\n",
    "    return centers, lens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_endpoints(mesh_obj, skel_mp):\n",
    "    # Process the skeleton to get the endpoints\n",
    "    interior_cc_mask = set()\n",
    "    el = nx.from_edgelist(skel_mp.edges)\n",
    "    comps = list(nx.connected_components(el))\n",
    "    for c in comps:\n",
    "        if len(c) < 10000:\n",
    "            n_con = mesh_obj.contains(skel_mp.vertices[list(c)])\n",
    "            if np.sum(n_con) / n_con.shape[0] > .05:\n",
    "                interior_cc_mask.update(list(c))\n",
    "    # Process the skeleton to get the endpoints\n",
    "    edges = skel_mp.edges.copy()\n",
    "\n",
    "    edge_mask = ~np.isin(edges, interior_cc_mask)\n",
    "    edge_mask = edge_mask[:, 0] + edge_mask[:, 1]\n",
    "    edges = edges[edge_mask]\n",
    "    edges_flat  = edges.flatten()\n",
    "    edge_bins = np.bincount(edges_flat) \n",
    "\n",
    "    eps = np.squeeze(np.argwhere(edge_bins==1))\n",
    "    eps_nm = skel_mp.vertices[eps]\n",
    "\n",
    "    eps_comp = distance_matrix(eps_nm, eps_nm)\n",
    "    eps_comp[eps_comp == 0] = np.inf\n",
    "    eps_thresh = np.argwhere(~(np.min(eps_comp, axis=0) < 3000))\n",
    "\n",
    "    eps = np.squeeze(eps[eps_thresh])\n",
    "    eps_nm = np.squeeze(eps_nm[eps_thresh])\n",
    "    return eps, eps_nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mesh_errors(mesh_obj, centers, eps, eps_nm, lens, skel_mp):\n",
    "    print(\"Processing mesh errors\")\n",
    "    path_to_root_dict = {}\n",
    "    for ep in eps:\n",
    "        path_to_root_dict[ep] = skel_mp.path_to_root(ep)\n",
    "        \n",
    "    dists_defects = np.zeros(centers.shape[0])\n",
    "    sizes = np.zeros(centers.shape[0])\n",
    "    mesh_map = skel_mp.mesh_to_skel_map\n",
    "    closest_skel_pts = mesh_map[[l[0] for l in lens]]\n",
    "\n",
    "    # print(centers, eps_nm)\n",
    "\n",
    "    dist_matrix = distance_matrix(centers, eps_nm)\n",
    "    ct = 0\n",
    "\n",
    "    closest_tip = np.zeros((centers.shape[0]))\n",
    "\n",
    "    for center in tqdm(centers):\n",
    "    #     skel_pts_dists = np.linalg.norm(skel_mp.vertices - center, axis=1)\n",
    "    #     ep_pts_dists = np.linalg.norm(eps_nm - center, axis=1)\n",
    "        \n",
    "        closest_skel_pt = closest_skel_pts[ct]\n",
    "        min_ep = np.inf\n",
    "        eps_hit = []\n",
    "        for j, ep in enumerate(eps):\n",
    "            if closest_skel_pt in path_to_root_dict[ep]:\n",
    "                eps_hit.append(j)\n",
    "        if len(eps_hit) == 0:\n",
    "            dists_defects[ct] = np.inf\n",
    "            sizes[ct] = np.inf\n",
    "            ct+=1\n",
    "            continue\n",
    "        \n",
    "        dists = dist_matrix[ct, eps_hit]\n",
    "    #     print(dists, eps_hit, center / [4,4,40])\n",
    "        \n",
    "        amin = np.argmin(dists)\n",
    "        tip_hit = eps_hit[amin]\n",
    "        min_dist = dists[amin]\n",
    "        \n",
    "        closest_tip[ct] = tip_hit\n",
    "    #     print(np.argmin(ep_pts_dists), ep_found, eps_nm[np.argmin(ep_pts_dists)]/[4,4,40], eps_nm[j]/[4,4,40], center/[4,4,40])\n",
    "        dists_defects[ct] = min_dist\n",
    "        sizes[ct] = len(lens[ct])\n",
    "        ct+=1\n",
    "    dists_defects_sub = dists_defects[dists_defects < np.inf]\n",
    "    sizes_sub = sizes[dists_defects < np.inf]\n",
    "    centers_sub = centers[dists_defects < np.inf]\n",
    "    tips_hit_sub = closest_tip[dists_defects < np.inf]\n",
    "    closest_skel_pts_sub = closest_skel_pts[dists_defects < np.inf]\n",
    "    inds_sub = np.arange(centers.shape[0])[dists_defects < np.inf]\n",
    "\n",
    "\n",
    "    # Also ranking each component based on its PCA- if the first component is big enough, the points are mostly linear\n",
    "    # These point sets seem to be less likely to be true errors\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca_vec = np.zeros(inds_sub.shape[0])\n",
    "    for i in range(inds_sub.shape[0]):\n",
    "        pca = PCA()#n_components=2)\n",
    "        pca.fit(mesh_obj.vertices[lens[inds_sub[i]]])\n",
    "\n",
    "        pca_vec[i] = pca.explained_variance_ratio_[0]\n",
    "\n",
    "    dists_defects_sub[dists_defects_sub < 4000] = 100\n",
    "    dists_defects_norm = dists_defects_sub #/ np.max(dists_defects_sub)\n",
    "    ranks_ep = sizes_sub / dists_defects_norm * (1-pca_vec)\n",
    "    ranks = sizes_sub**2 * (1-pca_vec)\n",
    "\n",
    "    #ranks_ep_errors_filt = ranks_ep[ranks_ep > .1]\n",
    "    centers_ep_send_errors = centers_sub[np.argsort(ranks_ep)][::-1][:20]\n",
    "    final_mask_eps = np.full(centers_ep_send_errors.shape[0], True)\n",
    "    tips_hit_send_ep = tips_hit_sub[np.argsort(ranks_ep)][::-1][:20]\n",
    "    uns, nums = np.unique(tips_hit_send_ep, return_counts=True)\n",
    "\n",
    "    for un, num in zip(uns, nums):\n",
    "        if num > 1:\n",
    "            final_mask_eps[np.argwhere(tips_hit_send_ep == un)[1:]] = False\n",
    "    centers_errors_ep = centers_ep_send_errors[final_mask_eps]\n",
    "    centers_errors = centers_sub[np.argsort(ranks)[::-1]][:20]\n",
    "    return centers_errors, centers_errors_ep, ranks, ranks_ep, path_to_root_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mesh_facets(mesh_obj, skel_mp, eps, path_to_root_dict, eps_nm):\n",
    "    print(\"Processing facets\")\n",
    "    locs = np.argwhere(mesh_obj.facets_area > 50000)\n",
    "\n",
    "    mesh_map = skel_mp.mesh_to_skel_map\n",
    "    mesh_coords = mesh_obj.vertices[mesh_obj.faces]\n",
    "    mean_locs = []\n",
    "    mesh_ind = []\n",
    "    fs = []\n",
    "    for l in tqdm(locs):\n",
    "        fs.append(np.sum(mesh_obj.facets_area[l]))\n",
    "        fc = mesh_obj.facets[l[0]]\n",
    "        vert_locs = mesh_coords[fc]\n",
    "        mean_locs.append(np.mean(vert_locs[:, 0], axis=0))\n",
    "        mesh_ind.append(fc[0])\n",
    "    mesh_ind = mesh_obj.faces[mesh_ind][:, 0]\n",
    "    mean_locs = np.array(mean_locs)\n",
    "    dists_defects_facets = np.zeros(mean_locs.shape[0])\n",
    "    mesh_map_facets = skel_mp.mesh_to_skel_map\n",
    "    closest_skel_pts_facets = mesh_map[[m for m in mesh_ind]]\n",
    "    dist_matrix_facets = distance_matrix(mean_locs, eps_nm)\n",
    "    ct = 0\n",
    "\n",
    "    closest_tip_facets = np.zeros((mean_locs.shape[0]))\n",
    "\n",
    "    for center in tqdm(mean_locs):\n",
    "\n",
    "        closest_skel_pt = closest_skel_pts_facets[ct]\n",
    "        eps_hit = []\n",
    "        for j, ep in enumerate(eps):\n",
    "            if closest_skel_pt in path_to_root_dict[ep]:\n",
    "                eps_hit.append(j)\n",
    "        if len(eps_hit) == 0:\n",
    "            dists_defects_facets[ct] = np.inf\n",
    "            ct+=1\n",
    "            continue\n",
    "        \n",
    "        dists = dist_matrix_facets[ct, eps_hit]\n",
    "        \n",
    "        amin = np.argmin(dists)\n",
    "        tip_hit = eps_hit[amin]\n",
    "        min_dist = dists[amin]\n",
    "        \n",
    "        closest_tip_facets[ct] = tip_hit\n",
    "        dists_defects_facets[ct] = min_dist\n",
    "        ct+=1\n",
    "    dists_defects_sub_facets = dists_defects_facets[dists_defects_facets < np.inf]\n",
    "    sizes_sub_facets = np.array(fs)[dists_defects_facets < np.inf]\n",
    "    mean_locs_facets = mean_locs[dists_defects_facets < np.inf]\n",
    "    tips_hit_sub_facets = closest_tip_facets[dists_defects_facets < np.inf]\n",
    "    closest_skel_pts_sub_facets = closest_skel_pts_facets[dists_defects_facets < np.inf]\n",
    "    inds_sub_facets = np.arange(mean_locs.shape[0])[dists_defects_facets < np.inf]\n",
    "    ranks_ep_facets = sizes_sub_facets**2 / dists_defects_sub_facets\n",
    "    #ranks_ep_facets_filt = ranks_ep_facets[ranks_ep_facets > 2e7]\n",
    "    mean_locs_send_facets = mean_locs_facets[np.argsort(ranks_ep_facets)][::-1][:20]\n",
    "    final_mask_facets = np.full(mean_locs_send_facets.shape[0], True)\n",
    "    tips_hit_send_facets = tips_hit_sub_facets[np.argsort(ranks_ep_facets)][::-1][:20]\n",
    "    uns, nums = np.unique(tips_hit_send_facets, return_counts=True)\n",
    "\n",
    "    for un, num in zip(uns, nums):\n",
    "        if num > 1:\n",
    "            final_mask_facets[np.argwhere(tips_hit_send_facets == un)[1:]] = False\n",
    "    facets_send_final = mean_locs_send_facets[final_mask_facets] / [4,4,40]\n",
    "    return facets_send_final, ranks_ep_facets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TIP FINDING FUNCTION\n",
    "def error_locs_defects(root_id, soma_id = None, soma_table=None, center_collapse=True):\n",
    "    #print(\"START\", root_id)\n",
    "\n",
    "    mesh_obj = get_and_process_mesh(root_id)\n",
    "    if mesh_obj is None:\n",
    "        return None\n",
    "    # SKELETONIZE - if we are just looking for general errors, not errors at endpoints, this can be skipped\n",
    "    try:\n",
    "        if soma_table==None:\n",
    "            soma_table = get_soma(str(soma_id))\n",
    "        if soma_table[soma_table.id == soma_id].shape[0] > 0:\n",
    "            center = np.array(soma_table[soma_table.id == soma_id].pt_position)[0] * [4,4,40]\n",
    "        else:\n",
    "            center=None\n",
    "    except:\n",
    "        center = None\n",
    "    print(\"Subselecting largest connected component of mesh\")\n",
    "    mesh_obj, encapsulated_ids, max_verts = process_mesh_ccs(mesh_obj)\n",
    "\n",
    "    skel_mp = skeletonize.skeletonize_mesh(trimesh_io.Mesh(mesh_obj.vertices, \n",
    "                                            mesh_obj.faces),\n",
    "                                            invalidation_d=15000,\n",
    "                                            shape_function='cone',\n",
    "                                            collapse_function='branch',\n",
    "#                                             soma_radius = soma_radius,\n",
    "                                            soma_pt=center,\n",
    "                                            smooth_neighborhood=5,\n",
    "                                            cc_vertex_thresh=max_verts - 10\n",
    "#                                                     collapse_params = {'dynamic_threshold':True}\n",
    "                                            )\n",
    "    print(\"Skel done\")\n",
    "\n",
    "    # find edges that only occur once..  might be faster to find these in the sparse matrix..\n",
    "    centers, lens = process_defects(mesh_obj)\n",
    "    eps, eps_nm = process_endpoints(mesh_obj, skel_mp)\n",
    "\n",
    "    if len(centers) !=0:\n",
    "        centers_errors, centers_errors_ep, ranks, ranks_ep, path_to_root_dict = process_mesh_errors(mesh_obj, centers, eps, eps_nm, lens, skel_mp)\n",
    "        ranks_return = np.squeeze(ranks[np.argsort(ranks)[::-1]][:20])\n",
    "        ranks_ep_return = np.squeeze(ranks_ep[np.argsort(ranks_ep)][::-1][:20])\n",
    "    else:\n",
    "        # Assign placeholder values for each of the variables above.\n",
    "        centers_errors = np.zeros ((1,3))\n",
    "        centers_errors_ep = np.zeros ((1,3))\n",
    "        ranks = np.zeros ((1))\n",
    "        ranks_ep = np.zeros((1, 3))\n",
    "        path_to_root_dict = {}\n",
    "        for ep in eps:\n",
    "            path_to_root_dict[ep] = skel_mp.path_to_root(ep)\n",
    "\n",
    "        ranks_return = 0\n",
    "        ranks_ep_return = 0\n",
    "\n",
    "\n",
    "    #if len(centers_errors.shape) > 1 and centers_errors.shape[0] > 0 and len(centers_errors_ep.shape) > 1 and len(centers_errors_ep.shape[0] > 0):\n",
    "    #    centers_errors = centers_errors[np.min(distance_matrix(centers_errors, centers_errors_ep), axis=1)>1000]\n",
    "    facets_send_final, ranks_ep_facets = process_mesh_facets(mesh_obj, skel_mp, eps, path_to_root_dict, eps_nm)\n",
    "\n",
    "    errors_send = centers_errors / [4,4,40]\n",
    "    errors_tips_send = centers_errors_ep / [4,4,40]\n",
    "    encapsulated_centers = [e[0] for e in encapsulated_ids]\n",
    "    encapsulated_lens = [e[1] for e in encapsulated_ids]\n",
    "    sorted_encapsulated_send = np.array(encapsulated_centers)[np.argsort(encapsulated_lens)][::-1]\n",
    "\n",
    "\n",
    "\n",
    "    return sorted_encapsulated_send, facets_send_final, errors_send, errors_tips_send, np.squeeze(ranks_ep_facets[[np.argsort(ranks_ep_facets)][::-1][:20]]), ranks_return, ranks_ep_return\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASSUMING output from error_locs IS: \n",
    "facets_find_final: [[x,y,z], [x2,y2,z2], etc]\n",
    "errors_tips_send: [[x,y,z], [x2,y2,z2], etc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "points = [[1,2,3], [4,5,6]]\n",
    "for x in points:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n"
     ]
    }
   ],
   "source": [
    "points.append([7,8,9])\n",
    "print(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0], [3, 4, 5]]\n"
     ]
    }
   ],
   "source": [
    "array = [[0,0,0]]\n",
    "arr2 = [3,4,5]\n",
    "array.append(arr2)\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   col1  col2\n",
      "0     1     3\n",
      "1     2     4\n",
      "   col1  col2        empty\n",
      "0     1     3  [[0, 0, 0]]\n",
      "1     2     4  [[0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "d = {'col1': [1, 2], 'col2': [3, 4]}\n",
    "df = pd.DataFrame(data=d)\n",
    "print(df)\n",
    "df[\"empty\"] = df[\"col1\"].map(lambda x: [[0,0,0]])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_endpoints(dataframe) :\n",
    "    #assume I get in a dataframe with orphan(seg_ID), num endpoints, labelled endpoints\n",
    "    \n",
    "    #add collumn to df for predicted endpoints, add an empty array here\n",
    "    #dataframe[\"endpoints_generated\"] = [[0,0,0]]\n",
    "    dataframe[\"endpoints_generated\"] = dataframe[\"seg_id\"].map(lambda x: [[0,0,0]])\n",
    "    #first we need to iterate through the given dataframe row by row\n",
    "    for index, row in dataframe.iterrows(): \n",
    "        #get the neuron/orphan seg_ID\n",
    "        seg_id = row[\"seg_id\"]\n",
    "        #pass the seg_id into the generating function\n",
    "        sorted_encapsulated_send, facets_send_final, errors_send, errors_tips_send = error_locs_defects(seg_id)\n",
    "        #add the predicted endpoints into the dataframe row\n",
    "        for endpoint in facets_send_final:\n",
    "            row[\"endpoints_generated\"].append(endpoint)\n",
    "        for endpoints in errors_tips_send:\n",
    "            row[\"endpoints_generated\"].append(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Mesh\n",
      "Warning: deduplication not currently supported for this layer's variable layered draco meshes\n",
      "Vertices:  7993\n",
      "Subselecting largest connected component of mesh\n",
      "Processing CC's\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7992/7992 [00:00<00:00, 522425.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skel done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 85800.08it/s]\n",
      "Mesh is non-watertight for contained point query!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing facets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 3123.47it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 6212.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([], dtype=float64),\n",
       " array([[402970.10915493, 229214.63028169,  23552.55      ],\n",
       "        [402527.5625    , 228866.3125    ,  23991.45      ]]),\n",
       " array([[0., 0., 0.]]),\n",
       " array([[0., 0., 0.]]),\n",
       " array([3.91037449e+05, 2.16854490e+06, 2.23747224e+06, 2.07221305e+07,\n",
       "        5.31893622e+08, 1.50108566e+09]),\n",
       " 0,\n",
       " 0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example to see outputs\n",
    "error_locs_defects(864691135909994000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.73205081 1.73205081 1.73205081 1.73205081 1.73205081 1.73205081\n",
      "  1.73205081 1.73205081 1.73205081 1.73205081]\n",
      " [1.73205081 1.73205081 1.73205081 1.73205081 1.73205081 1.73205081\n",
      "  1.73205081 1.73205081 1.73205081 1.73205081]\n",
      " [1.73205081 1.73205081 1.73205081 1.73205081 1.73205081 1.73205081\n",
      "  1.73205081 1.73205081 1.73205081 1.73205081]\n",
      " [1.73205081 1.73205081 1.73205081 1.73205081 1.73205081 1.73205081\n",
      "  1.73205081 1.73205081 1.73205081 1.73205081]\n",
      " [1.73205081 1.73205081 1.73205081 1.73205081 1.73205081 1.73205081\n",
      "  1.73205081 1.73205081 1.73205081 1.73205081]\n",
      " [1.73205081 1.73205081 1.73205081 1.73205081 1.73205081 1.73205081\n",
      "  1.73205081 1.73205081 1.73205081 1.73205081]\n",
      " [1.73205081 1.73205081 1.73205081 1.73205081 1.73205081 1.73205081\n",
      "  1.73205081 1.73205081 1.73205081 1.73205081]\n",
      " [1.73205081 1.73205081 1.73205081 1.73205081 1.73205081 1.73205081\n",
      "  1.73205081 1.73205081 1.73205081 1.73205081]\n",
      " [1.73205081 1.73205081 1.73205081 1.73205081 1.73205081 1.73205081\n",
      "  1.73205081 1.73205081 1.73205081 1.73205081]\n",
      " [1.73205081 1.73205081 1.73205081 1.73205081 1.73205081 1.73205081\n",
      "  1.73205081 1.73205081 1.73205081 1.73205081]]\n"
     ]
    }
   ],
   "source": [
    "# Debugging\n",
    "\n",
    "x = np.zeros((10,3))\n",
    "y = np.ones((10,3))\n",
    "\n",
    "dist_matrix = distance_matrix(x,y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
