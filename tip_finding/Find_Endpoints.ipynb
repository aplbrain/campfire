{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloudvolume import CloudVolume\n",
    "from meshparty import skeletonize, trimesh_io\n",
    "from caveclient import CAVEclient\n",
    "import trimesh\n",
    "import numpy as np\n",
    "import datetime\n",
    "import networkx as nx\n",
    "from scipy.sparse import identity\n",
    "from scipy.spatial import distance_matrix\n",
    "import scipy \n",
    "from tqdm import tqdm\n",
    "# import aws\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pyembree\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.spatial as spatial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATAFRAME FOR NEURONS AND ORPHANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 seg_id  num_endpoints  \\\n",
      "0    864691135909994000              2   \n",
      "1    864691135247440303              3   \n",
      "2    864691134794123793              2   \n",
      "3    864691135772363453              7   \n",
      "4    864691135314714227              2   \n",
      "..                  ...            ...   \n",
      "127  864691135319600870              2   \n",
      "128  864691136558839249              2   \n",
      "129  864691135012763638             -1   \n",
      "130  864691135458639120              2   \n",
      "131  864691135826764827              2   \n",
      "\n",
      "                                             endpoints  \n",
      "0     (402584, 228856, 23991); (402985, 229235, 23554)  \n",
      "1    (401612, 224623, 23991); (405257, 226318, 2362...  \n",
      "2     (401242, 228382, 24444); (400982, 228457, 24513)  \n",
      "3    (401289, 218721, 23991); (399895, 216533, 2352...  \n",
      "4    (397867, 232230, 23999);  (397854, 232252, 23997)  \n",
      "..                                                 ...  \n",
      "127     (106608, 111803, 20796); (98290, 97555, 21045)  \n",
      "128    (103138, 188530, 21200); (91780, 189141, 20879)  \n",
      "129                                                 ()  \n",
      "130   (127475, 206611, 21525); (128120, 207267, 21512)  \n",
      "131   (122293, 186204, 19621); (124943, 185661, 18677)  \n",
      "\n",
      "[132 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "orphans = pd.read_csv(\"/Users/sheeltanna/Desktop/AGT_REPO/campfire/tip_finding/Orphans.csv\")\n",
    "print(orphans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = pd.read_csv(\"/Users/sheeltanna/Desktop/AGT_REPO/campfire/tip_finding/Neuron.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_array(x):\n",
    "    res = list(map(str.strip, x.split('; ')))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 seg_id  num_endpoints  \\\n",
      "0    864691135909994000              2   \n",
      "1    864691135247440303              3   \n",
      "2    864691134794123793              2   \n",
      "3    864691135772363453              7   \n",
      "4    864691135314714227              2   \n",
      "..                  ...            ...   \n",
      "127  864691135319600870              2   \n",
      "128  864691136558839249              2   \n",
      "129  864691135012763638             -1   \n",
      "130  864691135458639120              2   \n",
      "131  864691135826764827              2   \n",
      "\n",
      "                                             endpoints  \n",
      "0    [(402584, 228856, 23991), (402985, 229235, 235...  \n",
      "1    [(401612, 224623, 23991), (405257, 226318, 236...  \n",
      "2    [(401242, 228382, 24444), (400982, 228457, 245...  \n",
      "3    [(401289, 218721, 23991), (399895, 216533, 235...  \n",
      "4    [(397867, 232230, 23999), (397854, 232252, 239...  \n",
      "..                                                 ...  \n",
      "127   [(106608, 111803, 20796), (98290, 97555, 21045)]  \n",
      "128  [(103138, 188530, 21200), (91780, 189141, 20879)]  \n",
      "129                                               [()]  \n",
      "130  [(127475, 206611, 21525), (128120, 207267, 215...  \n",
      "131  [(122293, 186204, 19621), (124943, 185661, 186...  \n",
      "\n",
      "[132 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "orphans['endpoints'] = orphans['endpoints'].map(lambda x: list(map(str.strip, x.split('; '))))\n",
    "print(orphans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Neurons  num_endpoints  \\\n",
      "0   864691136577830164              6   \n",
      "1   864691135683615218             13   \n",
      "2   864691135864989916              2   \n",
      "3   864691135837578899              9   \n",
      "4   864691135501859650             14   \n",
      "5   864691135544674088              7   \n",
      "6   864691135614368971             12   \n",
      "7   864691135334773481              7   \n",
      "8   864691135614361291             12   \n",
      "9   864691135445907602             12   \n",
      "10  864691136390697343              8   \n",
      "11  864691135396741921              9   \n",
      "12  864691135360418631              6   \n",
      "13  864691135463797061              6   \n",
      "14  864691135526309723              9   \n",
      "\n",
      "                                            endpoints  \n",
      "0   [(402188, 228684, 24029), (401258, 224832, 240...  \n",
      "1   [(77233, 113188, 20454), (100382, 143836, 2164...  \n",
      "2   [(211973, 200313, 23233), (167751, 211623, 207...  \n",
      "3   [(351264, 143646, 15182), (344801, 142291, 169...  \n",
      "4   [(101840, 247377, 19543), (92823, 255328, 2038...  \n",
      "5   [(220484, 239044, 20481), (238449, 242308, 224...  \n",
      "6   [(107820, 233757, 21960), (116372, 239008, 232...  \n",
      "7   [(138311, 218409, 26379), (114789, 194406, 242...  \n",
      "8   [(123240, 178472, 21529), (121863, 182085, 215...  \n",
      "9   [(293401, 150411, 19104), (313432, 160982, 185...  \n",
      "10  [(142469, 252585, 22976), (114049, 233747, 220...  \n",
      "11  [(346513, 82659, 27098), (361886, 82055, 27634...  \n",
      "12  [(110598, 239837, 22040), (108642, 243230, 219...  \n",
      "13  [(103052, 175611, 18430), (104081, 174998, 184...  \n",
      "14  [(108958, 194091, 22035), (107125, 175763, 219...  \n"
     ]
    }
   ],
   "source": [
    "neurons['endpoints'] = neurons['endpoints'].map(lambda x: list(map(str.strip, x.split('; '))))\n",
    "print(neurons)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONS FOR TIP FINDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_process_mesh(root_id):\n",
    "    datastack_name = \"minnie65_phase3_v1\"\n",
    "    client = CAVEclient(datastack_name)\n",
    "    vol = CloudVolume(\n",
    "        client.info.segmentation_source(),\n",
    "        use_https=True,\n",
    "        progress=False,\n",
    "        bounded=False,\n",
    "        fill_missing=True,\n",
    "        secrets={\"token\": client.auth.token}\n",
    "    )\n",
    "    print(\"Downloading Mesh\")\n",
    "    mesh = vol.mesh.get(str(root_id))[root_id]\n",
    "    mesh_obj = trimesh.Trimesh(np.divide(mesh.vertices, np.array([1,1,1])), mesh.faces)\n",
    "    print(\"Vertices: \", mesh.vertices.shape[0])\n",
    "\n",
    "    if mesh_obj.volume > 4000000000000:\n",
    "        print(\"TOO BIG, SKIPPING\")\n",
    "        #queue_url_endpoints = sqs.get_or_create_queue(\"root_ids_functional_dlqueue\")\n",
    "\n",
    "        #entries=sqs.construct_rootid_entries([root_id])\n",
    "\n",
    "        #sqs.send_batch(queue_url_endpoints, entries)\n",
    "\n",
    "        return None\n",
    "    trimesh.repair.fix_normals(mesh_obj)\n",
    "    mesh_obj.fill_holes()\n",
    "\n",
    "    return mesh_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soma(soma_id:str):\n",
    "    cave_client = CAVEclient('minnie65_phase3_v1')\n",
    "    soma = cave_client.materialize.query_table(\n",
    "        \"nucleus_neuron_svm\",\n",
    "        filter_equal_dict={'id':soma_id}\n",
    "    )\n",
    "    return soma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mesh_ccs(mesh_obj):\n",
    "    print(\"Processing CC's\")\n",
    "    ccs_graph = trimesh.graph.connected_components(mesh_obj.edges)\n",
    "    ccs_len = [len(c) for c in ccs_graph]\n",
    "\n",
    "    # Subselect the parts of the mesh that are not inside one another \n",
    "    # the other components are an artifact of the soma seg and small unfilled sections\n",
    "    largest_component = ccs_graph[np.argmax(ccs_len)]\n",
    "    largest_component_remap = np.arange(ccs_graph[np.argmax(ccs_len)].shape[0])\n",
    "    face_dict = {largest_component[i]:largest_component_remap[i] for i in range(largest_component.shape[0])}\n",
    "\n",
    "    new_faces_mask = np.isin(mesh_obj.faces, list(face_dict.keys()))\n",
    "    new_faces_mask = new_faces_mask[:, 0]*new_faces_mask[:, 1]*new_faces_mask[:, 2]\n",
    "\n",
    "    new_faces = np.vectorize(face_dict.get)(mesh_obj.faces[new_faces_mask])\n",
    "    new_faces = new_faces[new_faces[:, 0] != None]\n",
    "    largest_component_mesh = trimesh.Trimesh(mesh_obj.vertices[largest_component], new_faces)\n",
    "\n",
    "    all_ids = set(largest_component)\n",
    "    encapsulated_ids = []\n",
    "\n",
    "    for i in range(1, len(ccs_graph)):\n",
    "        n_con = largest_component_mesh.contains(mesh_obj.vertices[ccs_graph[i]])\n",
    "        if np.sum(n_con) / n_con.shape[0] == 0 and n_con.shape[0] > 50:\n",
    "            all_ids.update(ccs_graph[i])\n",
    "        else:\n",
    "            if len(ccs_graph[i]) < 1000:\n",
    "                encapsulated_ids.append((np.mean(mesh_obj.vertices[ccs_graph[i]], axis=0)/[4,4,40], len(ccs_graph[i])))\n",
    "            \n",
    "    all_component = np.array(list(ccs_graph[np.argmax(ccs_len)]))\n",
    "    all_component_remap = np.arange(all_component.shape[0])\n",
    "    face_dict = {all_component[i]:all_component_remap[i] for i in range(all_component.shape[0])}\n",
    "    new_faces_mask = np.isin(mesh_obj.faces, list(face_dict.keys()))\n",
    "    new_faces_mask = new_faces_mask[:, 0]*new_faces_mask[:, 1]*new_faces_mask[:, 2]\n",
    "\n",
    "    new_faces = np.vectorize(face_dict.get)(mesh_obj.faces[new_faces_mask])\n",
    "    new_faces[new_faces[:, 0] != None]\n",
    "    \n",
    "    largest_component_mesh = trimesh.Trimesh(mesh_obj.vertices[all_component], new_faces)\n",
    "\n",
    "    mesh_obj = largest_component_mesh\n",
    "    return mesh_obj, encapsulated_ids, np.max(ccs_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_defects(mesh_obj, a=.75):\n",
    "    bad_edges = trimesh.grouping.group_rows(\n",
    "        mesh_obj.edges_sorted, require_count=1)\n",
    "    bad_edges_ind = mesh_obj.edges[bad_edges]\n",
    "    sparse_edges = mesh_obj.edges_sparse\n",
    "    xs = list(bad_edges_ind[:, 0]) + list(bad_edges_ind[:, 1]) \n",
    "    ys = list(bad_edges_ind[:, 1]) + list(bad_edges_ind[:, 0])\n",
    "    vs = [1]*bad_edges_ind.shape[0]*2\n",
    "    bad_inds = scipy.sparse.coo_matrix((vs, (xs, ys)), shape=(mesh_obj.vertices.shape[0], mesh_obj.vertices.shape[0]))\n",
    "    # Make it symmetrical and add identity so each integrates from itself too, then subtract singleton edges\n",
    "    # I noticed that the number of asymmetrical edges vs the number of single edges I find from group rows\n",
    "    # Are close but different. Haven't looked into that yet. Also removing edges 1 hop away from single edges to remove bias towards\n",
    "    # Holes in the mesh that are caused by mesh construction errors as opposed to segmentation errors\n",
    "    sparse_edges = mesh_obj.edges_sparse + mesh_obj.edges_sparse.T + identity(mesh_obj.edges_sparse.shape[0]) - sparse_edges.multiply(bad_inds) - bad_inds\n",
    "    degs = mesh_obj.vertex_degree + 1\n",
    "\n",
    "    # N_iter is a smoothing parameter here. The loop below smooths the vertex error about the mesh to get more consistent connected regions\n",
    "    n_iter = 2\n",
    "    angle_sum = np.array(abs(mesh_obj.face_angles_sparse).sum(axis=1)).flatten()\n",
    "    defs = (2 * np.pi) - angle_sum\n",
    "\n",
    "    abs_defs = np.abs(defs)\n",
    "    abs_defs_i = abs_defs.copy()\n",
    "    for i in range(n_iter):\n",
    "        abs_defs_i = sparse_edges.dot(abs_defs_i) / degs\n",
    "    \n",
    "    verts_select = np.argwhere((abs_defs_i > a))# & (abs_defs < 2.5))\n",
    "\n",
    "    edges_mask = np.isin(mesh_obj.edges, verts_select)\n",
    "    edges_mask[bad_edges] = False\n",
    "    edges_select = edges_mask[:, 0] * edges_mask[:, 1]\n",
    "    edges_select = mesh_obj.edges[edges_select]\n",
    "\n",
    "    G = nx.from_edgelist(edges_select)#f_edge_sub)\n",
    "\n",
    "    ccs = nx.connected_components(G)\n",
    "    subgraphs = [G.subgraph(cc).copy() for cc in ccs]\n",
    "\n",
    "    lens = []\n",
    "    lengths = []\n",
    "    for i in tqdm(range(len(subgraphs))):\n",
    "        ns = np.array(list(subgraphs[i].nodes()))\n",
    "    #     ns = ns[abs_defs[ns ]]\n",
    "        l = len(ns)\n",
    "        if l > 20 and l < 5000:\n",
    "            lens.append(ns)\n",
    "            lengths.append(l)\n",
    "    all_nodes = set()\n",
    "    for l in lens:\n",
    "        all_nodes.update(l)\n",
    "    all_nodes = np.array(list(all_nodes))\n",
    "    # sharp_pts = mesh_obj.vertices[all_nodes]\n",
    "    centers = np.array([np.mean(mesh_obj.vertices[list(ppts)],axis=0) for ppts in lens])\n",
    "\n",
    "    return centers, lens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_endpoints(mesh_obj, skel_mp):\n",
    "    # Process the skeleton to get the endpoints\n",
    "    interior_cc_mask = set()\n",
    "    el = nx.from_edgelist(skel_mp.edges)\n",
    "    comps = list(nx.connected_components(el))\n",
    "    for c in comps:\n",
    "        if len(c) < 10000:\n",
    "            n_con = mesh_obj.contains(skel_mp.vertices[list(c)])\n",
    "            if np.sum(n_con) / n_con.shape[0] > .05:\n",
    "                interior_cc_mask.update(list(c))\n",
    "    # Process the skeleton to get the endpoints\n",
    "    edges = skel_mp.edges.copy()\n",
    "\n",
    "    edge_mask = ~np.isin(edges, interior_cc_mask)\n",
    "    edge_mask = edge_mask[:, 0] + edge_mask[:, 1]\n",
    "    edges = edges[edge_mask]\n",
    "    edges_flat  = edges.flatten()\n",
    "    edge_bins = np.bincount(edges_flat) \n",
    "\n",
    "    eps = np.squeeze(np.argwhere(edge_bins==1))\n",
    "    eps_nm = skel_mp.vertices[eps]\n",
    "\n",
    "    eps_comp = distance_matrix(eps_nm, eps_nm)\n",
    "    eps_comp[eps_comp == 0] = np.inf\n",
    "    eps_thresh = np.argwhere(~(np.min(eps_comp, axis=0) < 3000))\n",
    "\n",
    "    eps = np.squeeze(eps[eps_thresh])\n",
    "    eps_nm = np.squeeze(eps_nm[eps_thresh])\n",
    "    return eps, eps_nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mesh_errors(mesh_obj, centers, eps, eps_nm, lens, skel_mp):\n",
    "    print(\"Processing mesh errors\")\n",
    "    path_to_root_dict = {}\n",
    "    for ep in eps:\n",
    "        path_to_root_dict[ep] = skel_mp.path_to_root(ep)\n",
    "        \n",
    "    dists_defects = np.zeros(centers.shape[0])\n",
    "    sizes = np.zeros(centers.shape[0])\n",
    "    mesh_map = skel_mp.mesh_to_skel_map\n",
    "    closest_skel_pts = mesh_map[[l[0] for l in lens]]\n",
    "\n",
    "    # print(centers, eps_nm)\n",
    "\n",
    "    dist_matrix = distance_matrix(centers, eps_nm)\n",
    "    ct = 0\n",
    "\n",
    "    closest_tip = np.zeros((centers.shape[0]))\n",
    "\n",
    "    for center in tqdm(centers):\n",
    "    #     skel_pts_dists = np.linalg.norm(skel_mp.vertices - center, axis=1)\n",
    "    #     ep_pts_dists = np.linalg.norm(eps_nm - center, axis=1)\n",
    "        \n",
    "        closest_skel_pt = closest_skel_pts[ct]\n",
    "        min_ep = np.inf\n",
    "        eps_hit = []\n",
    "        for j, ep in enumerate(eps):\n",
    "            if closest_skel_pt in path_to_root_dict[ep]:\n",
    "                eps_hit.append(j)\n",
    "        if len(eps_hit) == 0:\n",
    "            dists_defects[ct] = np.inf\n",
    "            sizes[ct] = np.inf\n",
    "            ct+=1\n",
    "            continue\n",
    "        \n",
    "        dists = dist_matrix[ct, eps_hit]\n",
    "    #     print(dists, eps_hit, center / [4,4,40])\n",
    "        \n",
    "        amin = np.argmin(dists)\n",
    "        tip_hit = eps_hit[amin]\n",
    "        min_dist = dists[amin]\n",
    "        \n",
    "        closest_tip[ct] = tip_hit\n",
    "    #     print(np.argmin(ep_pts_dists), ep_found, eps_nm[np.argmin(ep_pts_dists)]/[4,4,40], eps_nm[j]/[4,4,40], center/[4,4,40])\n",
    "        dists_defects[ct] = min_dist\n",
    "        sizes[ct] = len(lens[ct])\n",
    "        ct+=1\n",
    "    dists_defects_sub = dists_defects[dists_defects < np.inf]\n",
    "    sizes_sub = sizes[dists_defects < np.inf]\n",
    "    centers_sub = centers[dists_defects < np.inf]\n",
    "    tips_hit_sub = closest_tip[dists_defects < np.inf]\n",
    "    closest_skel_pts_sub = closest_skel_pts[dists_defects < np.inf]\n",
    "    inds_sub = np.arange(centers.shape[0])[dists_defects < np.inf]\n",
    "\n",
    "\n",
    "    # Also ranking each component based on its PCA- if the first component is big enough, the points are mostly linear\n",
    "    # These point sets seem to be less likely to be true errors\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca_vec = np.zeros(inds_sub.shape[0])\n",
    "    for i in range(inds_sub.shape[0]):\n",
    "        pca = PCA()#n_components=2)\n",
    "        pca.fit(mesh_obj.vertices[lens[inds_sub[i]]])\n",
    "\n",
    "        pca_vec[i] = pca.explained_variance_ratio_[0]\n",
    "\n",
    "    dists_defects_sub[dists_defects_sub < 4000] = 100\n",
    "    dists_defects_norm = dists_defects_sub #/ np.max(dists_defects_sub)\n",
    "    ranks_ep = sizes_sub / dists_defects_norm * (1-pca_vec)\n",
    "    ranks = sizes_sub**2 * (1-pca_vec)\n",
    "\n",
    "    #ranks_ep_errors_filt = ranks_ep[ranks_ep > .1]\n",
    "    centers_ep_send_errors = centers_sub[np.argsort(ranks_ep)][::-1][:20]\n",
    "    final_mask_eps = np.full(centers_ep_send_errors.shape[0], True)\n",
    "    tips_hit_send_ep = tips_hit_sub[np.argsort(ranks_ep)][::-1][:20]\n",
    "    uns, nums = np.unique(tips_hit_send_ep, return_counts=True)\n",
    "\n",
    "    for un, num in zip(uns, nums):\n",
    "        if num > 1:\n",
    "            final_mask_eps[np.argwhere(tips_hit_send_ep == un)[1:]] = False\n",
    "    centers_errors_ep = centers_ep_send_errors[final_mask_eps]\n",
    "    centers_errors = centers_sub[np.argsort(ranks)[::-1]][:20]\n",
    "    return centers_errors, centers_errors_ep, ranks, ranks_ep, path_to_root_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mesh_facets(mesh_obj, skel_mp, eps, path_to_root_dict, eps_nm):\n",
    "    print(\"Processing facets\")\n",
    "    locs = np.argwhere(mesh_obj.facets_area > 50000)\n",
    "\n",
    "    mesh_map = skel_mp.mesh_to_skel_map\n",
    "    mesh_coords = mesh_obj.vertices[mesh_obj.faces]\n",
    "    mean_locs = []\n",
    "    mesh_ind = []\n",
    "    fs = []\n",
    "    for l in tqdm(locs):\n",
    "        fs.append(np.sum(mesh_obj.facets_area[l]))\n",
    "        fc = mesh_obj.facets[l[0]]\n",
    "        vert_locs = mesh_coords[fc]\n",
    "        mean_locs.append(np.mean(vert_locs[:, 0], axis=0))\n",
    "        mesh_ind.append(fc[0])\n",
    "    mesh_ind = mesh_obj.faces[mesh_ind][:, 0]\n",
    "    mean_locs = np.array(mean_locs)\n",
    "    dists_defects_facets = np.zeros(mean_locs.shape[0])\n",
    "    mesh_map_facets = skel_mp.mesh_to_skel_map\n",
    "    closest_skel_pts_facets = mesh_map[[m for m in mesh_ind]]\n",
    "    dist_matrix_facets = distance_matrix(mean_locs, eps_nm)\n",
    "    ct = 0\n",
    "\n",
    "    closest_tip_facets = np.zeros((mean_locs.shape[0]))\n",
    "\n",
    "    for center in tqdm(mean_locs):\n",
    "\n",
    "        closest_skel_pt = closest_skel_pts_facets[ct]\n",
    "        eps_hit = []\n",
    "        for j, ep in enumerate(eps):\n",
    "            if closest_skel_pt in path_to_root_dict[ep]:\n",
    "                eps_hit.append(j)\n",
    "        if len(eps_hit) == 0:\n",
    "            dists_defects_facets[ct] = np.inf\n",
    "            ct+=1\n",
    "            continue\n",
    "        \n",
    "        dists = dist_matrix_facets[ct, eps_hit]\n",
    "        \n",
    "        amin = np.argmin(dists)\n",
    "        tip_hit = eps_hit[amin]\n",
    "        min_dist = dists[amin]\n",
    "        \n",
    "        closest_tip_facets[ct] = tip_hit\n",
    "        dists_defects_facets[ct] = min_dist\n",
    "        ct+=1\n",
    "    dists_defects_sub_facets = dists_defects_facets[dists_defects_facets < np.inf]\n",
    "    sizes_sub_facets = np.array(fs)[dists_defects_facets < np.inf]\n",
    "    mean_locs_facets = mean_locs[dists_defects_facets < np.inf]\n",
    "    tips_hit_sub_facets = closest_tip_facets[dists_defects_facets < np.inf]\n",
    "    closest_skel_pts_sub_facets = closest_skel_pts_facets[dists_defects_facets < np.inf]\n",
    "    inds_sub_facets = np.arange(mean_locs.shape[0])[dists_defects_facets < np.inf]\n",
    "    ranks_ep_facets = sizes_sub_facets**2 / dists_defects_sub_facets\n",
    "    #ranks_ep_facets_filt = ranks_ep_facets[ranks_ep_facets > 2e7]\n",
    "    mean_locs_send_facets = mean_locs_facets[np.argsort(ranks_ep_facets)][::-1][:20]\n",
    "    final_mask_facets = np.full(mean_locs_send_facets.shape[0], True)\n",
    "    tips_hit_send_facets = tips_hit_sub_facets[np.argsort(ranks_ep_facets)][::-1][:20]\n",
    "    uns, nums = np.unique(tips_hit_send_facets, return_counts=True)\n",
    "\n",
    "    for un, num in zip(uns, nums):\n",
    "        if num > 1:\n",
    "            final_mask_facets[np.argwhere(tips_hit_send_facets == un)[1:]] = False\n",
    "    facets_send_final = mean_locs_send_facets[final_mask_facets] / [4,4,40]\n",
    "    return facets_send_final, ranks_ep_facets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TIP FINDING FUNCTION\n",
    "def error_locs_defects(root_id, soma_id = None, soma_table=None, center_collapse=True):\n",
    "    #print(\"START\", root_id)\n",
    "\n",
    "    mesh_obj = get_and_process_mesh(root_id)\n",
    "    if mesh_obj is None:\n",
    "        return None\n",
    "    # SKELETONIZE - if we are just looking for general errors, not errors at endpoints, this can be skipped\n",
    "    try:\n",
    "        if soma_table==None:\n",
    "            soma_table = get_soma(str(soma_id))\n",
    "        if soma_table[soma_table.id == soma_id].shape[0] > 0:\n",
    "            center = np.array(soma_table[soma_table.id == soma_id].pt_position)[0] * [4,4,40]\n",
    "        else:\n",
    "            center=None\n",
    "    except:\n",
    "        center = None\n",
    "    print(\"Subselecting largest connected component of mesh\")\n",
    "    mesh_obj, encapsulated_ids, max_verts = process_mesh_ccs(mesh_obj)\n",
    "\n",
    "    skel_mp = skeletonize.skeletonize_mesh(trimesh_io.Mesh(mesh_obj.vertices, \n",
    "                                            mesh_obj.faces),\n",
    "                                            invalidation_d=15000,\n",
    "                                            shape_function='cone',\n",
    "                                            collapse_function='branch',\n",
    "#                                             soma_radius = soma_radius,\n",
    "                                            soma_pt=center,\n",
    "                                            smooth_neighborhood=5,\n",
    "                                            cc_vertex_thresh=max_verts - 10\n",
    "#                                                     collapse_params = {'dynamic_threshold':True}\n",
    "                                            )\n",
    "    print(\"Skel done\")\n",
    "\n",
    "    # find edges that only occur once..  might be faster to find these in the sparse matrix..\n",
    "    centers, lens = process_defects(mesh_obj)\n",
    "    eps, eps_nm = process_endpoints(mesh_obj, skel_mp)\n",
    "\n",
    "    if len(centers) !=0:\n",
    "        centers_errors, centers_errors_ep, ranks, ranks_ep, path_to_root_dict = process_mesh_errors(mesh_obj, centers, eps, eps_nm, lens, skel_mp)\n",
    "        ranks_return = np.squeeze(ranks[np.argsort(ranks)[::-1]][:20])\n",
    "        ranks_ep_return = np.squeeze(ranks_ep[np.argsort(ranks_ep)][::-1][:20])\n",
    "    else:\n",
    "        # Assign placeholder values for each of the variables above.\n",
    "        centers_errors = np.zeros ((1,3))\n",
    "        centers_errors_ep = np.zeros ((1,3))\n",
    "        ranks = np.zeros ((1))\n",
    "        ranks_ep = np.zeros((1, 3))\n",
    "        path_to_root_dict = {}\n",
    "        for ep in eps:\n",
    "            path_to_root_dict[ep] = skel_mp.path_to_root(ep)\n",
    "\n",
    "        ranks_return = 0\n",
    "        ranks_ep_return = 0\n",
    "\n",
    "\n",
    "    #if len(centers_errors.shape) > 1 and centers_errors.shape[0] > 0 and len(centers_errors_ep.shape) > 1 and len(centers_errors_ep.shape[0] > 0):\n",
    "    #    centers_errors = centers_errors[np.min(distance_matrix(centers_errors, centers_errors_ep), axis=1)>1000]\n",
    "    facets_send_final, ranks_ep_facets = process_mesh_facets(mesh_obj, skel_mp, eps, path_to_root_dict, eps_nm)\n",
    "\n",
    "    errors_send = centers_errors / [4,4,40]\n",
    "    errors_tips_send = centers_errors_ep / [4,4,40]\n",
    "    encapsulated_centers = [e[0] for e in encapsulated_ids]\n",
    "    encapsulated_lens = [e[1] for e in encapsulated_ids]\n",
    "    sorted_encapsulated_send = np.array(encapsulated_centers)[np.argsort(encapsulated_lens)][::-1]\n",
    "\n",
    "\n",
    "\n",
    "    return sorted_encapsulated_send, facets_send_final, errors_send, errors_tips_send, np.squeeze(ranks_ep_facets[[np.argsort(ranks_ep_facets)][::-1][:20]]), ranks_return, ranks_ep_return\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OUR tip generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_endpoints(row):\n",
    "    seg_id = row[\"seg_id\"]\n",
    "    sorted_encapsulated_send, facets_send_final, errors_send, errors_tips_send, dummy, dummy2, dummy3 = error_locs_defects(seg_id)\n",
    "    #facets_send_final = facets_send_final[facets_send_final != [0,0,0]]\n",
    "    #errors_tips_send = errors_tips_send[errors_tips_send != [0,0,0]]\n",
    "    together = np.vstack((facets_send_final, errors_tips_send))\n",
    "    #should alter \"together\" if not set together equal to this line\n",
    "    print(together)\n",
    "    new = together[together != [0,0,0]]\n",
    "    print(new)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_endpoints2(row):\n",
    "    seg_id = row[\"seg_id\"]\n",
    "    sorted_encapsulated_send, facets_send_final, errors_send, errors_tips_send, dummy, dummy2, dummy3 = error_locs_defects(seg_id)\n",
    "    #facets_send_final = facets_send_final[facets_send_final != [0,0,0]]\n",
    "    #errors_tips_send = errors_tips_send[errors_tips_send != [0,0,0]]\n",
    "    result = [[0,0,0]]\n",
    "    count = 0\n",
    "    for point in facets_send_final:\n",
    "        if(np.array_equal(point, [0,0,0])):\n",
    "            continue\n",
    "        if(count == 0) :\n",
    "            result = point\n",
    "            count = count + 1\n",
    "            continue\n",
    "        result = np.vstack((result, point))\n",
    "    for tip in errors_tips_send:\n",
    "        if(np.array_equal(tip, [0,0,0])):\n",
    "            continue\n",
    "        result = np.vstack((result, point))\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASSUMING output from error_locs IS: \n",
    "facets_find_final: [[x,y,z], [x2,y2,z2], etc]\n",
    "errors_tips_send: [[x,y,z], [x2,y2,z2], etc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_endpoints(dataframe) :\n",
    "    dataframe[\"endpoints_generated\"] = dataframe.apply(find_endpoints, axis = 1)\n",
    "    return dataframe\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_endpoints2(dataframe) :\n",
    "    dataframe[\"endpoints_generated\"] = dataframe.apply(find_endpoints2, axis = 1)\n",
    "    return dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = orphans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Mesh\n",
      "Warning: deduplication not currently supported for this layer's variable layered draco meshes\n",
      "Vertices:  7993\n",
      "Subselecting largest connected component of mesh\n",
      "Processing CC's\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7992/7992 [00:00<00:00, 1211452.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skel done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 163496.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing facets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 7632.95it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 36419.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[402970.10915493 229214.63028169  23552.55      ]\n",
      " [402527.5625     228866.3125      23991.45      ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Mesh\n",
      "Warning: deduplication not currently supported for this layer's variable layered draco meshes\n",
      "Vertices:  21335\n",
      "Subselecting largest connected component of mesh\n",
      "Processing CC's\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21334/21334 [00:00<00:00, 1416044.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skel done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:00<00:00, 250621.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing mesh errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 17367.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing facets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 37/37 [00:00<00:00, 14408.06it/s]\n",
      "100%|██████████| 37/37 [00:00<00:00, 37713.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[409125.11111111 219484.22222222  23865.45      ]\n",
      " [409125.11111111 219484.22222222  23865.45      ]]\n",
      "Downloading Mesh\n",
      "Warning: deduplication not currently supported for this layer's variable layered draco meshes\n",
      "Vertices:  2282\n",
      "Subselecting largest connected component of mesh\n",
      "Processing CC's\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2281/2281 [00:00<00:00, 551519.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skel done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 63550.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing mesh errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 14266.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing facets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 6802.03it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 96420.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Mesh\n",
      "Warning: deduplication not currently supported for this layer's variable layered draco meshes\n",
      "Vertices:  185182\n",
      "Subselecting largest connected component of mesh\n",
      "Processing CC's\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 184108/184108 [00:00<00:00, 608832.61it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skel done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [00:00<00:00, 344033.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing mesh errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:00<00:00, 18277.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing facets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 477/477 [00:00<00:00, 17826.47it/s]\n",
      "100%|██████████| 477/477 [00:00<00:00, 29187.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[403959.7173913  210610.40217391  23991.45      ]\n",
      " [401374.52489627 218762.27178423  23991.45      ]\n",
      " [371421.3125     191780.3125      21775.425     ]\n",
      " [371421.3125     191780.3125      21775.425     ]\n",
      " [371421.3125     191780.3125      21775.425     ]\n",
      " [371421.3125     191780.3125      21775.425     ]\n",
      " [371421.3125     191780.3125      21775.425     ]]\n",
      "Downloading Mesh\n",
      "Warning: deduplication not currently supported for this layer's variable layered draco meshes\n",
      "Vertices:  234\n",
      "Subselecting largest connected component of mesh\n",
      "Processing CC's\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 233/233 [00:00<00:00, 117843.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skel done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 16384.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing facets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 3114.97it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 28055.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/var/folders/bz/3tlf2mzd44x751_89tsfdgjh0000gn/T/ipykernel_33079/3796316872.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe[\"endpoints_generated\"] = dataframe.apply(find_endpoints2, axis = 1)\n"
     ]
    }
   ],
   "source": [
    "test = generate_endpoints2(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [[402970.1091549296, 229214.63028169013, 23552...\n",
      "1    [[409125.1111111111, 219484.22222222222, 23865...\n",
      "2                                                    0\n",
      "3    [[403959.7173913043, 210610.40217391305, 23991...\n",
      "4                                                    0\n",
      "Name: endpoints_generated, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(test[\"endpoints_generated\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[402970.10915493 229214.63028169  23552.55      ]\n",
      " [402527.5625     228866.3125      23991.45      ]]\n",
      "[[409125.11111111 219484.22222222  23865.45      ]\n",
      " [409125.11111111 219484.22222222  23865.45      ]]\n",
      "0\n",
      "[[403959.7173913  210610.40217391  23991.45      ]\n",
      " [401374.52489627 218762.27178423  23991.45      ]\n",
      " [371421.3125     191780.3125      21775.425     ]\n",
      " [371421.3125     191780.3125      21775.425     ]\n",
      " [371421.3125     191780.3125      21775.425     ]\n",
      " [371421.3125     191780.3125      21775.425     ]\n",
      " [371421.3125     191780.3125      21775.425     ]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for points in test[\"endpoints_generated\"]:\n",
    "    print(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACCURACY FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_eps_acc(gt_endpoints, pred_endpoints, threshold):\n",
    "    # Calculate distances\n",
    "    dist_matrix = np.array(spatial.distance.cdist(gt_endpoints, pred_endpoints, metric = 'euclidean'))\n",
    "\n",
    "    # Apply threshold\n",
    "    dist_matrix[dist_matrix > threshold] = 0\n",
    "\n",
    "    # Calculating accuracy\n",
    "    valid_eps = np.count_nonzero(dist_matrix, axis = 1)\n",
    "    accuracy = np.count_nonzero(valid_eps) / len(gt_endpoints)\n",
    "\n",
    "    # If more than one valid endpoint found for a single ground truth endpoint, add the other valid endpoints to extra_valid_pairs\n",
    "    extra_valid_pairs = []\n",
    "    [[extra_valid_pairs.append([gt_endpoints[i], pred_endpoints[index]]) \\\n",
    "        for index, j in enumerate(dist_matrix[i]) if j != np.min(dist_matrix[i][dist_matrix[i] != 0]) if j != 0] \\\n",
    "            for i in valid_eps if i > 1]\n",
    "\n",
    "    return accuracy, extra_valid_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seg_id</th>\n",
       "      <th>num_gt_eps</th>\n",
       "      <th>gt_eps</th>\n",
       "      <th>num_pred_eps</th>\n",
       "      <th>pred_eps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>864691136577830164</td>\n",
       "      <td>6</td>\n",
       "      <td>[[402188, 228684, 24029], [401258, 224832, 240...</td>\n",
       "      <td>6</td>\n",
       "      <td>[[400292, 220879, 24027], [401253, 228342, 244...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               seg_id  num_gt_eps  \\\n",
       "0  864691136577830164           6   \n",
       "\n",
       "                                              gt_eps  num_pred_eps  \\\n",
       "0  [[402188, 228684, 24029], [401258, 224832, 240...             6   \n",
       "\n",
       "                                            pred_eps  \n",
       "0  [[400292, 220879, 24027], [401253, 228342, 244...  "
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(columns=[\"seg_id\", \"num_gt_eps\", \"gt_eps\", \"num_pred_eps\", \"pred_eps\"])\n",
    "data.loc[len(data.index)] = [864691136577830164, 6, [\n",
    "    [402188, 228684, 24029], [401258, 224832, 24029], [401314, 228366, 24424], \\\n",
    "    [400199, 220721, 24029], [397870, 232292, 24004], [403272, 227529, 24394]], 6, \\\n",
    "    [[400292, 220879, 24027], [401253, 228342, 24408], [401253, 228342, 24409],\n",
    "        [402923, 227458, 24372], [402099, 228716, 24037], [402627, 231471, 24026]]]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "[[[401314, 228366, 24424], [401253, 228342, 24408]]]\n"
     ]
    }
   ],
   "source": [
    "acc, extra_valid_pairs = pred_eps_acc(\n",
    "    data.loc[0, \"gt_eps\"], data.loc[0, \"pred_eps\"], 100)\n",
    "print(acc)\n",
    "print(extra_valid_pairs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               seg_id  num_endpoints  \\\n",
      "0  864691135909994000              2   \n",
      "1  864691135247440303              3   \n",
      "2  864691134794123793              2   \n",
      "3  864691135772363453              7   \n",
      "4  864691135314714227              2   \n",
      "\n",
      "                                           endpoints  \\\n",
      "0  [(402584, 228856, 23991), (402985, 229235, 235...   \n",
      "1  [(401612, 224623, 23991), (405257, 226318, 236...   \n",
      "2  [(401242, 228382, 24444), (400982, 228457, 245...   \n",
      "3  [(401289, 218721, 23991), (399895, 216533, 235...   \n",
      "4  [(397867, 232230, 23999), (397854, 232252, 239...   \n",
      "\n",
      "                                 endpoints_generated  \n",
      "0  [[402970.1091549296, 229214.63028169013, 23552...  \n",
      "1  [[409125.1111111111, 219484.22222222222, 23865...  \n",
      "2                                        [[0, 0, 0]]  \n",
      "3  [[403959.7173913043, 210610.40217391305, 23991...  \n",
      "4                                        [[0, 0, 0]]  \n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(402584, 228856, 23991)', '(402985, 229235, 23554)']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.loc[0, \"endpoints\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[402970.10915493, 229214.63028169,  23552.55      ],\n",
       "       [402527.5625    , 228866.3125    ,  23991.45      ]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " test.loc[0, \"endpoints_generated\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "XA must be a 2-dimensional array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [137], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#THIS is only testing the first row! PUt in loop to get all accuracies\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m acc, extra_valid_pairs \u001b[39m=\u001b[39m pred_eps_acc(test\u001b[39m.\u001b[39;49mloc[\u001b[39m0\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mendpoints\u001b[39;49m\u001b[39m\"\u001b[39;49m], test\u001b[39m.\u001b[39;49mloc[\u001b[39m0\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mendpoints_generated\u001b[39;49m\u001b[39m\"\u001b[39;49m], \u001b[39m100\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [136], line 3\u001b[0m, in \u001b[0;36mpred_eps_acc\u001b[0;34m(gt_endpoints, pred_endpoints, threshold)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpred_eps_acc\u001b[39m(gt_endpoints, pred_endpoints, threshold):\n\u001b[1;32m      2\u001b[0m     \u001b[39m# Calculate distances\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     dist_matrix \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(spatial\u001b[39m.\u001b[39;49mdistance\u001b[39m.\u001b[39;49mcdist(gt_endpoints, pred_endpoints, metric \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39meuclidean\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m      5\u001b[0m     \u001b[39m# Apply threshold\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     dist_matrix[dist_matrix \u001b[39m>\u001b[39m threshold] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/scipy/spatial/distance.py:2906\u001b[0m, in \u001b[0;36mcdist\u001b[0;34m(XA, XB, metric, out, **kwargs)\u001b[0m\n\u001b[1;32m   2903\u001b[0m sB \u001b[39m=\u001b[39m XB\u001b[39m.\u001b[39mshape\n\u001b[1;32m   2905\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(s) \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m-> 2906\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mXA must be a 2-dimensional array.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   2907\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(sB) \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m   2908\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mXB must be a 2-dimensional array.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: XA must be a 2-dimensional array."
     ]
    }
   ],
   "source": [
    "#THIS is only testing the first row! PUt in loop to get all accuracies\n",
    "acc, extra_valid_pairs = pred_eps_acc(test.loc[0, \"endpoints\"], test.loc[0, \"endpoints_generated\"], 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
