{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloudvolume import CloudVolume\n",
    "from meshparty import skeletonize, trimesh_io\n",
    "from caveclient import CAVEclient\n",
    "import trimesh\n",
    "import numpy as np\n",
    "import datetime\n",
    "import networkx as nx\n",
    "from scipy.sparse import identity\n",
    "from scipy.spatial import distance_matrix\n",
    "import scipy \n",
    "from tqdm import tqdm\n",
    "# import aws\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pyembree\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.spatial as spatial\n",
    "import itertools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATAFRAME FOR NEURONS AND ORPHANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 seg_id  num_endpoints  \\\n",
      "0    864691135909994000              2   \n",
      "1    864691135247440303              3   \n",
      "2    864691134794123793              2   \n",
      "3    864691135772363453              7   \n",
      "4    864691135314714227              2   \n",
      "..                  ...            ...   \n",
      "127  864691135319600870              2   \n",
      "128  864691136558839249              2   \n",
      "129  864691135012763638             -1   \n",
      "130  864691135458639120              2   \n",
      "131  864691135826764827              2   \n",
      "\n",
      "                                             endpoints  \n",
      "0     (402584, 228856, 23991); (402985, 229235, 23554)  \n",
      "1    (401612, 224623, 23991); (405257, 226318, 2362...  \n",
      "2     (401242, 228382, 24444); (400982, 228457, 24513)  \n",
      "3    (401289, 218721, 23991); (399895, 216533, 2352...  \n",
      "4    (397867, 232230, 23999);  (397854, 232252, 23997)  \n",
      "..                                                 ...  \n",
      "127     (106608, 111803, 20796); (98290, 97555, 21045)  \n",
      "128    (103138, 188530, 21200); (91780, 189141, 20879)  \n",
      "129                                                 ()  \n",
      "130   (127475, 206611, 21525); (128120, 207267, 21512)  \n",
      "131   (122293, 186204, 19621); (124943, 185661, 18677)  \n",
      "\n",
      "[132 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "orphans = pd.read_csv(\"/Users/sheeltanna/Desktop/AGT_REPO/campfire/tip_finding/Orphans.csv\")\n",
    "print(orphans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = pd.read_csv(\"/Users/sheeltanna/Desktop/AGT_REPO/campfire/tip_finding/Neuron.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_array(x):\n",
    "    res = list(map(str.strip, x.split('; ')))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 seg_id  num_endpoints  \\\n",
      "0    864691135909994000              2   \n",
      "1    864691135247440303              3   \n",
      "2    864691134794123793              2   \n",
      "3    864691135772363453              7   \n",
      "4    864691135314714227              2   \n",
      "..                  ...            ...   \n",
      "127  864691135319600870              2   \n",
      "128  864691136558839249              2   \n",
      "129  864691135012763638             -1   \n",
      "130  864691135458639120              2   \n",
      "131  864691135826764827              2   \n",
      "\n",
      "                                             endpoints  \n",
      "0    [(402584, 228856, 23991), (402985, 229235, 235...  \n",
      "1    [(401612, 224623, 23991), (405257, 226318, 236...  \n",
      "2    [(401242, 228382, 24444), (400982, 228457, 245...  \n",
      "3    [(401289, 218721, 23991), (399895, 216533, 235...  \n",
      "4    [(397867, 232230, 23999), (397854, 232252, 239...  \n",
      "..                                                 ...  \n",
      "127   [(106608, 111803, 20796), (98290, 97555, 21045)]  \n",
      "128  [(103138, 188530, 21200), (91780, 189141, 20879)]  \n",
      "129                                               [()]  \n",
      "130  [(127475, 206611, 21525), (128120, 207267, 215...  \n",
      "131  [(122293, 186204, 19621), (124943, 185661, 186...  \n",
      "\n",
      "[132 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "orphans['endpoints'] = orphans['endpoints'].map(lambda x: list(map(str.strip, x.split('; '))))\n",
    "print(orphans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert from string list to 2-d array\n",
    "def convert_to_array(row):\n",
    "    count = 0\n",
    "    result = []\n",
    "    for endpoint in row[\"endpoints\"]:\n",
    "        # endpoint = tuple(map(int, endpoint.split(', ')))\n",
    "        endpoint = eval(endpoint)\n",
    "        # print()\n",
    "        # print(endpoint)\n",
    "        # print(type(endpoint))\n",
    "        if(count == 0):\n",
    "            result = np.array(endpoint)\n",
    "            count = count + 1\n",
    "        else:\n",
    "            result = np.vstack((result, np.array(endpoint)))\n",
    "            count = count + 1\n",
    "            #result = np.concatenate(result, list(tuple))\n",
    "    #check if there was only 1 point, convert to 2-d array:\n",
    "    if(count == 1 and result.size != 0):\n",
    "        result = result.reshape(1,3)\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "1\n",
      "[0. 0. 0.]\n",
      "[[0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros(3)\n",
    "print(a.shape)\n",
    "print(len(a.shape))\n",
    "print(a)\n",
    "a = a.reshape(1,3)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "orphans[\"real_endpoints\"] = orphans.apply(convert_to_array, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Neurons  num_endpoints  \\\n",
      "0   864691136577830164              6   \n",
      "1   864691135683615218             13   \n",
      "2   864691135864989916              2   \n",
      "3   864691135837578899              9   \n",
      "4   864691135501859650             14   \n",
      "5   864691135544674088              7   \n",
      "6   864691135614368971             12   \n",
      "7   864691135334773481              7   \n",
      "8   864691135614361291             12   \n",
      "9   864691135445907602             12   \n",
      "10  864691136390697343              8   \n",
      "11  864691135396741921              9   \n",
      "12  864691135360418631              6   \n",
      "13  864691135463797061              6   \n",
      "14  864691135526309723              9   \n",
      "\n",
      "                                            endpoints  \n",
      "0   [(402188, 228684, 24029), (401258, 224832, 240...  \n",
      "1   [(77233, 113188, 20454), (100382, 143836, 2164...  \n",
      "2   [(211973, 200313, 23233), (167751, 211623, 207...  \n",
      "3   [(351264, 143646, 15182), (344801, 142291, 169...  \n",
      "4   [(101840, 247377, 19543), (92823, 255328, 2038...  \n",
      "5   [(220484, 239044, 20481), (238449, 242308, 224...  \n",
      "6   [(107820, 233757, 21960), (116372, 239008, 232...  \n",
      "7   [(138311, 218409, 26379), (114789, 194406, 242...  \n",
      "8   [(123240, 178472, 21529), (121863, 182085, 215...  \n",
      "9   [(293401, 150411, 19104), (313432, 160982, 185...  \n",
      "10  [(142469, 252585, 22976), (114049, 233747, 220...  \n",
      "11  [(346513, 82659, 27098), (361886, 82055, 27634...  \n",
      "12  [(110598, 239837, 22040), (108642, 243230, 219...  \n",
      "13  [(103052, 175611, 18430), (104081, 174998, 184...  \n",
      "14  [(108958, 194091, 22035), (107125, 175763, 219...  \n"
     ]
    }
   ],
   "source": [
    "neurons['endpoints'] = neurons['endpoints'].map(lambda x: list(map(str.strip, x.split('; '))))\n",
    "print(neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons[\"real_endpoints\"] = neurons.apply(convert_to_array, axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONS FOR TIP FINDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_process_mesh(root_id):\n",
    "    datastack_name = \"minnie65_phase3_v1\"\n",
    "    client = CAVEclient(datastack_name)\n",
    "    vol = CloudVolume(\n",
    "        client.info.segmentation_source(),\n",
    "        use_https=True,\n",
    "        progress=False,\n",
    "        bounded=False,\n",
    "        fill_missing=True,\n",
    "        secrets={\"token\": client.auth.token}\n",
    "    )\n",
    "    print(\"Downloading Mesh\")\n",
    "    mesh = vol.mesh.get(str(root_id))[root_id]\n",
    "    mesh_obj = trimesh.Trimesh(np.divide(mesh.vertices, np.array([1,1,1])), mesh.faces)\n",
    "    print(\"Vertices: \", mesh.vertices.shape[0])\n",
    "\n",
    "    if mesh_obj.volume > 4000000000000:\n",
    "        print(\"TOO BIG, SKIPPING\")\n",
    "        #queue_url_endpoints = sqs.get_or_create_queue(\"root_ids_functional_dlqueue\")\n",
    "\n",
    "        #entries=sqs.construct_rootid_entries([root_id])\n",
    "\n",
    "        #sqs.send_batch(queue_url_endpoints, entries)\n",
    "\n",
    "        return None\n",
    "    trimesh.repair.fix_normals(mesh_obj)\n",
    "    mesh_obj.fill_holes()\n",
    "\n",
    "    return mesh_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soma(soma_id:str):\n",
    "    cave_client = CAVEclient('minnie65_phase3_v1')\n",
    "    soma = cave_client.materialize.query_table(\n",
    "        \"nucleus_neuron_svm\",\n",
    "        filter_equal_dict={'id':soma_id}\n",
    "    )\n",
    "    return soma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mesh_ccs(mesh_obj):\n",
    "    print(\"Processing CC's\")\n",
    "    ccs_graph = trimesh.graph.connected_components(mesh_obj.edges)\n",
    "    ccs_len = [len(c) for c in ccs_graph]\n",
    "\n",
    "    # Subselect the parts of the mesh that are not inside one another \n",
    "    # the other components are an artifact of the soma seg and small unfilled sections\n",
    "    largest_component = ccs_graph[np.argmax(ccs_len)]\n",
    "    largest_component_remap = np.arange(ccs_graph[np.argmax(ccs_len)].shape[0])\n",
    "    face_dict = {largest_component[i]:largest_component_remap[i] for i in range(largest_component.shape[0])}\n",
    "\n",
    "    new_faces_mask = np.isin(mesh_obj.faces, list(face_dict.keys()))\n",
    "    new_faces_mask = new_faces_mask[:, 0]*new_faces_mask[:, 1]*new_faces_mask[:, 2]\n",
    "\n",
    "    new_faces = np.vectorize(face_dict.get)(mesh_obj.faces[new_faces_mask])\n",
    "    new_faces = new_faces[new_faces[:, 0] != None]\n",
    "    largest_component_mesh = trimesh.Trimesh(mesh_obj.vertices[largest_component], new_faces)\n",
    "\n",
    "    all_ids = set(largest_component)\n",
    "    encapsulated_ids = []\n",
    "\n",
    "    for i in range(1, len(ccs_graph)):\n",
    "        n_con = largest_component_mesh.contains(mesh_obj.vertices[ccs_graph[i]])\n",
    "        if np.sum(n_con) / n_con.shape[0] == 0 and n_con.shape[0] > 50:\n",
    "            all_ids.update(ccs_graph[i])\n",
    "        else:\n",
    "            if len(ccs_graph[i]) < 1000:\n",
    "                encapsulated_ids.append((np.mean(mesh_obj.vertices[ccs_graph[i]], axis=0)/[4,4,40], len(ccs_graph[i])))\n",
    "            \n",
    "    all_component = np.array(list(ccs_graph[np.argmax(ccs_len)]))\n",
    "    all_component_remap = np.arange(all_component.shape[0])\n",
    "    face_dict = {all_component[i]:all_component_remap[i] for i in range(all_component.shape[0])}\n",
    "    new_faces_mask = np.isin(mesh_obj.faces, list(face_dict.keys()))\n",
    "    new_faces_mask = new_faces_mask[:, 0]*new_faces_mask[:, 1]*new_faces_mask[:, 2]\n",
    "\n",
    "    new_faces = np.vectorize(face_dict.get)(mesh_obj.faces[new_faces_mask])\n",
    "    new_faces[new_faces[:, 0] != None]\n",
    "    \n",
    "    largest_component_mesh = trimesh.Trimesh(mesh_obj.vertices[all_component], new_faces)\n",
    "    \n",
    "    mesh_obj = largest_component_mesh\n",
    "    return mesh_obj, encapsulated_ids, np.max(ccs_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_defects(mesh_obj, a=.75):\n",
    "    bad_edges = trimesh.grouping.group_rows(\n",
    "        mesh_obj.edges_sorted, require_count=1)\n",
    "    bad_edges_ind = mesh_obj.edges[bad_edges]\n",
    "    sparse_edges = mesh_obj.edges_sparse\n",
    "    xs = list(bad_edges_ind[:, 0]) + list(bad_edges_ind[:, 1]) \n",
    "    ys = list(bad_edges_ind[:, 1]) + list(bad_edges_ind[:, 0])\n",
    "    vs = [1]*bad_edges_ind.shape[0]*2\n",
    "    bad_inds = scipy.sparse.coo_matrix((vs, (xs, ys)), shape=(mesh_obj.vertices.shape[0], mesh_obj.vertices.shape[0]))\n",
    "    # Make it symmetrical and add identity so each integrates from itself too, then subtract singleton edges\n",
    "    # I noticed that the number of asymmetrical edges vs the number of single edges I find from group rows\n",
    "    # Are close but different. Haven't looked into that yet. Also removing edges 1 hop away from single edges to remove bias towards\n",
    "    # Holes in the mesh that are caused by mesh construction errors as opposed to segmentation errors\n",
    "    sparse_edges = mesh_obj.edges_sparse + mesh_obj.edges_sparse.T + identity(mesh_obj.edges_sparse.shape[0]) - sparse_edges.multiply(bad_inds) - bad_inds\n",
    "    degs = mesh_obj.vertex_degree + 1\n",
    "\n",
    "    # N_iter is a smoothing parameter here. The loop below smooths the vertex error about the mesh to get more consistent connected regions\n",
    "    n_iter = 2\n",
    "    angle_sum = np.array(abs(mesh_obj.face_angles_sparse).sum(axis=1)).flatten()\n",
    "    defs = (2 * np.pi) - angle_sum\n",
    "\n",
    "    abs_defs = np.abs(defs)\n",
    "    abs_defs_i = abs_defs.copy()\n",
    "    for i in range(n_iter):\n",
    "        abs_defs_i = sparse_edges.dot(abs_defs_i) / degs\n",
    "    \n",
    "    verts_select = np.argwhere((abs_defs_i > a))# & (abs_defs < 2.5))\n",
    "\n",
    "    edges_mask = np.isin(mesh_obj.edges, verts_select)\n",
    "    edges_mask[bad_edges] = False\n",
    "    edges_select = edges_mask[:, 0] * edges_mask[:, 1]\n",
    "    edges_select = mesh_obj.edges[edges_select]\n",
    "\n",
    "    G = nx.from_edgelist(edges_select)#f_edge_sub)\n",
    "\n",
    "    ccs = nx.connected_components(G)\n",
    "    subgraphs = [G.subgraph(cc).copy() for cc in ccs]\n",
    "\n",
    "    lens = []\n",
    "    lengths = []\n",
    "    for i in tqdm(range(len(subgraphs))):\n",
    "        ns = np.array(list(subgraphs[i].nodes()))\n",
    "    #     ns = ns[abs_defs[ns ]]\n",
    "        l = len(ns)\n",
    "        if l > 20 and l < 5000:\n",
    "            lens.append(ns)\n",
    "            lengths.append(l)\n",
    "    all_nodes = set()\n",
    "    for l in lens:\n",
    "        all_nodes.update(l)\n",
    "    all_nodes = np.array(list(all_nodes))\n",
    "    # sharp_pts = mesh_obj.vertices[all_nodes]\n",
    "    centers = np.array([np.mean(mesh_obj.vertices[list(ppts)],axis=0) for ppts in lens])\n",
    "\n",
    "    return centers, lens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_endpoints(mesh_obj, skel_mp):\n",
    "    # Process the skeleton to get the endpoints\n",
    "    interior_cc_mask = set()\n",
    "    el = nx.from_edgelist(skel_mp.edges)\n",
    "    comps = list(nx.connected_components(el))\n",
    "    for c in comps:\n",
    "        if len(c) < 10000:\n",
    "            n_con = mesh_obj.contains(skel_mp.vertices[list(c)])\n",
    "            if np.sum(n_con) / n_con.shape[0] > .05:\n",
    "                interior_cc_mask.update(list(c))\n",
    "    # Process the skeleton to get the endpoints\n",
    "    edges = skel_mp.edges.copy()\n",
    "\n",
    "    edge_mask = ~np.isin(edges, interior_cc_mask)\n",
    "    edge_mask = edge_mask[:, 0] + edge_mask[:, 1]\n",
    "    edges = edges[edge_mask]\n",
    "    edges_flat  = edges.flatten()\n",
    "    edge_bins = np.bincount(edges_flat) \n",
    "\n",
    "    eps = np.squeeze(np.argwhere(edge_bins==1))\n",
    "    eps_nm = skel_mp.vertices[eps]\n",
    "\n",
    "    eps_comp = distance_matrix(eps_nm, eps_nm)\n",
    "    eps_comp[eps_comp == 0] = np.inf\n",
    "    eps_thresh = np.argwhere(~(np.min(eps_comp, axis=0) < 3000))\n",
    "\n",
    "    eps = np.squeeze(eps[eps_thresh])\n",
    "    eps_nm = np.squeeze(eps_nm[eps_thresh])\n",
    "    return eps, eps_nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mesh_errors(mesh_obj, centers, eps, eps_nm, lens, skel_mp):\n",
    "    print(\"Processing mesh errors\")\n",
    "    path_to_root_dict = {}\n",
    "    for ep in eps:\n",
    "        path_to_root_dict[ep] = skel_mp.path_to_root(ep)\n",
    "        \n",
    "    dists_defects = np.zeros(centers.shape[0])\n",
    "    sizes = np.zeros(centers.shape[0])\n",
    "    mesh_map = skel_mp.mesh_to_skel_map\n",
    "    closest_skel_pts = mesh_map[[l[0] for l in lens]]\n",
    "\n",
    "    # print(centers, eps_nm)\n",
    "\n",
    "    dist_matrix = distance_matrix(centers, eps_nm)\n",
    "    ct = 0\n",
    "\n",
    "    closest_tip = np.zeros((centers.shape[0]))\n",
    "\n",
    "    for center in tqdm(centers):\n",
    "    #     skel_pts_dists = np.linalg.norm(skel_mp.vertices - center, axis=1)\n",
    "    #     ep_pts_dists = np.linalg.norm(eps_nm - center, axis=1)\n",
    "        \n",
    "        closest_skel_pt = closest_skel_pts[ct]\n",
    "        min_ep = np.inf\n",
    "        eps_hit = []\n",
    "        for j, ep in enumerate(eps):\n",
    "            if closest_skel_pt in path_to_root_dict[ep]:\n",
    "                eps_hit.append(j)\n",
    "        if len(eps_hit) == 0:\n",
    "            dists_defects[ct] = np.inf\n",
    "            sizes[ct] = np.inf\n",
    "            ct+=1\n",
    "            continue\n",
    "        \n",
    "        dists = dist_matrix[ct, eps_hit]\n",
    "    #     print(dists, eps_hit, center / [4,4,40])\n",
    "        \n",
    "        amin = np.argmin(dists)\n",
    "        tip_hit = eps_hit[amin]\n",
    "        min_dist = dists[amin]\n",
    "        \n",
    "        closest_tip[ct] = tip_hit\n",
    "    #     print(np.argmin(ep_pts_dists), ep_found, eps_nm[np.argmin(ep_pts_dists)]/[4,4,40], eps_nm[j]/[4,4,40], center/[4,4,40])\n",
    "        dists_defects[ct] = min_dist\n",
    "        sizes[ct] = len(lens[ct])\n",
    "        ct+=1\n",
    "    dists_defects_sub = dists_defects[dists_defects < np.inf]\n",
    "    sizes_sub = sizes[dists_defects < np.inf]\n",
    "    centers_sub = centers[dists_defects < np.inf]\n",
    "    tips_hit_sub = closest_tip[dists_defects < np.inf]\n",
    "    closest_skel_pts_sub = closest_skel_pts[dists_defects < np.inf]\n",
    "    inds_sub = np.arange(centers.shape[0])[dists_defects < np.inf]\n",
    "\n",
    "\n",
    "    # Also ranking each component based on its PCA- if the first component is big enough, the points are mostly linear\n",
    "    # These point sets seem to be less likely to be true errors\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca_vec = np.zeros(inds_sub.shape[0])\n",
    "    for i in range(inds_sub.shape[0]):\n",
    "        pca = PCA()#n_components=2)\n",
    "        pca.fit(mesh_obj.vertices[lens[inds_sub[i]]])\n",
    "\n",
    "        pca_vec[i] = pca.explained_variance_ratio_[0]\n",
    "\n",
    "    dists_defects_sub[dists_defects_sub < 4000] = 100\n",
    "    dists_defects_norm = dists_defects_sub #/ np.max(dists_defects_sub)\n",
    "    ranks_ep = sizes_sub / dists_defects_norm * (1-pca_vec)\n",
    "    ranks = sizes_sub**2 * (1-pca_vec)\n",
    "\n",
    "    #ranks_ep_errors_filt = ranks_ep[ranks_ep > .1]\n",
    "    centers_ep_send_errors = centers_sub[np.argsort(ranks_ep)][::-1][:20]\n",
    "    final_mask_eps = np.full(centers_ep_send_errors.shape[0], True)\n",
    "    tips_hit_send_ep = tips_hit_sub[np.argsort(ranks_ep)][::-1][:20]\n",
    "    uns, nums = np.unique(tips_hit_send_ep, return_counts=True)\n",
    "\n",
    "    for un, num in zip(uns, nums):\n",
    "        if num > 1:\n",
    "            final_mask_eps[np.argwhere(tips_hit_send_ep == un)[1:]] = False\n",
    "    centers_errors_ep = centers_ep_send_errors[final_mask_eps]\n",
    "    centers_errors = centers_sub[np.argsort(ranks)[::-1]][:20]\n",
    "    return centers_errors, centers_errors_ep, ranks, ranks_ep, path_to_root_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mesh_facets(mesh_obj, skel_mp, eps, path_to_root_dict, eps_nm):\n",
    "    print(\"Processing facets\")\n",
    "    locs = np.argwhere(mesh_obj.facets_area > 50000)\n",
    "\n",
    "    mesh_map = skel_mp.mesh_to_skel_map\n",
    "    mesh_coords = mesh_obj.vertices[mesh_obj.faces]\n",
    "    mean_locs = []\n",
    "    mesh_ind = []\n",
    "    fs = []\n",
    "    for l in tqdm(locs):\n",
    "        fs.append(np.sum(mesh_obj.facets_area[l]))\n",
    "        fc = mesh_obj.facets[l[0]]\n",
    "        vert_locs = mesh_coords[fc]\n",
    "        mean_locs.append(np.mean(vert_locs[:, 0], axis=0))\n",
    "        mesh_ind.append(fc[0])\n",
    "    mesh_ind = mesh_obj.faces[mesh_ind][:, 0]\n",
    "    mean_locs = np.array(mean_locs)\n",
    "    dists_defects_facets = np.zeros(mean_locs.shape[0])\n",
    "    mesh_map_facets = skel_mp.mesh_to_skel_map\n",
    "    closest_skel_pts_facets = mesh_map[[m for m in mesh_ind]]\n",
    "    dist_matrix_facets = distance_matrix(mean_locs, eps_nm)\n",
    "    ct = 0\n",
    "\n",
    "    closest_tip_facets = np.zeros((mean_locs.shape[0]))\n",
    "\n",
    "    for center in tqdm(mean_locs):\n",
    "\n",
    "        closest_skel_pt = closest_skel_pts_facets[ct]\n",
    "        eps_hit = []\n",
    "        for j, ep in enumerate(eps):\n",
    "            if closest_skel_pt in path_to_root_dict[ep]:\n",
    "                eps_hit.append(j)\n",
    "        if len(eps_hit) == 0:\n",
    "            dists_defects_facets[ct] = np.inf\n",
    "            ct+=1\n",
    "            continue\n",
    "        \n",
    "        dists = dist_matrix_facets[ct, eps_hit]\n",
    "        \n",
    "        amin = np.argmin(dists)\n",
    "        tip_hit = eps_hit[amin]\n",
    "        min_dist = dists[amin]\n",
    "        \n",
    "        closest_tip_facets[ct] = tip_hit\n",
    "        dists_defects_facets[ct] = min_dist\n",
    "        ct+=1\n",
    "    dists_defects_sub_facets = dists_defects_facets[dists_defects_facets < np.inf]\n",
    "    sizes_sub_facets = np.array(fs)[dists_defects_facets < np.inf]\n",
    "    mean_locs_facets = mean_locs[dists_defects_facets < np.inf]\n",
    "    tips_hit_sub_facets = closest_tip_facets[dists_defects_facets < np.inf]\n",
    "    closest_skel_pts_sub_facets = closest_skel_pts_facets[dists_defects_facets < np.inf]\n",
    "    inds_sub_facets = np.arange(mean_locs.shape[0])[dists_defects_facets < np.inf]\n",
    "    ranks_ep_facets = sizes_sub_facets**2 / dists_defects_sub_facets\n",
    "    #ranks_ep_facets_filt = ranks_ep_facets[ranks_ep_facets > 2e7]\n",
    "    mean_locs_send_facets = mean_locs_facets[np.argsort(ranks_ep_facets)][::-1][:20]\n",
    "    final_mask_facets = np.full(mean_locs_send_facets.shape[0], True)\n",
    "    tips_hit_send_facets = tips_hit_sub_facets[np.argsort(ranks_ep_facets)][::-1][:20]\n",
    "    uns, nums = np.unique(tips_hit_send_facets, return_counts=True)\n",
    "\n",
    "    for un, num in zip(uns, nums):\n",
    "        if num > 1:\n",
    "            final_mask_facets[np.argwhere(tips_hit_send_facets == un)[1:]] = False\n",
    "    facets_send_final = mean_locs_send_facets[final_mask_facets] / [4,4,40]\n",
    "    return facets_send_final, ranks_ep_facets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TIP FINDING FUNCTION\n",
    "def error_locs_defects(root_id, soma_id = None, soma_table=None, center_collapse=True):\n",
    "    #print(\"START\", root_id)\n",
    "\n",
    "    mesh_obj = get_and_process_mesh(root_id)\n",
    "    if mesh_obj is None:\n",
    "        return None\n",
    "    # SKELETONIZE - if we are just looking for general errors, not errors at endpoints, this can be skipped\n",
    "    try:\n",
    "        if soma_table==None:\n",
    "            soma_table = get_soma(str(soma_id))\n",
    "        if soma_table[soma_table.id == soma_id].shape[0] > 0:\n",
    "            center = np.array(soma_table[soma_table.id == soma_id].pt_position)[0] * [4,4,40]\n",
    "        else:\n",
    "            center=None\n",
    "    except:\n",
    "        center = None\n",
    "    print(\"Subselecting largest connected component of mesh\")\n",
    "    mesh_obj, encapsulated_ids, max_verts = process_mesh_ccs(mesh_obj)\n",
    "\n",
    "    skel_mp = skeletonize.skeletonize_mesh(trimesh_io.Mesh(mesh_obj.vertices, \n",
    "                                            mesh_obj.faces),\n",
    "                                            invalidation_d=15000,\n",
    "                                            shape_function='cone',\n",
    "                                            collapse_function='branch',\n",
    "#                                             soma_radius = soma_radius,\n",
    "                                            soma_pt=center,\n",
    "                                            smooth_neighborhood=5,\n",
    "                                            cc_vertex_thresh=max_verts - 10\n",
    "#                                                     collapse_params = {'dynamic_threshold':True}\n",
    "                                            )\n",
    "    print(\"Skel done\")\n",
    "\n",
    "    # find edges that only occur once..  might be faster to find these in the sparse matrix..\n",
    "    centers, lens = process_defects(mesh_obj)\n",
    "    eps, eps_nm = process_endpoints(mesh_obj, skel_mp)\n",
    "\n",
    "    if len(centers) !=0:\n",
    "        centers_errors, centers_errors_ep, ranks, ranks_ep, path_to_root_dict = process_mesh_errors(mesh_obj, centers, eps, eps_nm, lens, skel_mp)\n",
    "        ranks_return = np.squeeze(ranks[np.argsort(ranks)[::-1]][:20])\n",
    "        ranks_ep_return = np.squeeze(ranks_ep[np.argsort(ranks_ep)][::-1][:20])\n",
    "    else:\n",
    "        # Assign placeholder values for each of the variables above.\n",
    "        centers_errors = np.zeros ((1,3))\n",
    "        centers_errors_ep = np.zeros ((1,3))\n",
    "        ranks = np.zeros ((1))\n",
    "        ranks_ep = np.zeros((1, 3))\n",
    "        path_to_root_dict = {}\n",
    "        for ep in eps:\n",
    "            path_to_root_dict[ep] = skel_mp.path_to_root(ep)\n",
    "\n",
    "        ranks_return = 0\n",
    "        ranks_ep_return = 0\n",
    "\n",
    "\n",
    "    #if len(centers_errors.shape) > 1 and centers_errors.shape[0] > 0 and len(centers_errors_ep.shape) > 1 and len(centers_errors_ep.shape[0] > 0):\n",
    "    #    centers_errors = centers_errors[np.min(distance_matrix(centers_errors, centers_errors_ep), axis=1)>1000]\n",
    "    facets_send_final, ranks_ep_facets = process_mesh_facets(mesh_obj, skel_mp, eps, path_to_root_dict, eps_nm)\n",
    "\n",
    "    errors_send = centers_errors / [4,4,40]\n",
    "    errors_tips_send = centers_errors_ep / [4,4,40]\n",
    "    encapsulated_centers = [e[0] for e in encapsulated_ids]\n",
    "    encapsulated_lens = [e[1] for e in encapsulated_ids]\n",
    "    sorted_encapsulated_send = np.array(encapsulated_centers)[np.argsort(encapsulated_lens)][::-1]\n",
    "\n",
    "\n",
    "\n",
    "    return sorted_encapsulated_send, facets_send_final, errors_send, errors_tips_send, np.squeeze(ranks_ep_facets[[np.argsort(ranks_ep_facets)][::-1][:20]]), ranks_return, ranks_ep_return\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OUR tip generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_endpoints(row):\n",
    "    seg_id = row[\"seg_id\"]\n",
    "    sorted_encapsulated_send, facets_send_final, errors_send, errors_tips_send, dummy, dummy2, dummy3 = error_locs_defects(seg_id)\n",
    "    #facets_send_final = facets_send_final[facets_send_final != [0,0,0]]\n",
    "    #errors_tips_send = errors_tips_send[errors_tips_send != [0,0,0]]\n",
    "    together = np.vstack((facets_send_final, errors_tips_send))\n",
    "    #should alter \"together\" if not set together equal to this line\n",
    "    # print(together)\n",
    "    # new = together[together != [0,0,0]]\n",
    "    mask=np.sum(together,axis=1)\n",
    "    together = together[mask > 0]\n",
    "    return together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_endpoints2(row):\n",
    "    seg_id = row[\"seg_id\"]\n",
    "    sorted_encapsulated_send, facets_send_final, errors_send, errors_tips_send, dummy, dummy2, dummy3 = error_locs_defects(seg_id)\n",
    "    #facets_send_final = facets_send_final[facets_send_final != [0,0,0]]\n",
    "    #errors_tips_send = errors_tips_send[errors_tips_send != [0,0,0]]\n",
    "    result = [[0,0,0]]\n",
    "    count = 0\n",
    "    for point in facets_send_final:\n",
    "        if(np.array_equal(point, [0,0,0])):\n",
    "            continue\n",
    "        if(count == 0) :\n",
    "            result = point\n",
    "            count = count + 1\n",
    "            continue\n",
    "        result = np.vstack((result, point))\n",
    "    for tip in errors_tips_send:\n",
    "        if(np.array_equal(tip, [0,0,0])):\n",
    "            continue\n",
    "        result = np.vstack((result, point))\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASSUMING output from error_locs IS: \n",
    "facets_find_final: [[x,y,z], [x2,y2,z2], etc]\n",
    "errors_tips_send: [[x,y,z], [x2,y2,z2], etc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_endpoints(dataframe) :\n",
    "    dataframe[\"endpoints_generated\"] = dataframe.apply(find_endpoints, axis = 1)\n",
    "    return dataframe\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_endpoints2(dataframe) :\n",
    "    dataframe[\"endpoints_generated\"] = dataframe.apply(find_endpoints2, axis = 1)\n",
    "    return dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACCURACY FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_eps_acc(gt_endpoints, pred_endpoints, threshold):\n",
    "    # Calculate distances\n",
    "    dist_matrix = np.array(spatial.distance.cdist(gt_endpoints, pred_endpoints, metric = 'euclidean'))\n",
    "\n",
    "    # Apply threshold\n",
    "    dist_matrix[dist_matrix > threshold] = 0\n",
    "\n",
    "    # Calculating accuracy\n",
    "    valid_eps = np.count_nonzero(dist_matrix, axis = 1)\n",
    "    accuracy = np.count_nonzero(valid_eps) / len(gt_endpoints)\n",
    "\n",
    "    # If more than one valid endpoint found for a single ground truth endpoint, add the other valid endpoints to extra_valid_pairs\n",
    "    extra_valid_pairs = []\n",
    "    [[extra_valid_pairs.append([gt_endpoints[i], pred_endpoints[index]]) \\\n",
    "        for index, j in enumerate(dist_matrix[i]) if j != np.min(dist_matrix[i][dist_matrix[i] != 0]) if j != 0] \\\n",
    "            for i in valid_eps if i > 1]\n",
    "\n",
    "    return accuracy, extra_valid_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seg_id</th>\n",
       "      <th>num_gt_eps</th>\n",
       "      <th>gt_eps</th>\n",
       "      <th>num_pred_eps</th>\n",
       "      <th>pred_eps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>864691136577830164</td>\n",
       "      <td>6</td>\n",
       "      <td>[[402188, 228684, 24029], [401258, 224832, 240...</td>\n",
       "      <td>6</td>\n",
       "      <td>[[400292, 220879, 24027], [401253, 228342, 244...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               seg_id  num_gt_eps  \\\n",
       "0  864691136577830164           6   \n",
       "\n",
       "                                              gt_eps  num_pred_eps  \\\n",
       "0  [[402188, 228684, 24029], [401258, 224832, 240...             6   \n",
       "\n",
       "                                            pred_eps  \n",
       "0  [[400292, 220879, 24027], [401253, 228342, 244...  "
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(columns=[\"seg_id\", \"num_gt_eps\", \"gt_eps\", \"num_pred_eps\", \"pred_eps\"])\n",
    "data.loc[len(data.index)] = [864691136577830164, 6, [\n",
    "    [402188, 228684, 24029], [401258, 224832, 24029], [401314, 228366, 24424], \\\n",
    "    [400199, 220721, 24029], [397870, 232292, 24004], [403272, 227529, 24394]], 6, \\\n",
    "    [[400292, 220879, 24027], [401253, 228342, 24408], [401253, 228342, 24409],\n",
    "        [402923, 227458, 24372], [402099, 228716, 24037], [402627, 231471, 24026]]]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "[[[401314, 228366, 24424], [401253, 228342, 24408]]]\n"
     ]
    }
   ],
   "source": [
    "acc, extra_valid_pairs = pred_eps_acc(\n",
    "    data.loc[0, \"gt_eps\"], data.loc[0, \"pred_eps\"], 100)\n",
    "print(acc)\n",
    "print(extra_valid_pairs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING ON FIRST 50 of ORPHANS DATAFRAME\n",
    "orphans_test = orphans.head(50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Mesh\n",
      "Warning: deduplication not currently supported for this layer's variable layered draco meshes\n",
      "Vertices:  7993\n",
      "Subselecting largest connected component of mesh\n",
      "Processing CC's\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7992/7992 [00:00<00:00, 1073939.63it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [250], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m orphans_test \u001b[39m=\u001b[39m generate_endpoints(orphans_test)\n",
      "Cell \u001b[0;32mIn [205], line 2\u001b[0m, in \u001b[0;36mgenerate_endpoints\u001b[0;34m(dataframe)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_endpoints\u001b[39m(dataframe) :\n\u001b[0;32m----> 2\u001b[0m     dataframe[\u001b[39m\"\u001b[39m\u001b[39mendpoints_generated\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataframe\u001b[39m.\u001b[39;49mapply(find_endpoints, axis \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m     \u001b[39mreturn\u001b[39;00m dataframe\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/frame.py:9555\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9544\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9546\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[1;32m   9547\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   9548\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9553\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[1;32m   9554\u001b[0m )\n\u001b[0;32m-> 9555\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/apply.py:746\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[1;32m    744\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[0;32m--> 746\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/apply.py:873\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 873\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[1;32m    875\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[1;32m    876\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/apply.py:889\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    887\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[1;32m    888\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[1;32m    890\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    891\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    892\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    893\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn [203], line 3\u001b[0m, in \u001b[0;36mfind_endpoints\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_endpoints\u001b[39m(row):\n\u001b[1;32m      2\u001b[0m     seg_id \u001b[39m=\u001b[39m row[\u001b[39m\"\u001b[39m\u001b[39mseg_id\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m     sorted_encapsulated_send, facets_send_final, errors_send, errors_tips_send, dummy, dummy2, dummy3 \u001b[39m=\u001b[39m error_locs_defects(seg_id)\n\u001b[1;32m      4\u001b[0m     \u001b[39m#facets_send_final = facets_send_final[facets_send_final != [0,0,0]]\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[39m#errors_tips_send = errors_tips_send[errors_tips_send != [0,0,0]]\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     together \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack((facets_send_final, errors_tips_send))\n",
      "Cell \u001b[0;32mIn [202], line 21\u001b[0m, in \u001b[0;36merror_locs_defects\u001b[0;34m(root_id, soma_id, soma_table, center_collapse)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSubselecting largest connected component of mesh\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m     mesh_obj, encapsulated_ids, max_verts \u001b[39m=\u001b[39m process_mesh_ccs(mesh_obj)\n\u001b[0;32m---> 21\u001b[0m     skel_mp \u001b[39m=\u001b[39m skeletonize\u001b[39m.\u001b[39;49mskeletonize_mesh(trimesh_io\u001b[39m.\u001b[39;49mMesh(mesh_obj\u001b[39m.\u001b[39;49mvertices, \n\u001b[1;32m     22\u001b[0m                                             mesh_obj\u001b[39m.\u001b[39;49mfaces),\n\u001b[1;32m     23\u001b[0m                                             invalidation_d\u001b[39m=\u001b[39;49m\u001b[39m15000\u001b[39;49m,\n\u001b[1;32m     24\u001b[0m                                             shape_function\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcone\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     25\u001b[0m                                             collapse_function\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mbranch\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     26\u001b[0m \u001b[39m#                                             soma_radius = soma_radius,\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m                                             soma_pt\u001b[39m=\u001b[39;49mcenter,\n\u001b[1;32m     28\u001b[0m                                             smooth_neighborhood\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m     29\u001b[0m                                             cc_vertex_thresh\u001b[39m=\u001b[39;49mmax_verts \u001b[39m-\u001b[39;49m \u001b[39m10\u001b[39;49m\n\u001b[1;32m     30\u001b[0m \u001b[39m#                                                     collapse_params = {'dynamic_threshold':True}\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m                                             )\n\u001b[1;32m     32\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSkel done\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m     \u001b[39m# find edges that only occur once..  might be faster to find these in the sparse matrix..\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/meshparty/skeletonize.py:223\u001b[0m, in \u001b[0;36mskeletonize_mesh\u001b[0;34m(mesh, soma_pt, soma_radius, collapse_soma, collapse_function, invalidation_d, smooth_vertices, compute_radius, shape_function, compute_original_index, verbose, smooth_iterations, smooth_neighborhood, smooth_r, cc_vertex_thresh, root_index, remove_zero_length_edges, collapse_params, meta)\u001b[0m\n\u001b[1;32m    221\u001b[0m         rs \u001b[39m=\u001b[39m ray_trace_distance(orig_skel_index[vert_filter], mesh)\n\u001b[1;32m    222\u001b[0m     \u001b[39melif\u001b[39;00m shape_function \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcone\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 223\u001b[0m         rs \u001b[39m=\u001b[39m shape_diameter_function(orig_skel_index[vert_filter], mesh)\n\u001b[1;32m    224\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     rs \u001b[39m=\u001b[39m rs[vert_filter]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/meshparty/ray_tracing.py:214\u001b[0m, in \u001b[0;36mshape_diameter_function\u001b[0;34m(mesh_inds, mesh, num_points, cone_angle)\u001b[0m\n\u001b[1;32m    210\u001b[0m rep_inds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([ii\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39mones(num_points, dtype\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m)\n\u001b[1;32m    211\u001b[0m                            \u001b[39mfor\u001b[39;00m ii \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(start\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])])\n\u001b[1;32m    212\u001b[0m starts \u001b[39m=\u001b[39m start[rep_inds]\n\u001b[0;32m--> 214\u001b[0m vs \u001b[39m=\u001b[39m _compute_ray_vectors(mesh, mesh_inds, num_points, cone_angle)\n\u001b[1;32m    216\u001b[0m ray_inter \u001b[39m=\u001b[39m ray_pyembree\u001b[39m.\u001b[39mRayMeshIntersector(mesh)\n\u001b[1;32m    217\u001b[0m rtrace \u001b[39m=\u001b[39m ray_inter\u001b[39m.\u001b[39mintersects_location(starts, vs, multiple_hits\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/meshparty/ray_tracing.py:186\u001b[0m, in \u001b[0;36m_compute_ray_vectors\u001b[0;34m(mesh, mesh_inds, num_points, cone_angle)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_compute_ray_vectors\u001b[39m(mesh, mesh_inds, num_points, cone_angle):\n\u001b[0;32m--> 186\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mvstack(oriented_vector_cones(\u001b[39m-\u001b[39;49mmesh\u001b[39m.\u001b[39;49mvertex_normals[mesh_inds], num_points, cone_angle))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/meshparty/ray_tracing.py:137\u001b[0m, in \u001b[0;36moriented_vector_cones\u001b[0;34m(center_vectors, num_points, widest_angle, normalize)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mfor\u001b[39;00m phi, theta \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(phis, thetas):\n\u001b[1;32m    136\u001b[0m     data\u001b[39m.\u001b[39mappend((phi, theta, vs_raw))\n\u001b[0;32m--> 137\u001b[0m vector_cones \u001b[39m=\u001b[39m mu\u001b[39m.\u001b[39;49mmultiprocess_func(_rotated_cone, data)\n\u001b[1;32m    138\u001b[0m \u001b[39mreturn\u001b[39;00m vector_cones\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/multiwrapper/multiprocessing_utils.py:58\u001b[0m, in \u001b[0;36mmultiprocess_func\u001b[0;34m(func, params, debug, verbose, n_threads)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m debug:\n\u001b[1;32m     57\u001b[0m     pool \u001b[39m=\u001b[39m Pool(n_threads)\n\u001b[0;32m---> 58\u001b[0m     result \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49mmap(func, params)\n\u001b[1;32m     59\u001b[0m     pool\u001b[39m.\u001b[39mclose()\n\u001b[1;32m     60\u001b[0m     pool\u001b[39m.\u001b[39mjoin()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/pool.py:364\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[39m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[39m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, mapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    766\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    557\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 558\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    559\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    303\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "orphans_test = generate_endpoints(orphans_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.25\n",
      "0.0\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "XA must be a 2-dimensional array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [226], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#apply function to entire df \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m index, row \u001b[39min\u001b[39;00m orphans_test\u001b[39m.\u001b[39miterrows():\n\u001b[0;32m----> 3\u001b[0m     acc, extra_valid_pairs \u001b[39m=\u001b[39m pred_eps_acc(row[\u001b[39m\"\u001b[39;49m\u001b[39mreal_endpoints\u001b[39;49m\u001b[39m\"\u001b[39;49m], row[\u001b[39m\"\u001b[39;49m\u001b[39mendpoints_generated\u001b[39;49m\u001b[39m\"\u001b[39;49m], \u001b[39m100\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m     \u001b[39mprint\u001b[39m(acc)\n",
      "Cell \u001b[0;32mIn [211], line 3\u001b[0m, in \u001b[0;36mpred_eps_acc\u001b[0;34m(gt_endpoints, pred_endpoints, threshold)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpred_eps_acc\u001b[39m(gt_endpoints, pred_endpoints, threshold):\n\u001b[1;32m      2\u001b[0m     \u001b[39m# Calculate distances\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     dist_matrix \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(spatial\u001b[39m.\u001b[39;49mdistance\u001b[39m.\u001b[39;49mcdist(gt_endpoints, pred_endpoints, metric \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39meuclidean\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m      5\u001b[0m     \u001b[39m# Apply threshold\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     dist_matrix[dist_matrix \u001b[39m>\u001b[39m threshold] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/scipy/spatial/distance.py:2906\u001b[0m, in \u001b[0;36mcdist\u001b[0;34m(XA, XB, metric, out, **kwargs)\u001b[0m\n\u001b[1;32m   2903\u001b[0m sB \u001b[39m=\u001b[39m XB\u001b[39m.\u001b[39mshape\n\u001b[1;32m   2905\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(s) \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m-> 2906\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mXA must be a 2-dimensional array.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   2907\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(sB) \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m   2908\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mXB must be a 2-dimensional array.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: XA must be a 2-dimensional array."
     ]
    }
   ],
   "source": [
    "#apply function to entire df \n",
    "for index, row in orphans_test.iterrows():\n",
    "    acc, extra_valid_pairs = pred_eps_acc(row[\"real_endpoints\"], row[\"endpoints_generated\"], 100)\n",
    "    print(acc)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
