{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "from drive import segment_points\n",
    "from tip_finding import tip_finding\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: deduplication not currently supported for this layer's variable layered draco meshes\n",
      "branch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21334/21334 [00:00<00:00, 268041.25it/s]\n",
      "/Users/sheeltanna/campfire-1/tip_finding/tip_finding.py:516: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  locs = np.array([np.array(list(nx.get_node_attributes(g, 'mean_loc').values())) for g in graphs])\n",
      "100%|██████████| 14/14 [00:00<00:00, 628.76it/s]\n",
      "2it [00:00, 367.92it/s]\n",
      "1it [00:00, 225.74it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m good_tips_thick, good_tips_thin, good_tips_bad_thick, good_tips_bad_thin, just_tips, just_means  \u001b[39m=\u001b[39m tip_finding\u001b[39m.\u001b[39mendpoints_from_rid(root_id)\n\u001b[1;32m      6\u001b[0m \u001b[39m#our list of all endpoints to extend\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m endpoints \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate([good_tips_thick, good_tips_thin])\n\u001b[1;32m      8\u001b[0m \u001b[39m#OBTAIN ACTUAL THRESHOLD NUMBER\u001b[39;00m\n\u001b[1;32m      9\u001b[0m threshold \u001b[39m=\u001b[39m \u001b[39m0.3\u001b[39m\\\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "#use append and pop functions (acts LIFO)\n",
    "stack = deque()\n",
    "root_id = 864691135247440303\n",
    "good_tips_thick, good_tips_thin, good_tips_bad_thick, good_tips_bad_thin, just_tips, just_means  = tip_finding.endpoints_from_rid(root_id)\n",
    "#our list of all endpoints to extend\n",
    "endpoints = np.concatenate([good_tips_thick, good_tips_thin])\n",
    "#OBTAIN ACTUAL THRESHOLD NUMBER\n",
    "threshold = 0.3\\\n",
    "#this is for one SEG_ID--PUT INTO FUNCTION; run this for each seg_ID\n",
    "for tip in endpoints:\n",
    "    #reset stack everytime we move on to extending a new tip(endpoint)\n",
    "    next_seg_id = tip\n",
    "    while(max_confidence > threshold and next_seg_id.num_soma == (0)):\n",
    "        stack.append(next_seg_id)\n",
    "        ext, num = segment_points(endpoint=stack.pop(), root_id=root_id, point_id=0, resolution=[8,8,40])\n",
    "        #need to loop through values to get largest confidence\n",
    "        val_list = list(ext.merges.values())\n",
    "        key_list = list(ext.merges.keys())\n",
    "        max_confidence = max(ext.merges.values())\n",
    "        position = val_list.index(max_confidence)\n",
    "        #obtain the extension associated with highest confidence\n",
    "        next_seg_id = key_list[position]\n",
    "        #stack.append(next_seg_id)\n",
    "        #make sure this next seg_ID is an orphan and has enough confidence\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VERSION 2\n",
    "from collections import deque\n",
    "#use append and pop functions (acts LIFO)\n",
    "stack = deque()\n",
    "root_id = 864691135247440303\n",
    "good_tips_thick, good_tips_thin, good_tips_bad_thick, good_tips_bad_thin, just_tips, just_means  = tip_finding.endpoints_from_rid(root_id)\n",
    "#our list of all endpoints to extend\n",
    "endpoints = np.concatenate([good_tips_thick, good_tips_thin])\n",
    "#OBTAIN ACTUAL THRESHOLD NUMBER\n",
    "threshold = 0.3\n",
    "#this is for one SEG_ID--PUT INTO FUNCTION; run this for each seg_ID\n",
    " #add all endpoints in stack\n",
    "for tip in endpoints:\n",
    "    #reset stack everytime we move on to extending a new tip(endpoint)\n",
    "    #TUPLE: (SEG_ID1, tip1), (SEG_ID1, tip2)\n",
    "    stack.append((tip, root_id))\n",
    "while(stack):\n",
    "    next_seg_id_endpoint = stack.pop()\n",
    "    ext, num = segment_points(endpoint=next_seg_id_endpoint[0], root_id=next_seg_id_endpoint[1], point_id=0, resolution=[8,8,40])\n",
    "    #need to loop through values to get largest confidence\n",
    "    val_list = list(ext.merges.values())\n",
    "    key_list = list(ext.merges.keys())\n",
    "    max_confidence = max(ext.merges.values())\n",
    "    position = val_list.index(max_confidence)\n",
    "    #obtain the extension associated with highest confidence\n",
    "    next_seg_id = key_list[position]\n",
    "    if (max_confidence > threshold and next_seg_id.num_soma == (0)):\n",
    "        #get endpoints for next seg_ID\n",
    "        good_tips_thick, good_tips_thin, good_tips_bad_thick, good_tips_bad_thin, just_tips, \n",
    "        just_means = tip_finding.endpoints_from_rid(next_seg_id)\n",
    "        endpoints = np.concatenate([good_tips_thick, good_tips_thin])\n",
    "        #append all of the endpoints\n",
    "        for tip in endpoints:\n",
    "            #reset stack everytime we move on to extending a new tip(endpoint)\n",
    "            #TUPLE: (SEG_ID1, tip1), (SEG_ID1, tip2)\n",
    "            stack.append((tip, next_seg_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of pseudocode from ipad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "from drive import segment_points\n",
    "from tip_finding import tip_finding\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_endpoints_to_stack(extension_stack, root_id, ext_ids):\n",
    "    #root_id = root_id\n",
    "    # Find endpoints\n",
    "    good_tips_thick, good_tips_thin, good_tips_bad_thick, good_tips_bad_thin, just_tips, just_means = tip_finding.endpoints_from_rid(root_id)\n",
    "    endpoints = []\n",
    "    # if seg_id is not in ext_ids, then add it to ext_ids\n",
    "    if ((root_id in ext_ids[\"seg_id\"].unique()) == False):\n",
    "        #adding to end\n",
    "        ext_ids.loc[len(ext_ids.index)] = [root_id, 0, {}]\n",
    "\n",
    "        # FIND A BETTER WAY TO DO THIS\n",
    "        for i in good_tips_thick:\n",
    "            extension_stack.append((root_id, i))\n",
    "            endpoints.append(i)\n",
    "        for i in good_tips_thin:\n",
    "            extension_stack.append((root_id, i))\n",
    "            endpoints.append(i)\n",
    "        \n",
    "        endpoints_ext_dict = {}\n",
    "        for i in endpoints:\n",
    "            endpoints_ext_dict[tuple(i.tolist())] = 0\n",
    "            # endpoints_ext_dict[tuple(i)] = 0\n",
    "            #already a list right?\n",
    "        \n",
    "        #create vector or list of each collumn: one vector is all extensions the other is all endpoints but must match\n",
    "        #make them into 2 seperate collumns \n",
    "        #isn't ep_and_next_seg a tuple of dict,next_seg_id? so how are you just adding the dict directly without slicing? \n",
    "        ext_ids.loc[ext_ids['seg_id'] == root_id,\n",
    "                    'ep_and_next_seg'] = [endpoints_ext_dict]\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    # if seg_id is in ext_ids, check if it is fully extended. If it is not fully extended, add the non-extended points to the extension_stack\n",
    "    # Theoretically should never reach here because if we hit a seg_id, it should either be fully extended or not, but never in between, but this is here just in case ig <3 ...?\n",
    "    # Have not tested this!\n",
    "    else:\n",
    "        if (check_fully_extended(ext_ids = ext_ids, root_id = root_id) == False):\n",
    "            dict_of_endpoints_and_extensions = ext_ids.loc[ext_ids.seg_id == root_id].ep_and_next_seg[0]\n",
    "            # find endpoints in dict with next_seg_ids = 0 and add these to the extension_stack\n",
    "            endpoints = {i for i in dict_of_endpoints_and_extensions if dict_of_endpoints_and_extensions[i] == 0}\n",
    "            \n",
    "            # Push (root_id, endpoint) onto stack for each un-extended endpoint\n",
    "            for i in endpoints:\n",
    "                extension_stack.append((root_id, i))\n",
    "\n",
    "    return extension_stack, ext_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highest_confidence(ext):\n",
    "    # use ext to extrace highest confidence merge and associated confidence\n",
    "\n",
    "    #example:\n",
    "    highest_confidence_merge = 864691134406233920\n",
    "    highest_confidence = 0.8\n",
    "    \n",
    "    #implementation:\n",
    "    #need to loop through values to get largest confidence\n",
    "    val_list = list(ext.merges.values())\n",
    "    key_list = list(ext.merges.keys())\n",
    "    #index of highest value\n",
    "    highest_confidence = np.max(val_list)\n",
    "    highest_confidence_position = np.argmax(val_list)\n",
    "    #position = val_list.index(highest_confidence)\n",
    "    #obtain the extension associated with highest confidence\n",
    "    highest_confidence_merge = key_list[highest_confidence_position]\n",
    "    \n",
    "    return highest_confidence_merge, highest_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_fully_extended(ext_ids, root_id):\n",
    "    return 0 in ext_ids.loc[ext_ids.seg_id == root_id].ep_and_next_seg[0].values()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing all necessary values\n",
    "#creating our dataframe\n",
    "ext_ids = pd.DataFrame(columns=[\"seg_id\", \"fully_extended\", \"ep_and_next_seg\"])\n",
    "extension_stack = deque()\n",
    "min_confidence = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: deduplication not currently supported for this layer's variable layered draco meshes\n",
      "branch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21334/21334 [00:00<00:00, 101300.06it/s]\n",
      "/Users/RupaChalavadi/Desktop/CRIMSON/Summer/Campfire/campfire/tip_finding/tip_finding.py:516: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  locs = np.array([np.array(list(nx.get_node_attributes(g, 'mean_loc').values())) for g in graphs])\n",
      "100%|██████████| 14/14 [00:00<00:00, 292.15it/s]\n",
      "2it [00:00, 95.12it/s]\n",
      "1it [00:00, 64.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# Actually interacting with stack\n",
    "first_root_id = 864691135247440303\n",
    "extension_stack, ext_ids = append_endpoints_to_stack(extension_stack = extension_stack, root_id = first_root_id, ext_ids = ext_ids)\n",
    "\n",
    "while(len(extension_stack) != 0):\n",
    "#while(extension_stack)\n",
    "    #breaks up tuple for you?\n",
    "    curr_root_id, curr_endpoint = extension_stack.pop()\n",
    "    \n",
    "    # if this seg_id has been fully extended, skip it\n",
    "    if (int(ext_ids.loc[ext_ids.seg_id == curr_root_id].fully_extended) == 1):\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        ext, num = segment_points(endpoint = curr_endpoint, root_id = curr_root_id, point_id = 0, resolution = [8,8,40])\n",
    "        #ext = 0\n",
    "        next_seg, highest_confidence = get_highest_confidence(ext)\n",
    "        if (highest_confidence >= min_confidence):\n",
    "            # Updating the next_seg_id for the current endpoint in the ext_ids dataFrame\n",
    "            #updating the dictionary\n",
    "            ext_ids.loc[ext_ids.seg_id == curr_root_id].ep_and_next_seg[0][tuple(curr_endpoint.tolist())] = next_seg\n",
    "\n",
    "            # checking if curr_seg id is fully extended, if yes, update it in ext_ids\n",
    "            if (check_fully_extended(ext_ids = ext_ids, root_id = curr_root_id)):\n",
    "                ext_ids.loc[ext_ids.seg_id == curr_root_id].fully_extended = 1\n",
    "            \n",
    "            # If the next_seg is not fully extended, add it's endpoints to the stack\n",
    "            # This if statement is the only one that i haven't tested yet\n",
    "            # Avoids pushing endpoints in next_seg that are already segmented in append_endpoints_to_stack() - although this should theroetically never happen.\n",
    "            if ((next_seg not in ext_ids['seg_id'].unique()) or (int(ext_ids.loc[ext_ids.seg_id == next_seg].fully_extended) == 0)):\n",
    "                extension_stack, ext_ids = append_endpoints_to_stack(extension_stack = extension_stack, root_id = next_seg, ext_ids = ext_ids)\n",
    "\n",
    "        else:\n",
    "            ext_ids.loc[ext_ids.seg_id == curr_root_id].ep_and_next_seg = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: deduplication not currently supported for this layer's variable layered draco meshes\n",
      "branch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21334/21334 [00:00<00:00, 332310.62it/s]\n",
      "/Users/sheeltanna/campfire-1/tip_finding/tip_finding.py:516: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  locs = np.array([np.array(list(nx.get_node_attributes(g, 'mean_loc').values())) for g in graphs])\n",
      "100%|██████████| 14/14 [00:00<00:00, 646.73it/s]\n",
      "3it [00:00, 339.90it/s]\n",
      "1it [00:00, 195.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1620072.11650485  906418.7184466   944748.        ]\n",
      "[1635877.40909091  879300.86363636  955709.04545455]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "root_id = 864691135247440303\n",
    "    # Find endpoints\n",
    "good_tips_thick, good_tips_thin, good_tips_bad_thick, good_tips_bad_thin, just_tips, just_means = tip_finding.endpoints_from_rid(root_id)\n",
    "endpoints = []\n",
    "for i in good_tips_thick:\n",
    "    endpoints.append(i)\n",
    "    for i in good_tips_thin:\n",
    "        endpoints.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1620072.1165048543, 906418.718446602, 944748.0)\n",
      "(1635877.4090909092, 879300.8636363636, 955709.0454545454)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "endpoints_ext_dict = {}\n",
    "for i in endpoints:\n",
    "    # print(i)\n",
    "    # print()\n",
    "    endpoints_ext_dict[tuple(i)] = 0\n",
    "\n",
    "for i in endpoints_ext_dict.keys():\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('AGTS_2022': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5796726b1a94aac0f08323043a0a9796c93d03d7eeac7ac13359ed865c5f5621"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
