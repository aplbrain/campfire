{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "from drive import segment_points\n",
    "from tip_finding import tip_finding\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "#use append and pop functions (acts LIFO)\n",
    "stack = deque()\n",
    "root_id = 864691135247440303\n",
    "good_tips_thick, good_tips_thin, good_tips_bad_thick, good_tips_bad_thin, just_tips, just_means  = tip_finding.endpoints_from_rid(root_id)\n",
    "#our list of all endpoints to extend\n",
    "endpoints = np.concatenate([good_tips_thick, good_tips_thin])\n",
    "#OBTAIN ACTUAL THRESHOLD NUMBER\n",
    "threshold = 0.3\\\n",
    "#this is for one SEG_ID--PUT INTO FUNCTION; run this for each seg_ID\n",
    "for tip in endpoints:\n",
    "    #reset stack everytime we move on to extending a new tip(endpoint)\n",
    "    next_seg_id = tip\n",
    "    while(max_confidence > threshold and next_seg_id.num_soma == (0)):\n",
    "        stack.append(next_seg_id)\n",
    "        ext, num = segment_points(endpoint=stack.pop(), root_id=root_id, point_id=0, resolution=[8,8,40])\n",
    "        #need to loop through values to get largest confidence\n",
    "        val_list = list(ext.merges.values())\n",
    "        key_list = list(ext.merges.keys())\n",
    "        max_confidence = max(ext.merges.values())\n",
    "        position = val_list.index(max_confidence)\n",
    "        #obtain the extension associated with highest confidence\n",
    "        next_seg_id = key_list[position]\n",
    "        #stack.append(next_seg_id)\n",
    "        #make sure this next seg_ID is an orphan and has enough confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VERSION 2\n",
    "from collections import deque\n",
    "#use append and pop functions (acts LIFO)\n",
    "stack = deque()\n",
    "root_id = 864691135247440303\n",
    "good_tips_thick, good_tips_thin, good_tips_bad_thick, good_tips_bad_thin, just_tips, just_means  = tip_finding.endpoints_from_rid(root_id)\n",
    "#our list of all endpoints to extend\n",
    "endpoints = np.concatenate([good_tips_thick, good_tips_thin])\n",
    "#OBTAIN ACTUAL THRESHOLD NUMBER\n",
    "threshold = 0.3\n",
    "#this is for one SEG_ID--PUT INTO FUNCTION; run this for each seg_ID\n",
    " #add all endpoints in stack\n",
    "for tip in endpoints:\n",
    "    #reset stack everytime we move on to extending a new tip(endpoint)\n",
    "    #TUPLE: (SEG_ID1, tip1), (SEG_ID1, tip2)\n",
    "    stack.append((tip, root_id))\n",
    "while(stack):\n",
    "    next_seg_id_endpoint = stack.pop()\n",
    "    ext, num = segment_points(endpoint=next_seg_id_endpoint[0], root_id=next_seg_id_endpoint[1], point_id=0, resolution=[8,8,40])\n",
    "    #need to loop through values to get largest confidence\n",
    "    val_list = list(ext.merges.values())\n",
    "    key_list = list(ext.merges.keys())\n",
    "    max_confidence = max(ext.merges.values())\n",
    "    position = val_list.index(max_confidence)\n",
    "    #obtain the extension associated with highest confidence\n",
    "    next_seg_id = key_list[position]\n",
    "    if (max_confidence > threshold and next_seg_id.num_soma == (0)):\n",
    "        #get endpoints for next seg_ID\n",
    "        good_tips_thick, good_tips_thin, good_tips_bad_thick, good_tips_bad_thin, just_tips, \n",
    "        just_means = tip_finding.endpoints_from_rid(next_seg_id)\n",
    "        endpoints = np.concatenate([good_tips_thick, good_tips_thin])\n",
    "        #append all of the endpoints\n",
    "        for tip in endpoints:\n",
    "            #reset stack everytime we move on to extending a new tip(endpoint)\n",
    "            #TUPLE: (SEG_ID1, tip1), (SEG_ID1, tip2)\n",
    "            stack.append((tip, next_seg_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of pseudocode from ipad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "from drive import segment_points\n",
    "from tip_finding import tip_finding\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_endpoints_to_stack(extension_stack, root_id, ext_ids):\n",
    "    root_id = root_id\n",
    "    # Find endpoints\n",
    "    good_tips_thick, good_tips_thin, good_tips_bad_thick, good_tips_bad_thin, just_tips, just_means = tip_finding.endpoints_from_rid(root_id)\n",
    "    endpoints = []\n",
    "\n",
    "    # if seg_id is not in ext_ids, then add it to ext_ids\n",
    "    if ((root_id in ext_ids[\"seg_id\"].unique()) == False):\n",
    "        ext_ids.loc[len(ext_ids.index)] = [root_id, 0, {}]\n",
    "\n",
    "        # FIND A BETTER WAY TO DO THIS\n",
    "        for i in good_tips_thick:\n",
    "            extension_stack.append((root_id, i))\n",
    "            endpoints.append(i)\n",
    "        for i in good_tips_thin:\n",
    "            extension_stack.append((root_id, i))\n",
    "            endpoints.append(i)\n",
    "        \n",
    "        endpoints_ext_dict = {}\n",
    "        for i in endpoints:\n",
    "            endpoints_ext_dict[tuple(i.tolist())] = 0\n",
    "\n",
    "        ext_ids.loc[ext_ids['seg_id'] == first_root_id,\n",
    "                    'ep_and_next_seg'] = [endpoints_ext_dict]\n",
    "\n",
    "    \n",
    "    # if seg_id is in ext_ids, check if it is fully extended. If it is not fully extended, add the non-extended points to the extension_stack\n",
    "    # Theoretically should never reach here because if we hit a seg_id, it should either be fully extended or not, but never in between, but this is here just in case ig <3 ...?\n",
    "    # Have not tested this!\n",
    "    else:\n",
    "        if (check_fully_extended(ext_ids = ext_ids, root_id = root_id) == False):\n",
    "            dict_of_endpoints_and_extensions = ext_ids.loc[ext_ids.seg_id == root_id].ep_and_next_seg[0]\n",
    "            # find endpoints in dict with next_seg_ids = 0 and add these to the extension_stack\n",
    "            endpoints = {i for i in dict_of_endpoints_and_extensions if dict_of_endpoints_and_extensions[i] == 0}\n",
    "            \n",
    "            # Push (root_id, endpoint) onto stack for each un-extended endpoint\n",
    "            for i in endpoints:\n",
    "                extension_stack.append((root_id, i))\n",
    "\n",
    "    return extension_stack, ext_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highest_confidence(ext):\n",
    "    # use ext to extrace highest confidence merge and associated confidence\n",
    "\n",
    "    highest_confidence_merge = 864691134406233920\n",
    "    highest_confidence = 0.8\n",
    "    \n",
    "    return highest_confidence_merge, highest_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_fully_extended(ext_ids, root_id):\n",
    "    return 0 in ext_ids.loc[ext_ids.seg_id == root_id].ep_and_next_seg[0].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing all necessary values\n",
    "ext_ids = pd.DataFrame(columns=[\"seg_id\", \"fully_extended\", \"ep_and_next_seg\"])\n",
    "extension_stack = deque()\n",
    "min_confidence = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: deduplication not currently supported for this layer's variable layered draco meshes\n",
      "branch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21334/21334 [00:00<00:00, 101300.06it/s]\n",
      "/Users/RupaChalavadi/Desktop/CRIMSON/Summer/Campfire/campfire/tip_finding/tip_finding.py:516: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  locs = np.array([np.array(list(nx.get_node_attributes(g, 'mean_loc').values())) for g in graphs])\n",
      "100%|██████████| 14/14 [00:00<00:00, 292.15it/s]\n",
      "2it [00:00, 95.12it/s]\n",
      "1it [00:00, 64.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# DRIVER CELL\n",
    "\n",
    "# Actually interacting with stack\n",
    "first_root_id = 864691135247440303\n",
    "extension_stack, ext_ids = append_endpoints_to_stack(extension_stack = extension_stack, root_id = first_root_id, ext_ids = ext_ids)\n",
    "\n",
    "while(len(extension_stack) != 0):\n",
    "    curr_root_id, curr_endpoint = extension_stack.pop()\n",
    "    \n",
    "    # if this seg_id has been fully extended, skip it\n",
    "    if (int(ext_ids.loc[ext_ids.seg_id == curr_root_id].fully_extended) == 1):\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        # ext, num = segment_points(endpoint = curr_endpoint, root_id = curr_root_id, point_id = 0, resolution = [8,8,40])\n",
    "        ext = 0\n",
    "        next_seg, highest_confidence = get_highest_confidence(ext)\n",
    "        if (highest_confidence >= min_confidence):\n",
    "            # Updating the next_seg_id for the current endpoint in the ext_ids dataFrame\n",
    "            ext_ids.loc[ext_ids.seg_id == curr_root_id].ep_and_next_seg[0][tuple(curr_endpoint.tolist())] = next_seg\n",
    "\n",
    "            # checking if curr_seg id is fully extended, if yes, update it in ext_ids\n",
    "            if (check_fully_extended(ext_ids = ext_ids, root_id = curr_root_id)):\n",
    "                ext_ids.loc[ext_ids.seg_id == curr_root_id].fully_extended = 1\n",
    "            \n",
    "            # If the next_seg is not fully extended, add it's endpoints to the stack\n",
    "            # This if statement is the only one that i haven't tested yet\n",
    "            # Avoids pushing endpoints in next_seg that are already segmented in append_endpoints_to_stack() - although this should theroetically never happen.\n",
    "            if ((next_seg not in ext_ids['seg_id'].unique()) or (int(ext_ids.loc[ext_ids.seg_id == next_seg].fully_extended) == 0)):\n",
    "                extension_stack, ext_ids = append_endpoints_to_stack(extension_stack = extension_stack, root_id = next_seg, ext_ids = ext_ids)\n",
    "\n",
    "        else:\n",
    "            ext_ids.loc[ext_ids.seg_id == curr_root_id].ep_and_next_seg = 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
