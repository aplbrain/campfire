{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from caveclient import CAVEclient\n",
    "from intern import array\n",
    "import pickle\n",
    "import numpy as np\n",
    "from agents import data_loader\n",
    "from cloudvolume import CloudVolume\n",
    "from membrane_detection import membranes\n",
    "from agents.scripts import precompute_membrane_vectors, create_post_matrix, merge_paths, get_soma\n",
    "import agents.sensor\n",
    "from agents.run import run_agents\n",
    "import aws.sqs as sqs\n",
    "import sys\n",
    "import time\n",
    "import ast\n",
    "import pandas as pd\n",
    "import agents.scripts as scripts\n",
    "from drive import drive\n",
    "from finding_orphans import *\n",
    "from math import sqrt\n",
    "from tip_finding.tip_finding import tip_finder_decimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_euclidian_distance(proposed_endpoint, gt_endponts_array):\n",
    "    diffs = gt_endponts_array - proposed_endpoint\n",
    "    diffs_distance = np.sqrt(np.sum(np.square(diffs), axis=1))\n",
    "    min_dist_ind = np.argmin(diffs_distance)\n",
    "    min_dist = diffs_distance[min_dist_ind]\n",
    "    return min_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def endpoint_generator(load, invalidation_d, humfrey_iters, decimation_factor):\n",
    "    for idx, i in enumerate(load['seg_id']):\n",
    "        try:\n",
    "            t1, skel, mesh_obj = tip_finder_decimation(root_id = str(i), inval_d = invalidation_d, num_humfrey_iters = humfrey_iters, decimation_factor= decimation_factor)\n",
    "            endpoints_dict[i] = t1\n",
    "            print(\"proposed endpoints: \")\n",
    "            print(t1)\n",
    "        except:\n",
    "            print(f\"\\n\\nSeg {i} returned error on get. Skipping.\\n\")\n",
    "            pass\n",
    "    load['endpoints'] = load['endpoints'].apply(\n",
    "        lambda x: list(ast.literal_eval(x)))\n",
    "    load['proposed_endpoints'] = load.seg_id.map(endpoints_dict)\n",
    "    return load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_metrics(load, threshold, run, output):\n",
    "    for index, row in load.iterrows():\n",
    "        # if idx < 123:\n",
    "        #     pass\n",
    "        # elif idx > 123:\n",
    "        #     pass\n",
    "\n",
    "        endpoint_array = np.array(row[\"endpoints\"])\n",
    "        proposed_endpoints_array = np.array(row[\"proposed_endpoints\"])\n",
    "        segID = row[\"seg_id\"]\n",
    "\n",
    "        # #skip anything seg_id that isn't 'good'\n",
    "        # if row[\"comments\"] != 'good':\n",
    "        #     print(\"skipping \\n\")\n",
    "        #     continue\n",
    "\n",
    "        #if both say no endpoints, then it's correct\n",
    "        if (len(proposed_endpoints_array.shape) == 0 and len(endpoint_array.shape) == 0):\n",
    "            output.loc[len(output.index)] = [run, 1.0,1.0,1.0, segID, len(\n",
    "                endpoint_array), endpoint_array, row[\"comments\"], proposed_endpoints_array]\n",
    "            continue\n",
    "\n",
    "        #if we propose no endpoints but there are endpoints, it's wrong\n",
    "        elif (len(proposed_endpoints_array.shape) == 0 and len(endpoint_array.shape) > 0):\n",
    "            output.loc[len(output.index)] = [run, 0.0,0.0,0.0, segID, len(\n",
    "                endpoint_array), endpoint_array, row[\"comments\"], proposed_endpoints_array]\n",
    "            continue\n",
    "\n",
    "        #if we propose endpoints but there are none, it's wrong\n",
    "        elif proposed_endpoints_array.size > 0 and endpoint_array.size == 0:\n",
    "            output.loc[len(output.index)] = [run, 0.0,0.0,0.0, segID, len(\n",
    "                endpoint_array), endpoint_array, row[\"comments\"], proposed_endpoints_array]\n",
    "            continue\n",
    "\n",
    "        #should skip for now if the endpoint array is size zero to avoid divide by zero error\n",
    "        elif endpoint_array.size == 0:\n",
    "            output.loc[len(output.index)] = [run,-1,-1,-1,segID, len(endpoint_array), endpoint_array, row[\"comments\"], proposed_endpoints_array]\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            endpoint_ids = np.arange(0, len(endpoint_array))\n",
    "            test_ids = np.arange(\n",
    "                0, len(proposed_endpoints_array)) + len(proposed_endpoints_array)\n",
    "\n",
    "            #get precision and recall at end of each iteration\n",
    "            analysis = run_synapse_analysis(\n",
    "                endpoint_array,\n",
    "                np.array(endpoint_ids),\n",
    "                proposed_endpoints_array,\n",
    "                np.array(test_ids),\n",
    "                threshold,\n",
    "                iso_correction=10,\n",
    "            )\n",
    "\n",
    "            output.loc[len(output.index)] = [run, analysis.precision, analysis.recall, analysis.f1, segID, len(\n",
    "                endpoint_array), endpoint_array, row[\"comments\"], proposed_endpoints_array]\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Various utility classes and functions for Confirms.\n",
    "\"\"\"\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "def calculate_precision_recall(tp, fp, fn):\n",
    "    \"\"\"\n",
    "    Calculate precision/recall from given true positives, false positives, and false negatives.\n",
    "    Parameters\n",
    "    ----------\n",
    "    tp : int\n",
    "        The number of true positives.\n",
    "    fp : int\n",
    "        The number of false positives.\n",
    "    fn : int\n",
    "        The number of false negatives.\n",
    "    Returns\n",
    "    -------\n",
    "    precision : float\n",
    "        The precision score.\n",
    "    recall : float\n",
    "        The recall score.\n",
    "    \"\"\"\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def calculate_f1(precision, recall):\n",
    "    \"\"\"\n",
    "    Calculate the F1 score from precision/recall scores.\n",
    "    Parameters\n",
    "    ----------\n",
    "    precision : float\n",
    "        The precision score.\n",
    "    recall : float\n",
    "        The recall score.\n",
    "    Returns\n",
    "    -------\n",
    "    f1 : float\n",
    "        The F1 score.\n",
    "    \"\"\"\n",
    "    return 2 * ((precision * recall) / (precision + recall))\n",
    "\n",
    "\n",
    "def get_summary_metrics(array):\n",
    "    \"\"\"\n",
    "    Calculate a number of summary metrics on an array of numbers.\n",
    "    Parameters\n",
    "    ----------\n",
    "    array : array_like\n",
    "        Array containing numbers who summary metrics is desired.\n",
    "    Returns\n",
    "    -------\n",
    "    metrics : dict\n",
    "        Dict containing the mean, median, max, min, range, standard deviation,\n",
    "        and variance of the input array.\n",
    "    \"\"\"\n",
    "    summary_object = {}\n",
    "    array = np.array(array)\n",
    "\n",
    "    summary_object[\"mean\"] = np.mean(array)\n",
    "    summary_object[\"median\"] = np.median(array)\n",
    "    summary_object[\"max\"] = np.amax(array)\n",
    "    summary_object[\"min\"] = np.amin(array)\n",
    "    summary_object[\"range\"] = np.mean(array)\n",
    "    summary_object[\"stddev\"] = np.std(array)\n",
    "    summary_object[\"variance\"] = np.var(array)\n",
    "\n",
    "    return summary_object\n",
    "\n",
    "\n",
    "def munkres_assignment(workers, jobs):\n",
    "    \"\"\"\n",
    "    Perform hungarian-munkres assignment.\n",
    "    Parameters\n",
    "    ----------\n",
    "    workers : array_like\n",
    "        Array containing the first set of points.\n",
    "    jobs : array_like\n",
    "        Array containing the second set of points.\n",
    "    Returns\n",
    "    -------\n",
    "    cost_matrix : numpy.ndarray\n",
    "        Matrix containing pairwise distances (the cost of assignment).\n",
    "    row_ind : numpy.ndarray\n",
    "        Row indices of cost_matrix for optimal assignment.\n",
    "    col_ind : numpy.ndarray\n",
    "        Column indices of cost_matrx for optimal assignment.\n",
    "    \"\"\"\n",
    "    cost_matrix = spatial.distance.cdist(workers, jobs, \"euclidean\")\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    return cost_matrix, row_ind, col_ind\n",
    "\n",
    "\n",
    "def make_isotropic(xyz, correction, dimen=2):\n",
    "    \"\"\"\n",
    "    Correct anisotropy in a collection of x, y, z coordinates.\n",
    "    This function performs a deepcopy of xyz before making the necessary modifications.\n",
    "    Parameters\n",
    "    ----------\n",
    "    xyz : numpy.ndarray or pandas.DataFrame:\n",
    "        The coordinate values.\n",
    "    correction : float\n",
    "        The value to correct anisotrophy.\n",
    "    dimen : int\n",
    "        Index of last dimension to which to apply the correction. Default is 2.\n",
    "    Returns\n",
    "    -------\n",
    "    iso_xyz : numpy.ndarray or pandas.DataFrame\n",
    "        An isotropic version of xyz.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(xyz, np.ndarray) and not isinstance(xyz, pd.DataFrame):\n",
    "        xyz = np.asarray(xyz)\n",
    "\n",
    "    shape = np.shape(xyz)\n",
    "    isotropic_xyz = xyz.copy()\n",
    "    size = shape[-1]\n",
    "    if dimen >= size or dimen < 0:\n",
    "        raise ValueError(\n",
    "            \"improper dimen value (valid: 0 through {})\".format(size - 1))\n",
    "    if isinstance(xyz, np.ndarray):\n",
    "        isotropic_xyz[..., dimen] = isotropic_xyz[..., dimen] * correction\n",
    "    else:\n",
    "        isotropic_xyz.iloc[:, dimen] = isotropic_xyz.iloc[:,\n",
    "                                                          dimen] * correction\n",
    "    return isotropic_xyz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Confirms synapse processing and analysis functions.\n",
    "\"\"\"\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# from . import utils\n",
    "# import utils\n",
    "\n",
    "SynapseMetrics = namedtuple(\n",
    "    \"SynapseMetrics\",\n",
    "    [\"precision\", \"recall\", \"f1\", \"tp_gt_ids\", \"tp_test_ids\", \"fp_ids\", \"fn_ids\"],\n",
    ")\n",
    "\n",
    "\n",
    "def filter_synapse_id_core(volume, xyz, ids, box_radius_nm=2500):\n",
    "    \"\"\"\n",
    "    Filter synapses to only return those within a central core.\n",
    "    Parameters\n",
    "    ----------\n",
    "    volume : dict\n",
    "        The volume to filter on.\n",
    "    xyz : array_like\n",
    "        Synapse xyz coordinates.\n",
    "    ids : array_like\n",
    "        Synapse hash (ids).\n",
    "    box_radius_nm : int\n",
    "        radius of cube, from volume core.\n",
    "    Returns\n",
    "    -------\n",
    "        xyz : numpy.ndarray\n",
    "            Synapse coordinates.\n",
    "        ids : numpy.ndarray\n",
    "            Synapse ids.\n",
    "    \"\"\"\n",
    "\n",
    "    xyz_out = []\n",
    "    id_out = []\n",
    "    center = np.asarray(volume[\"center\"], \"float\")\n",
    "    base_resolution = np.asarray(volume[\"base_resolution\"], \"float\")\n",
    "    annotation_resolution = np.asarray(volume[\"resolution\"], \"float\")\n",
    "\n",
    "    pad_vx = box_radius_nm / base_resolution[0] / (2 ** annotation_resolution)\n",
    "    pad_vy = box_radius_nm / base_resolution[1] / (2 ** annotation_resolution)\n",
    "    pad_vz = box_radius_nm / base_resolution[2]\n",
    "    xr = (center[0] - pad_vx, center[0] + pad_vx)\n",
    "    yr = (center[1] - pad_vy, center[1] + pad_vy)\n",
    "    zr = (center[2] - pad_vz, center[2] + pad_vz)\n",
    "\n",
    "    for i in range(len(xyz)):\n",
    "        x = xyz[i][0]\n",
    "        y = xyz[i][1]\n",
    "        z = xyz[i][2]\n",
    "\n",
    "        if (\n",
    "            x > xr[0]\n",
    "            and x < xr[1]\n",
    "            and y > yr[0]\n",
    "            and y < yr[1]\n",
    "            and z > zr[0]\n",
    "            and z < zr[1]\n",
    "        ):\n",
    "            xyz_out.append(xyz[i])\n",
    "            id_out.append(ids[i])\n",
    "\n",
    "    return np.array(xyz_out), np.array(id_out, dtype=np.object)\n",
    "\n",
    "\n",
    "def synapse_match(\n",
    "    xyz_truth, xyz_detect, id_truth, id_detect, thresh\n",
    "):  # pylint: disable=R0914\n",
    "    \"\"\"\n",
    "    <Description here>\n",
    "    Parameters\n",
    "    ----------\n",
    "    xyz_truth : array_like\n",
    "        <description>\n",
    "    xyz_detect : array_like\n",
    "        <description>\n",
    "    id_truth : array_like\n",
    "        <description>\n",
    "    id_detech : array_like\n",
    "        <description>\n",
    "    thresh : float\n",
    "        <description>\n",
    "    Returns\n",
    "    -------\n",
    "    id_lookup : <type>\n",
    "        <description>\n",
    "    \"\"\"\n",
    "\n",
    "    # pylint: disable=C0103\n",
    "\n",
    "    # Ensure we have numpy arrays\n",
    "    xyz_truth = np.asarray(xyz_truth)\n",
    "    xyz_detect = np.asarray(xyz_detect)\n",
    "    id_truth = np.asarray(id_truth)\n",
    "    id_detect = np.asarray(id_detect)\n",
    "\n",
    "    cost, row_ind, col_ind = munkres_assignment(xyz_truth, xyz_detect)\n",
    "    print(cost)\n",
    "    match_idx = np.where(cost[row_ind, col_ind] < thresh)\n",
    "\n",
    "    if len(match_idx) > 0:\n",
    "        # row is idx of GT TP\n",
    "        # col is idx of student TP\n",
    "        gt_tp_idx, det_tp_idx = row_ind[match_idx], col_ind[match_idx]\n",
    "        gt_tp_ids, det_tp_ids = id_truth[gt_tp_idx], id_detect[det_tp_idx]\n",
    "        # Combine into pairs\n",
    "        id_lookup = np.column_stack((gt_tp_ids, det_tp_ids))\n",
    "    else:\n",
    "        gt_tp_idx = []\n",
    "        det_tp_idx = []\n",
    "        id_lookup = np.column_stack(\n",
    "            (np.array([], dtype=\"object\"), np.array([], dtype=\"object\"))\n",
    "        )\n",
    "\n",
    "    # not in row (set diff) are FN\n",
    "    gt_syn_idx = np.arange(0, len(xyz_truth))\n",
    "    fn_idx = np.setdiff1d(gt_syn_idx, gt_tp_idx)\n",
    "    if len(fn_idx) > 0:\n",
    "        fn_ids = id_truth[fn_idx]\n",
    "        id_lookup_fn = np.column_stack((fn_ids, np.repeat(None, len(fn_ids))))\n",
    "        id_lookup = np.concatenate((id_lookup, id_lookup_fn))\n",
    "\n",
    "    # not in col (set diff) are FP\n",
    "    det_syn_idx = set(np.arange(0, len(xyz_detect)))\n",
    "    fp_idx = np.asarray(list(det_syn_idx.difference(det_tp_idx)))\n",
    "    if len(fp_idx) > 0:\n",
    "        fp_ids = id_detect[fp_idx]\n",
    "        id_lookup_fp = np.column_stack((np.repeat(None, len(fp_ids)), fp_ids))\n",
    "        id_lookup = np.concatenate((id_lookup, id_lookup_fp))\n",
    "\n",
    "    return pd.DataFrame(id_lookup, columns=[\"ground_truth\", \"detect\"])\n",
    "\n",
    "\n",
    "def run_synapse_analysis(\n",
    "    gt_xyzs,\n",
    "    gt_ids,\n",
    "    test_xyzs,\n",
    "    test_ids,\n",
    "    threshold,\n",
    "    iso_corrected=False,\n",
    "    iso_correction=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    <Description here>\n",
    "    Parameters\n",
    "    ----------\n",
    "    gt_xyzs : numpy.ndarray\n",
    "        Array of ground truth xyz coordinates\n",
    "    gt_ids : numpy.ndarray\n",
    "        Array of ids associated with ground truth xyz coordinates\n",
    "    test_xyzs : numpy.ndarray\n",
    "        Array of test xyz coordinates\n",
    "    test_ids : numpy.ndarray\n",
    "        Array of ids associated with test xyz coordinates\n",
    "    threshold : float\n",
    "        Synapse matching threshold\n",
    "    iso_corrected : boolean\n",
    "        Mark whether the data is isotropic. If not, it will be made isotropic using the\n",
    "        `iso_correction` parameter.\n",
    "    iso_correction : float\n",
    "        Value to correct anistropy.\n",
    "    Returns\n",
    "    -------\n",
    "    sm : SynapseMetrics\n",
    "        Resultant object containing precision, recall, and F1 scores, along with\n",
    "        with true positive (both ground truth and test), false positive, and false negative\n",
    "        ids.\n",
    "    \"\"\"\n",
    "    # pylint: disable=R0913,R0914\n",
    "    if not iso_corrected:\n",
    "        # gt_xyzs = utils.make_isotropic(gt_xyzs, iso_correction)\n",
    "        gt_xyzs = make_isotropic(gt_xyzs, iso_correction)\n",
    "        # test_xyzs = utils.make_isotropic(test_xyzs, iso_correction)\n",
    "        test_xyzs = make_isotropic(test_xyzs, iso_correction)\n",
    "\n",
    "    results_table = synapse_match(\n",
    "        gt_xyzs, test_xyzs, gt_ids, test_ids, threshold)\n",
    "\n",
    "    tp = results_table.dropna()\n",
    "    fn = results_table[results_table.detect.isnull()]\n",
    "    fp = results_table[results_table.ground_truth.isnull()]\n",
    "\n",
    "    assert len(tp.ground_truth) == len(\n",
    "        tp.detect\n",
    "    ), \"true positive ground truth and test size mismatch\"\n",
    "\n",
    "    tp_count = len(tp)\n",
    "    fp_count = len(fp)\n",
    "    fn_count = len(fn)\n",
    "\n",
    "    try:\n",
    "        # precision, recall = utils.calculate_precision_recall(\n",
    "        #     tp_count, fp_count, fn_count\n",
    "        # )\n",
    "\n",
    "        precision, recall = calculate_precision_recall(\n",
    "            tp_count, fp_count, fn_count\n",
    "        )\n",
    "\n",
    "    except ZeroDivisionError:\n",
    "        precision, recall = np.nan, np.nan\n",
    "\n",
    "    try:\n",
    "        # f1 = utils.calculate_f1(precision, recall)\n",
    "        f1 = calculate_f1(precision, recall)\n",
    "    except ZeroDivisionError:\n",
    "        f1 = np.nan\n",
    "\n",
    "    sm = SynapseMetrics(\n",
    "        precision=precision,\n",
    "        recall=recall,\n",
    "        f1=f1,\n",
    "        tp_gt_ids=np.asarray(tp.ground_truth),\n",
    "        tp_test_ids=np.asarray(tp.detect),\n",
    "        fp_ids=np.asarray(fp.detect),\n",
    "        fn_ids=np.asarray(fn.ground_truth),\n",
    "    )\n",
    "    return sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in data\n",
    "load = pd.read_csv(\"agents/Data/endpoints_gt5.csv\")\n",
    "endpoints_dict = {}\n",
    "\n",
    "# #creating series objects for the desired collumns\n",
    "# segIDs = load['seg_id']\n",
    "# Endpoints = load['endpoints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate out_df\n",
    "\n",
    "out_df = pd.DataFrame(columns=[\"run\", \"precision\", \"recall\", \"f1\", \"seg_id\",\n",
    "                      \"num_endpoints\", \"endpoints\", \"comments\", \"proposed_endpoints\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = 0\n",
    "# for invalidation_d in range(4000, 6001, 200):\n",
    "for invalidation_d in range(4000, 4201, 200):\n",
    "    # for num_humfrey_iters in range(0,251,25):\n",
    "    for num_humfrey_iters in range(0, 251, 25):\n",
    "        # for decimation_factor in np.arange(0.3, 0.71, 0.04):\n",
    "        for decimation_factor in np.linspace(0.3, 0.71, num = 10):\n",
    "            load = pd.read_csv(\"agents/Data/endpoints_gt5.csv\") #NOT THE BEST WAY TO DO THIS!!!\n",
    "            run += 1\n",
    "            load_with_proposed_endpoints = endpoint_generator(\n",
    "                load, invalidation_d=invalidation_d, humfrey_iters=num_humfrey_iters, decimation_factor=decimation_factor)\n",
    "            out_df = testing_metrics(\n",
    "                load_with_proposed_endpoints, threshold=500, run=run, output=out_df)\n",
    "#for loop bug fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load = pd.read_csv(\"agents/Data/endpoints_gt5.csv\")\n",
    "load = load[load[\"seg_id\"] == 864691135489218668]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING ALL PARAM COMBOS ON A SINGLE SEG ID\n",
    "\n",
    "load = pd.read_csv(\"agents/Data/endpoints_gt5.csv\")\n",
    "load = load[load[\"seg_id\"] == 864691135489218668]\n",
    "endpoints_dict = {}\n",
    "print(load)\n",
    "# Generate out_df\n",
    "\n",
    "out_df = pd.DataFrame(columns=[\"run\", \"precision\", \"recall\", \"f1\", \"seg_id\",\n",
    "                      \"num_endpoints\", \"endpoints\", \"comments\", \"proposed_endpoints\"])\n",
    "#print(out_df)\n",
    "\n",
    "run = 0\n",
    "for invalidation_d in range(5200, 5600, 200):\n",
    "# for invalidation_d in range(4000, 4201, 200):\n",
    "    for num_humfrey_iters in range(0,250,25):\n",
    "    # for num_humfrey_iters in range(50, 76, 25):=\n",
    "        for decimation_factor in np.linspace(0.1, 0.9, num = 10):\n",
    "        # for decimation_factor in np.linspace(0.3, 0.35, num=2):\n",
    "            # NOT THE BEST WAY TO DO THIS!!!\n",
    "            load = pd.read_csv(\"agents/Data/endpoints_gt5.csv\")\n",
    "            load = load[load[\"seg_id\"] == 864691135489218668]\n",
    "            load_with_proposed_endpoints = endpoint_generator(\n",
    "                load, invalidation_d=invalidation_d, humfrey_iters=num_humfrey_iters, decimation_factor=decimation_factor)\n",
    "            #print(load.iloc[0][\"proposed_endpoints\"])\n",
    "            print(load_with_proposed_endpoints.iloc[0][\"proposed_endpoints\"])\n",
    "            run += 1\n",
    "            out_df = testing_metrics(\n",
    "                load_with_proposed_endpoints, threshold=500, run=run, output=out_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.to_csv('results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
