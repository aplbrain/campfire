{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from caveclient import CAVEclient\n",
    "from intern import array\n",
    "import pickle\n",
    "import numpy as np\n",
    "from agents import data_loader\n",
    "from cloudvolume import CloudVolume\n",
    "from membrane_detection import membranes\n",
    "from agents.scripts import precompute_membrane_vectors, create_post_matrix, merge_paths, get_soma\n",
    "import agents.sensor\n",
    "from agents.run import run_agents\n",
    "import aws.sqs as sqs\n",
    "import sys\n",
    "import time\n",
    "import ast\n",
    "import pandas as pd\n",
    "import agents.scripts as scripts\n",
    "from drive import drive\n",
    "from finding_orphans import *\n",
    "from math import sqrt\n",
    "from tip_finding import tip_finder_decimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_euclidian_distance(proposed_endpoint, gt_endponts_array):\n",
    "    diffs = gt_endponts_array - proposed_endpoint\n",
    "    diffs_distance = np.sqrt(np.sum(np.square(diffs), axis=1))\n",
    "    min_dist_ind = np.argmin(diffs_distance)\n",
    "    min_dist = diffs_distance[min_dist_ind]\n",
    "    return min_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def endpoint_generator_voxelise(load, invalidation_d, cube_side, decimation_factor):\n",
    "    for idx, i in enumerate(load['seg_id']):\n",
    "        try:\n",
    "            t1, skel, mesh_obj = tip_finder_decimation(root_id = str(i), inval_d = invalidation_d, cube_side_len = cube_side, decimation_factor= decimation_factor)\n",
    "            endpoints_dict[i] = t1\n",
    "            print(\"proposed endpoints: \")\n",
    "            print(t1)\n",
    "        except:\n",
    "            print(f\"\\n\\nSeg {i} returned error on get. Skipping.\\n\")\n",
    "            pass\n",
    "    load['endpoints'] = load['endpoints'].apply(\n",
    "        lambda x: list(ast.literal_eval(x)))\n",
    "    load['proposed_endpoints'] = load.seg_id.map(endpoints_dict)\n",
    "    return load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def endpoint_generator_humfrey(load, invalidation_d, num_humfrey_iters, decimation_factor):\n",
    "    for idx, i in enumerate(load['seg_id']):\n",
    "        try:\n",
    "            t1, skel, mesh_obj = tip_finder_decimation(root_id=str(\n",
    "                i), inval_d=invalidation_d, num_humfrey_iters = num_humfrey_iters, decimation_factor=decimation_factor)\n",
    "            endpoints_dict[i] = t1\n",
    "            print(\"proposed endpoints: \")\n",
    "            print(t1)\n",
    "        except:\n",
    "            print(f\"\\n\\nSeg {i} returned error on get. Skipping.\\n\")\n",
    "            pass\n",
    "    load['endpoints'] = load['endpoints'].apply(\n",
    "        lambda x: list(ast.literal_eval(x)))\n",
    "    load['proposed_endpoints'] = load.seg_id.map(endpoints_dict)\n",
    "    return load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def endpoint_generator_bilaplacian(load, invalidation_d, num_laplacian_iters, decimation_factor):\n",
    "    for idx, i in enumerate(load['seg_id']):\n",
    "        try:\n",
    "            t1, skel, mesh_obj = tip_finder_decimation(root_id=str(\n",
    "                i), inval_d=invalidation_d, num_laplacian_iters = num_laplacian_iters, decimation_factor=decimation_factor)\n",
    "            endpoints_dict[i] = t1\n",
    "            print(\"proposed endpoints: \")\n",
    "            print(t1)\n",
    "        except:\n",
    "            print(f\"\\n\\nSeg {i} returned error on get. Skipping.\\n\")\n",
    "            pass\n",
    "    load['endpoints'] = load['endpoints'].apply(\n",
    "        lambda x: list(ast.literal_eval(x)))\n",
    "    load['proposed_endpoints'] = load.seg_id.map(endpoints_dict)\n",
    "    return load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_metrics(load, threshold, run, output, invalidation_d, num_laplacian_iters, decimation_factor):\n",
    "    for index, row in load.iterrows():\n",
    "        # if idx < 123:\n",
    "        #     pass\n",
    "        # elif idx > 123:\n",
    "        #     pass\n",
    "\n",
    "        endpoint_array = np.array(row[\"endpoints\"])\n",
    "        proposed_endpoints_array = np.array(row[\"proposed_endpoints\"])\n",
    "        segID = row[\"seg_id\"]\n",
    "\n",
    "        #skip anything seg_id that isn't 'good'\n",
    "        if row[\"comments\"] != 'good':\n",
    "            print(\"skipping \\n\")\n",
    "            continue\n",
    "\n",
    "        #if both say no endpoints, then it's correct\n",
    "        if (len(proposed_endpoints_array.shape) == 0 and len(endpoint_array.shape) == 0):\n",
    "            output.loc[len(output.index)] = [run, 1.0,1.0,1.0, segID, len(\n",
    "                endpoint_array), endpoint_array, invalidation_d, num_laplacian_iters, decimation_factor, row[\"comments\"], proposed_endpoints_array]\n",
    "            continue\n",
    "\n",
    "        #if we propose no endpoints but there are endpoints, it's wrong\n",
    "        elif (len(proposed_endpoints_array.shape) == 0 and len(endpoint_array.shape) > 0):\n",
    "            output.loc[len(output.index)] = [run, 0.0,0.0,0.0, segID, len(\n",
    "                endpoint_array), endpoint_array, invalidation_d,  num_laplacian_iters, decimation_factor, row[\"comments\"], proposed_endpoints_array]\n",
    "            continue\n",
    "\n",
    "        #if we propose endpoints but there are none, it's wrong\n",
    "        elif proposed_endpoints_array.size > 0 and endpoint_array.size == 0:\n",
    "            output.loc[len(output.index)] = [run, 0.0,0.0,0.0, segID, len(\n",
    "                endpoint_array), endpoint_array, invalidation_d,  num_laplacian_iters, decimation_factor, row[\"comments\"], proposed_endpoints_array]\n",
    "            continue\n",
    "\n",
    "        #should skip for now if the endpoint array is size zero to avoid divide by zero error\n",
    "        elif endpoint_array.size == 0:\n",
    "            output.loc[len(output.index)] = [run,-1,-1,-1,segID, len(endpoint_array), endpoint_array, invalidation_d,  num_laplacian_iters, decimation_factor, row[\"comments\"], proposed_endpoints_array]\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            endpoint_ids = np.arange(0, len(endpoint_array))\n",
    "            test_ids = np.arange(\n",
    "                0, len(proposed_endpoints_array)) + len(proposed_endpoints_array)\n",
    "\n",
    "            #get precision and recall at end of each iteration\n",
    "            analysis = run_synapse_analysis(\n",
    "                endpoint_array,\n",
    "                np.array(endpoint_ids),\n",
    "                proposed_endpoints_array,\n",
    "                np.array(test_ids),\n",
    "                threshold,\n",
    "                iso_correction=10,\n",
    "            )\n",
    "\n",
    "            output.loc[len(output.index)] = [run, analysis.precision, analysis.recall, analysis.f1, segID, len(\n",
    "                endpoint_array), endpoint_array,invalidation_d, num_laplacian_iters, decimation_factor, row[\"comments\"], proposed_endpoints_array]\n",
    "            # new_row = [run, analysis.precision, analysis.recall, analysis.f1, segID, len(\n",
    "            #     endpoint_array), endpoint_array,invalidation_d, num_humfrey_iters, decimation_factor, row[\"comments\"], proposed_endpoints_array]\n",
    "            # output.append(pd.Series(new_row, index=output.columns[:len(new_row)]), ignore_index=True)\n",
    "            \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Various utility classes and functions for Confirms.\n",
    "\"\"\"\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "def calculate_precision_recall(tp, fp, fn):\n",
    "    \"\"\"\n",
    "    Calculate precision/recall from given true positives, false positives, and false negatives.\n",
    "    Parameters\n",
    "    ----------\n",
    "    tp : int\n",
    "        The number of true positives.\n",
    "    fp : int\n",
    "        The number of false positives.\n",
    "    fn : int\n",
    "        The number of false negatives.\n",
    "    Returns\n",
    "    -------\n",
    "    precision : float\n",
    "        The precision score.\n",
    "    recall : float\n",
    "        The recall score.\n",
    "    \"\"\"\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def calculate_f1(precision, recall):\n",
    "    \"\"\"\n",
    "    Calculate the F1 score from precision/recall scores.\n",
    "    Parameters\n",
    "    ----------\n",
    "    precision : float\n",
    "        The precision score.\n",
    "    recall : float\n",
    "        The recall score.\n",
    "    Returns\n",
    "    -------\n",
    "    f1 : float\n",
    "        The F1 score.\n",
    "    \"\"\"\n",
    "    return 2 * ((precision * recall) / (precision + recall))\n",
    "\n",
    "\n",
    "def get_summary_metrics(array):\n",
    "    \"\"\"\n",
    "    Calculate a number of summary metrics on an array of numbers.\n",
    "    Parameters\n",
    "    ----------\n",
    "    array : array_like\n",
    "        Array containing numbers who summary metrics is desired.\n",
    "    Returns\n",
    "    -------\n",
    "    metrics : dict\n",
    "        Dict containing the mean, median, max, min, range, standard deviation,\n",
    "        and variance of the input array.\n",
    "    \"\"\"\n",
    "    summary_object = {}\n",
    "    array = np.array(array)\n",
    "\n",
    "    summary_object[\"mean\"] = np.mean(array)\n",
    "    summary_object[\"median\"] = np.median(array)\n",
    "    summary_object[\"max\"] = np.amax(array)\n",
    "    summary_object[\"min\"] = np.amin(array)\n",
    "    summary_object[\"range\"] = np.mean(array)\n",
    "    summary_object[\"stddev\"] = np.std(array)\n",
    "    summary_object[\"variance\"] = np.var(array)\n",
    "\n",
    "    return summary_object\n",
    "\n",
    "\n",
    "def munkres_assignment(workers, jobs):\n",
    "    \"\"\"\n",
    "    Perform hungarian-munkres assignment.\n",
    "    Parameters\n",
    "    ----------\n",
    "    workers : array_like\n",
    "        Array containing the first set of points.\n",
    "    jobs : array_like\n",
    "        Array containing the second set of points.\n",
    "    Returns\n",
    "    -------\n",
    "    cost_matrix : numpy.ndarray\n",
    "        Matrix containing pairwise distances (the cost of assignment).\n",
    "    row_ind : numpy.ndarray\n",
    "        Row indices of cost_matrix for optimal assignment.\n",
    "    col_ind : numpy.ndarray\n",
    "        Column indices of cost_matrx for optimal assignment.\n",
    "    \"\"\"\n",
    "    cost_matrix = spatial.distance.cdist(workers, jobs, \"euclidean\")\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    return cost_matrix, row_ind, col_ind\n",
    "\n",
    "\n",
    "def make_isotropic(xyz, correction, dimen=2):\n",
    "    \"\"\"\n",
    "    Correct anisotropy in a collection of x, y, z coordinates.\n",
    "    This function performs a deepcopy of xyz before making the necessary modifications.\n",
    "    Parameters\n",
    "    ----------\n",
    "    xyz : numpy.ndarray or pandas.DataFrame:\n",
    "        The coordinate values.\n",
    "    correction : float\n",
    "        The value to correct anisotrophy.\n",
    "    dimen : int\n",
    "        Index of last dimension to which to apply the correction. Default is 2.\n",
    "    Returns\n",
    "    -------\n",
    "    iso_xyz : numpy.ndarray or pandas.DataFrame\n",
    "        An isotropic version of xyz.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(xyz, np.ndarray) and not isinstance(xyz, pd.DataFrame):\n",
    "        xyz = np.asarray(xyz)\n",
    "\n",
    "    shape = np.shape(xyz)\n",
    "    isotropic_xyz = xyz.copy()\n",
    "    size = shape[-1]\n",
    "    if dimen >= size or dimen < 0:\n",
    "        raise ValueError(\n",
    "            \"improper dimen value (valid: 0 through {})\".format(size - 1))\n",
    "    if isinstance(xyz, np.ndarray):\n",
    "        isotropic_xyz[..., dimen] = isotropic_xyz[..., dimen] * correction\n",
    "    else:\n",
    "        isotropic_xyz.iloc[:, dimen] = isotropic_xyz.iloc[:,\n",
    "                                                          dimen] * correction\n",
    "    return isotropic_xyz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Confirms synapse processing and analysis functions.\n",
    "\"\"\"\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# from . import utils\n",
    "# import utils\n",
    "\n",
    "SynapseMetrics = namedtuple(\n",
    "    \"SynapseMetrics\",\n",
    "    [\"precision\", \"recall\", \"f1\", \"tp_gt_ids\", \"tp_test_ids\", \"fp_ids\", \"fn_ids\"],\n",
    ")\n",
    "\n",
    "\n",
    "def filter_synapse_id_core(volume, xyz, ids, box_radius_nm=2500):\n",
    "    \"\"\"\n",
    "    Filter synapses to only return those within a central core.\n",
    "    Parameters\n",
    "    ----------\n",
    "    volume : dict\n",
    "        The volume to filter on.\n",
    "    xyz : array_like\n",
    "        Synapse xyz coordinates.\n",
    "    ids : array_like\n",
    "        Synapse hash (ids).\n",
    "    box_radius_nm : int\n",
    "        radius of cube, from volume core.\n",
    "    Returns\n",
    "    -------\n",
    "        xyz : numpy.ndarray\n",
    "            Synapse coordinates.\n",
    "        ids : numpy.ndarray\n",
    "            Synapse ids.\n",
    "    \"\"\"\n",
    "\n",
    "    xyz_out = []\n",
    "    id_out = []\n",
    "    center = np.asarray(volume[\"center\"], \"float\")\n",
    "    base_resolution = np.asarray(volume[\"base_resolution\"], \"float\")\n",
    "    annotation_resolution = np.asarray(volume[\"resolution\"], \"float\")\n",
    "\n",
    "    pad_vx = box_radius_nm / base_resolution[0] / (2 ** annotation_resolution)\n",
    "    pad_vy = box_radius_nm / base_resolution[1] / (2 ** annotation_resolution)\n",
    "    pad_vz = box_radius_nm / base_resolution[2]\n",
    "    xr = (center[0] - pad_vx, center[0] + pad_vx)\n",
    "    yr = (center[1] - pad_vy, center[1] + pad_vy)\n",
    "    zr = (center[2] - pad_vz, center[2] + pad_vz)\n",
    "\n",
    "    for i in range(len(xyz)):\n",
    "        x = xyz[i][0]\n",
    "        y = xyz[i][1]\n",
    "        z = xyz[i][2]\n",
    "\n",
    "        if (\n",
    "            x > xr[0]\n",
    "            and x < xr[1]\n",
    "            and y > yr[0]\n",
    "            and y < yr[1]\n",
    "            and z > zr[0]\n",
    "            and z < zr[1]\n",
    "        ):\n",
    "            xyz_out.append(xyz[i])\n",
    "            id_out.append(ids[i])\n",
    "\n",
    "    return np.array(xyz_out), np.array(id_out, dtype=np.object)\n",
    "\n",
    "\n",
    "def synapse_match(\n",
    "    xyz_truth, xyz_detect, id_truth, id_detect, thresh\n",
    "):  # pylint: disable=R0914\n",
    "    \"\"\"\n",
    "    <Description here>\n",
    "    Parameters\n",
    "    ----------\n",
    "    xyz_truth : array_like\n",
    "        <description>\n",
    "    xyz_detect : array_like\n",
    "        <description>\n",
    "    id_truth : array_like\n",
    "        <description>\n",
    "    id_detech : array_like\n",
    "        <description>\n",
    "    thresh : float\n",
    "        <description>\n",
    "    Returns\n",
    "    -------\n",
    "    id_lookup : <type>\n",
    "        <description>\n",
    "    \"\"\"\n",
    "\n",
    "    # pylint: disable=C0103\n",
    "\n",
    "    # Ensure we have numpy arrays\n",
    "    xyz_truth = np.asarray(xyz_truth)\n",
    "    xyz_detect = np.asarray(xyz_detect)\n",
    "    id_truth = np.asarray(id_truth)\n",
    "    id_detect = np.asarray(id_detect)\n",
    "\n",
    "    cost, row_ind, col_ind = munkres_assignment(xyz_truth, xyz_detect)\n",
    "    print(cost)\n",
    "    match_idx = np.where(cost[row_ind, col_ind] < thresh)\n",
    "\n",
    "    if len(match_idx) > 0:\n",
    "        # row is idx of GT TP\n",
    "        # col is idx of student TP\n",
    "        gt_tp_idx, det_tp_idx = row_ind[match_idx], col_ind[match_idx]\n",
    "        gt_tp_ids, det_tp_ids = id_truth[gt_tp_idx], id_detect[det_tp_idx]\n",
    "        # Combine into pairs\n",
    "        id_lookup = np.column_stack((gt_tp_ids, det_tp_ids))\n",
    "    else:\n",
    "        gt_tp_idx = []\n",
    "        det_tp_idx = []\n",
    "        id_lookup = np.column_stack(\n",
    "            (np.array([], dtype=\"object\"), np.array([], dtype=\"object\"))\n",
    "        )\n",
    "\n",
    "    # not in row (set diff) are FN\n",
    "    gt_syn_idx = np.arange(0, len(xyz_truth))\n",
    "    fn_idx = np.setdiff1d(gt_syn_idx, gt_tp_idx)\n",
    "    if len(fn_idx) > 0:\n",
    "        fn_ids = id_truth[fn_idx]\n",
    "        id_lookup_fn = np.column_stack((fn_ids, np.repeat(None, len(fn_ids))))\n",
    "        id_lookup = np.concatenate((id_lookup, id_lookup_fn))\n",
    "\n",
    "    # not in col (set diff) are FP\n",
    "    det_syn_idx = set(np.arange(0, len(xyz_detect)))\n",
    "    fp_idx = np.asarray(list(det_syn_idx.difference(det_tp_idx)))\n",
    "    if len(fp_idx) > 0:\n",
    "        fp_ids = id_detect[fp_idx]\n",
    "        id_lookup_fp = np.column_stack((np.repeat(None, len(fp_ids)), fp_ids))\n",
    "        id_lookup = np.concatenate((id_lookup, id_lookup_fp))\n",
    "\n",
    "    return pd.DataFrame(id_lookup, columns=[\"ground_truth\", \"detect\"])\n",
    "\n",
    "\n",
    "def run_synapse_analysis(\n",
    "    gt_xyzs,\n",
    "    gt_ids,\n",
    "    test_xyzs,\n",
    "    test_ids,\n",
    "    threshold,\n",
    "    iso_corrected=False,\n",
    "    iso_correction=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    <Description here>\n",
    "    Parameters\n",
    "    ----------\n",
    "    gt_xyzs : numpy.ndarray\n",
    "        Array of ground truth xyz coordinates\n",
    "    gt_ids : numpy.ndarray\n",
    "        Array of ids associated with ground truth xyz coordinates\n",
    "    test_xyzs : numpy.ndarray\n",
    "        Array of test xyz coordinates\n",
    "    test_ids : numpy.ndarray\n",
    "        Array of ids associated with test xyz coordinates\n",
    "    threshold : float\n",
    "        Synapse matching threshold\n",
    "    iso_corrected : boolean\n",
    "        Mark whether the data is isotropic. If not, it will be made isotropic using the\n",
    "        `iso_correction` parameter.\n",
    "    iso_correction : float\n",
    "        Value to correct anistropy.\n",
    "    Returns\n",
    "    -------\n",
    "    sm : SynapseMetrics\n",
    "        Resultant object containing precision, recall, and F1 scores, along with\n",
    "        with true positive (both ground truth and test), false positive, and false negative\n",
    "        ids.\n",
    "    \"\"\"\n",
    "    # pylint: disable=R0913,R0914\n",
    "    if not iso_corrected:\n",
    "        # gt_xyzs = utils.make_isotropic(gt_xyzs, iso_correction)\n",
    "        gt_xyzs = make_isotropic(gt_xyzs, iso_correction)\n",
    "        # test_xyzs = utils.make_isotropic(test_xyzs, iso_correction)\n",
    "        test_xyzs = make_isotropic(test_xyzs, iso_correction)\n",
    "\n",
    "    results_table = synapse_match(\n",
    "        gt_xyzs, test_xyzs, gt_ids, test_ids, threshold)\n",
    "\n",
    "    tp = results_table.dropna()\n",
    "    fn = results_table[results_table.detect.isnull()]\n",
    "    fp = results_table[results_table.ground_truth.isnull()]\n",
    "\n",
    "    assert len(tp.ground_truth) == len(\n",
    "        tp.detect\n",
    "    ), \"true positive ground truth and test size mismatch\"\n",
    "\n",
    "    tp_count = len(tp)\n",
    "    fp_count = len(fp)\n",
    "    fn_count = len(fn)\n",
    "\n",
    "    try:\n",
    "        # precision, recall = utils.calculate_precision_recall(\n",
    "        #     tp_count, fp_count, fn_count\n",
    "        # )\n",
    "\n",
    "        precision, recall = calculate_precision_recall(\n",
    "            tp_count, fp_count, fn_count\n",
    "        )\n",
    "\n",
    "    except ZeroDivisionError:\n",
    "        precision, recall = np.nan, np.nan\n",
    "\n",
    "    try:\n",
    "        # f1 = utils.calculate_f1(precision, recall)\n",
    "        f1 = calculate_f1(precision, recall)\n",
    "    except ZeroDivisionError:\n",
    "        f1 = np.nan\n",
    "\n",
    "    sm = SynapseMetrics(\n",
    "        precision=precision,\n",
    "        recall=recall,\n",
    "        f1=f1,\n",
    "        tp_gt_ids=np.asarray(tp.ground_truth),\n",
    "        tp_test_ids=np.asarray(tp.detect),\n",
    "        fp_ids=np.asarray(fp.detect),\n",
    "        fn_ids=np.asarray(fn.ground_truth),\n",
    "    )\n",
    "    return sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neuron</th>\n",
       "      <th>ng_link</th>\n",
       "      <th>seg_id</th>\n",
       "      <th>pink_pts</th>\n",
       "      <th>num_endpoints</th>\n",
       "      <th>endpoints</th>\n",
       "      <th>comments</th>\n",
       "      <th>detailed_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.646911e+17</td>\n",
       "      <td>https://neuroglancer.neuvue.io/?json_url=https...</td>\n",
       "      <td>864691135909994000</td>\n",
       "      <td>(402188, 228684, 24029)</td>\n",
       "      <td>2</td>\n",
       "      <td>((402584, 228856, 23991), (402985, 229235, 235...</td>\n",
       "      <td>good</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>864691135247440303</td>\n",
       "      <td>(401258, 224832, 24029)</td>\n",
       "      <td>3</td>\n",
       "      <td>((401612, 224623, 23991), (405257, 226318, 236...</td>\n",
       "      <td>good</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>864691134794123793</td>\n",
       "      <td>(401314, 228366, 24424)</td>\n",
       "      <td>2</td>\n",
       "      <td>((401242, 228382, 24444), (400982, 228457, 245...</td>\n",
       "      <td>m</td>\n",
       "      <td>merged to two axon pieces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>864691135772363453</td>\n",
       "      <td>(400199, 220721, 24029)</td>\n",
       "      <td>7</td>\n",
       "      <td>((401289, 218721, 23991), (399895, 216533, 235...</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>864691135314714227</td>\n",
       "      <td>(397870, 232292, 24004)</td>\n",
       "      <td>2</td>\n",
       "      <td>((397867, 232230, 23999), (397854, 232252, 239...</td>\n",
       "      <td>good</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>864691135319600870</td>\n",
       "      <td>(106639, 111664, 20802)</td>\n",
       "      <td>2</td>\n",
       "      <td>((106608, 111803, 20796), (98290, 97555, 21045))</td>\n",
       "      <td>good</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>864691136558839249</td>\n",
       "      <td>(103176, 188510, 21202)</td>\n",
       "      <td>2</td>\n",
       "      <td>((103138, 188530, 21200), (91780, 189141, 20879))</td>\n",
       "      <td>good</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>864691135012763638</td>\n",
       "      <td>(118152, 182120, 20651)</td>\n",
       "      <td>-1</td>\n",
       "      <td>()</td>\n",
       "      <td>e</td>\n",
       "      <td>too many merge errors that will lead to useles...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>864691135458639120</td>\n",
       "      <td>(127406, 206585, 21529)</td>\n",
       "      <td>2</td>\n",
       "      <td>((127475, 206611, 21525), (128120, 207267, 215...</td>\n",
       "      <td>good</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>864691135826764827</td>\n",
       "      <td>(122260, 186180, 19628)</td>\n",
       "      <td>2</td>\n",
       "      <td>((122293, 186204, 19621), (124943, 185661, 186...</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           neuron                                            ng_link  \\\n",
       "0    8.646911e+17  https://neuroglancer.neuvue.io/?json_url=https...   \n",
       "1             NaN                                                NaN   \n",
       "2             NaN                                                NaN   \n",
       "3             NaN                                                NaN   \n",
       "4             NaN                                                NaN   \n",
       "..            ...                                                ...   \n",
       "127           NaN                                                NaN   \n",
       "128           NaN                                                NaN   \n",
       "129           NaN                                                NaN   \n",
       "130           NaN                                                NaN   \n",
       "131           NaN                                                NaN   \n",
       "\n",
       "                 seg_id                 pink_pts  num_endpoints  \\\n",
       "0    864691135909994000  (402188, 228684, 24029)              2   \n",
       "1    864691135247440303  (401258, 224832, 24029)              3   \n",
       "2    864691134794123793  (401314, 228366, 24424)              2   \n",
       "3    864691135772363453  (400199, 220721, 24029)              7   \n",
       "4    864691135314714227  (397870, 232292, 24004)              2   \n",
       "..                  ...                      ...            ...   \n",
       "127  864691135319600870  (106639, 111664, 20802)              2   \n",
       "128  864691136558839249  (103176, 188510, 21202)              2   \n",
       "129  864691135012763638  (118152, 182120, 20651)             -1   \n",
       "130  864691135458639120  (127406, 206585, 21529)              2   \n",
       "131  864691135826764827  (122260, 186180, 19628)              2   \n",
       "\n",
       "                                             endpoints comments  \\\n",
       "0    ((402584, 228856, 23991), (402985, 229235, 235...     good   \n",
       "1    ((401612, 224623, 23991), (405257, 226318, 236...     good   \n",
       "2    ((401242, 228382, 24444), (400982, 228457, 245...        m   \n",
       "3    ((401289, 218721, 23991), (399895, 216533, 235...        m   \n",
       "4    ((397867, 232230, 23999), (397854, 232252, 239...     good   \n",
       "..                                                 ...      ...   \n",
       "127   ((106608, 111803, 20796), (98290, 97555, 21045))     good   \n",
       "128  ((103138, 188530, 21200), (91780, 189141, 20879))     good   \n",
       "129                                                 ()        e   \n",
       "130  ((127475, 206611, 21525), (128120, 207267, 215...     good   \n",
       "131  ((122293, 186204, 19621), (124943, 185661, 186...        m   \n",
       "\n",
       "                                     detailed_comments  \n",
       "0                                                  NaN  \n",
       "1                                                  NaN  \n",
       "2                            merged to two axon pieces  \n",
       "3                                                  NaN  \n",
       "4                                                  NaN  \n",
       "..                                                 ...  \n",
       "127                                                NaN  \n",
       "128                                                NaN  \n",
       "129  too many merge errors that will lead to useles...  \n",
       "130                                                NaN  \n",
       "131                                                NaN  \n",
       "\n",
       "[132 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading in data\n",
    "load = pd.read_csv(\"agents/Data/endpoints_gt5.csv\")\n",
    "endpoints_dict = {}\n",
    "\n",
    "load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>seg_id</th>\n",
       "      <th>num_endpoints</th>\n",
       "      <th>endpoints</th>\n",
       "      <th>invalidation_d</th>\n",
       "      <th>num_laplacian_iters</th>\n",
       "      <th>decimation_factor</th>\n",
       "      <th>comments</th>\n",
       "      <th>proposed_endpoints</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [run, precision, recall, f1, seg_id, num_endpoints, endpoints, invalidation_d, num_laplacian_iters, decimation_factor, comments, proposed_endpoints]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate out_df\n",
    "\n",
    "out_df = pd.DataFrame(columns=[\"run\", \"precision\", \"recall\", \"f1\", \"seg_id\",\n",
    "                      \"num_endpoints\", \"endpoints\",\"invalidation_d\", \"num_laplacian_iters\",\"decimation_factor\", \"comments\", \"proposed_endpoints\"])\n",
    "out_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run = 0\n",
    "# for invalidation_d in range(4000, 6001, 200):\n",
    "# # for invalidation_d in range(4000, 4201, 200):\n",
    "#     # for num_humfrey_iters in range(0,251,25):\n",
    "#     # for num_humfrey_iters in range(0, 251, 25):\n",
    "#     for cube_side_len in range(500,901,50):\n",
    "#         # for decimation_factor in np.arange(0.3, 0.71, 0.04):\n",
    "#         for decimation_factor in np.linspace(0.3, 0.71, num = 10):\n",
    "#             load = pd.read_csv(\"agents/Data/endpoints_gt5.csv\") #NOT THE BEST WAY TO DO THIS!!!\n",
    "#             run += 1\n",
    "#             load_with_proposed_endpoints = endpoint_generator(\n",
    "#                 load, invalidation_d=invalidation_d, cube_side = cube_side_len, decimation_factor=decimation_factor)\n",
    "#             out_df = testing_metrics(\n",
    "#                 load_with_proposed_endpoints, threshold=500, run=run, output=out_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running all seg ids with one param combo\n",
    "run = 1\n",
    "load_with_proposed_endpoints = endpoint_generator_bilaplacian(\n",
    "    load, invalidation_d=5600, num_laplacian_iters=60, decimation_factor=0.5)\n",
    "out_df = testing_metrics(\n",
    "    load_with_proposed_endpoints, threshold=500, run=run, output=out_df, invalidation_d=5600, num_laplacian_iters=60, decimation_factor=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/12/2022 12:04:28 PM WARNING: face_normals incorrect shape, ignoring! \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: deduplication not currently supported for this layer's variable layered draco meshes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4419 [00:00<?, ?it/s]/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/meshparty/skeletonize.py:613: RuntimeWarning: invalid value encountered in multiply\n",
      "  target = np.nanargmax(root_ds * valid)\n",
      "100%|██████████| 4419/4419 [00:00<00:00, 346590.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'trimesh.base.Trimesh'>\n",
      "\n",
      "<trimesh.Trimesh(vertices.shape=(4568, 3), faces.shape=(8835, 3))>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14505/14505 [00:00<00:00, 542971.96it/s]\n",
      "100%|██████████| 115/115 [00:00<00:00, 28164.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proposed endpoints: \n",
      "[(76765, 99760, 21201), (77285, 107835, 21214), (77290, 106433, 21256), (77379, 106380, 21248)]\n",
      "[[8116.20040413  860.80427508 1866.01205784 1843.1616858 ]\n",
      " [ 335.93154064 8274.91540742 6886.33603595 6833.50320114]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/12/2022 12:04:30 PM WARNING: face_normals incorrect shape, ignoring! \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: deduplication not currently supported for this layer's variable layered draco meshes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5199 [00:00<?, ?it/s]/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/meshparty/skeletonize.py:613: RuntimeWarning: invalid value encountered in multiply\n",
      "  target = np.nanargmax(root_ds * valid)\n",
      "100%|██████████| 5199/5199 [00:00<00:00, 532689.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'trimesh.base.Trimesh'>\n",
      "\n",
      "<trimesh.Trimesh(vertices.shape=(5369, 3), faces.shape=(10394, 3))>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14505/14505 [00:00<00:00, 538840.98it/s]\n",
      "100%|██████████| 115/115 [00:00<00:00, 22308.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proposed endpoints: \n",
      "[(76765, 99760, 21201), (77285, 107835, 21214), (77290, 106433, 21256), (77379, 106380, 21248)]\n",
      "[[8116.20040413  860.80427508 1866.01205784 1843.1616858 ]\n",
      " [ 335.93154064 8274.91540742 6886.33603595 6833.50320114]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/12/2022 12:04:33 PM WARNING: face_normals incorrect shape, ignoring! \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: deduplication not currently supported for this layer's variable layered draco meshes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5973 [00:00<?, ?it/s]/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/meshparty/skeletonize.py:613: RuntimeWarning: invalid value encountered in multiply\n",
      "  target = np.nanargmax(root_ds * valid)\n",
      "100%|██████████| 5973/5973 [00:00<00:00, 524222.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'trimesh.base.Trimesh'>\n",
      "\n",
      "<trimesh.Trimesh(vertices.shape=(6168, 3), faces.shape=(11944, 3))>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14505/14505 [00:00<00:00, 545034.44it/s]\n",
      "100%|██████████| 115/115 [00:00<00:00, 29977.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proposed endpoints: \n",
      "[(76765, 99760, 21201), (77285, 107835, 21214), (77290, 106433, 21256), (77379, 106380, 21248)]\n",
      "[[8116.20040413  860.80427508 1866.01205784 1843.1616858 ]\n",
      " [ 335.93154064 8274.91540742 6886.33603595 6833.50320114]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/12/2022 12:04:35 PM WARNING: face_normals incorrect shape, ignoring! \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: deduplication not currently supported for this layer's variable layered draco meshes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6749 [00:00<?, ?it/s]/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/meshparty/skeletonize.py:613: RuntimeWarning: invalid value encountered in multiply\n",
      "  target = np.nanargmax(root_ds * valid)\n",
      "100%|██████████| 6749/6749 [00:00<00:00, 524394.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'trimesh.base.Trimesh'>\n",
      "\n",
      "<trimesh.Trimesh(vertices.shape=(6968, 3), faces.shape=(13496, 3))>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14505/14505 [00:00<00:00, 546449.24it/s]\n",
      "100%|██████████| 115/115 [00:00<00:00, 29488.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proposed endpoints: \n",
      "[(76765, 99760, 21201), (77285, 107835, 21214), (77290, 106433, 21256), (77379, 106380, 21248)]\n",
      "[[8116.20040413  860.80427508 1866.01205784 1843.1616858 ]\n",
      " [ 335.93154064 8274.91540742 6886.33603595 6833.50320114]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/12/2022 12:04:37 PM WARNING: face_normals incorrect shape, ignoring! \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: deduplication not currently supported for this layer's variable layered draco meshes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7526 [00:00<?, ?it/s]/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/meshparty/skeletonize.py:613: RuntimeWarning: invalid value encountered in multiply\n",
      "  target = np.nanargmax(root_ds * valid)\n",
      "100%|██████████| 7526/7526 [00:00<00:00, 543544.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'trimesh.base.Trimesh'>\n",
      "\n",
      "<trimesh.Trimesh(vertices.shape=(7768, 3), faces.shape=(15050, 3))>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14505/14505 [00:00<00:00, 547753.01it/s]\n",
      "100%|██████████| 115/115 [00:00<00:00, 22980.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proposed endpoints: \n",
      "[(76765, 99760, 21201), (77285, 107835, 21214), (77290, 106433, 21256), (77379, 106380, 21248)]\n",
      "[[8116.20040413  860.80427508 1866.01205784 1843.1616858 ]\n",
      " [ 335.93154064 8274.91540742 6886.33603595 6833.50320114]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/12/2022 12:04:39 PM WARNING: face_normals incorrect shape, ignoring! \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: deduplication not currently supported for this layer's variable layered draco meshes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8455 [00:00<?, ?it/s]/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/meshparty/skeletonize.py:613: RuntimeWarning: invalid value encountered in multiply\n",
      "  target = np.nanargmax(root_ds * valid)\n",
      "100%|██████████| 8455/8455 [00:00<00:00, 498556.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'trimesh.base.Trimesh'>\n",
      "\n",
      "<trimesh.Trimesh(vertices.shape=(8568, 3), faces.shape=(16908, 3))>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14505/14505 [00:00<00:00, 506581.23it/s]\n",
      "100%|██████████| 115/115 [00:00<00:00, 24759.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proposed endpoints: \n",
      "[(76765, 99760, 21201), (77285, 107835, 21214), (77290, 106433, 21256), (77379, 106380, 21248)]\n",
      "[[8116.20040413  860.80427508 1866.01205784 1843.1616858 ]\n",
      " [ 335.93154064 8274.91540742 6886.33603595 6833.50320114]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/12/2022 12:04:41 PM WARNING: face_normals incorrect shape, ignoring! \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: deduplication not currently supported for this layer's variable layered draco meshes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9246 [00:00<?, ?it/s]/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/meshparty/skeletonize.py:613: RuntimeWarning: invalid value encountered in multiply\n",
      "  target = np.nanargmax(root_ds * valid)\n",
      "100%|██████████| 9246/9246 [00:00<00:00, 516543.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'trimesh.base.Trimesh'>\n",
      "\n",
      "<trimesh.Trimesh(vertices.shape=(9369, 3), faces.shape=(18490, 3))>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14505/14505 [00:00<00:00, 486812.19it/s]\n",
      "100%|██████████| 115/115 [00:00<00:00, 24785.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proposed endpoints: \n",
      "[(76765, 99760, 21201), (77285, 107835, 21214), (77290, 106433, 21256), (77379, 106380, 21248)]\n",
      "[[8116.20040413  860.80427508 1866.01205784 1843.1616858 ]\n",
      " [ 335.93154064 8274.91540742 6886.33603595 6833.50320114]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/12/2022 12:04:42 PM WARNING: face_normals incorrect shape, ignoring! \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: deduplication not currently supported for this layer's variable layered draco meshes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10040 [00:00<?, ?it/s]/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/meshparty/skeletonize.py:613: RuntimeWarning: invalid value encountered in multiply\n",
      "  target = np.nanargmax(root_ds * valid)\n",
      "100%|██████████| 10040/10040 [00:00<00:00, 564140.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'trimesh.base.Trimesh'>\n",
      "\n",
      "<trimesh.Trimesh(vertices.shape=(10168, 3), faces.shape=(20078, 3))>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14505/14505 [00:00<00:00, 543034.97it/s]\n",
      "100%|██████████| 115/115 [00:00<00:00, 20473.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proposed endpoints: \n",
      "[(76765, 99760, 21201), (77285, 107835, 21214), (77290, 106433, 21256), (77379, 106380, 21248)]\n",
      "[[8116.20040413  860.80427508 1866.01205784 1843.1616858 ]\n",
      " [ 335.93154064 8274.91540742 6886.33603595 6833.50320114]]\n"
     ]
    }
   ],
   "source": [
    "# TESTING ALL PARAM COMBOS ON A SINGLE SEG ID\n",
    "\n",
    "# Seg_id at index 97 - 864691135319600870\n",
    "\n",
    "load = pd.read_csv(\"agents/Data/endpoints_gt5.csv\")\n",
    "load = load[load[\"seg_id\"] ==  864691135920135864]\n",
    "endpoints_dict = {}\n",
    "\n",
    "# Generate out_df\n",
    "out_df = pd.DataFrame(columns=[\"run\", \"precision\", \"recall\", \"f1\", \"seg_id\",\n",
    "                      \"num_endpoints\", \"endpoints\", \"invalidation_d\",\"num_laplacian_iters\",\"decimation_factor\",\"comments\", \"proposed_endpoints\"])\n",
    "\n",
    "run = 0\n",
    "for invalidation_d in range(5200, 5601, 200):\n",
    "# for invalidation_d in range(4000, 4201, 200):\n",
    "    for num_laplacian_iters in range(0, 250, 25):\n",
    "        # for decimation_factor in np.linspace(0.31, 0.8, num = 10):\n",
    "        for decimation_factor in np.linspace(0.31, 0.8, num=10):\n",
    "            load = pd.read_csv(\"agents/Data/endpoints_gt5.csv\")\n",
    "            load = load[load[\"seg_id\"] ==  864691135920135864]\n",
    "            load_with_proposed_endpoints = endpoint_generator_bilaplacian(\n",
    "                load, invalidation_d=invalidation_d, num_laplacian_iters = num_laplacian_iters, decimation_factor=decimation_factor)\n",
    "            run += 1\n",
    "            out_df = testing_metrics(\n",
    "                load_with_proposed_endpoints, threshold=500, run=run, output=out_df, invalidation_d = invalidation_d, num_laplacian_iters = num_laplacian_iters, decimation_factor = decimation_factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.to_csv('bilaplacian_combo1_all_segs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e70cd6dbfbd84db0df722de387fd9a98787af89bf0e7b259e8f6e74e5c75f4bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
