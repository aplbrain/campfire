{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from caveclient import CAVEclient\n",
    "from intern import array\n",
    "import pickle\n",
    "import numpy as np\n",
    "from agents import data_loader\n",
    "from cloudvolume import CloudVolume\n",
    "from membrane_detection import membranes\n",
    "from agents.scripts import precompute_membrane_vectors, create_post_matrix, merge_paths, get_soma\n",
    "import agents.sensor\n",
    "from agents.run import run_agents\n",
    "import aws.sqs as sqs\n",
    "import sys\n",
    "import time\n",
    "import ast\n",
    "import pandas as pd\n",
    "import agents.scripts as scripts\n",
    "from drive import drive\n",
    "from finding_orphans import *\n",
    "from math import sqrt\n",
    "from tip_finding import tip_finder_decimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = pd.read_csv(\"agents/Data/endpoints_gt5.csv\")\n",
    "endpoints_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neuron</th>\n",
       "      <th>ng_link</th>\n",
       "      <th>seg_id</th>\n",
       "      <th>pink_pts</th>\n",
       "      <th>num_endpoints</th>\n",
       "      <th>endpoints</th>\n",
       "      <th>comments</th>\n",
       "      <th>detailed_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.646911e+17</td>\n",
       "      <td>https://neuroglancer.neuvue.io/?json_url=https...</td>\n",
       "      <td>864691135909994000</td>\n",
       "      <td>(402188, 228684, 24029)</td>\n",
       "      <td>2</td>\n",
       "      <td>((402584, 228856, 23991), (402985, 229235, 235...</td>\n",
       "      <td>good</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>864691135247440303</td>\n",
       "      <td>(401258, 224832, 24029)</td>\n",
       "      <td>3</td>\n",
       "      <td>((401612, 224623, 23991), (405257, 226318, 236...</td>\n",
       "      <td>good</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>864691134794123793</td>\n",
       "      <td>(401314, 228366, 24424)</td>\n",
       "      <td>2</td>\n",
       "      <td>((401242, 228382, 24444), (400982, 228457, 245...</td>\n",
       "      <td>m</td>\n",
       "      <td>merged to two axon pieces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>864691135772363453</td>\n",
       "      <td>(400199, 220721, 24029)</td>\n",
       "      <td>7</td>\n",
       "      <td>((401289, 218721, 23991), (399895, 216533, 235...</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>864691135314714227</td>\n",
       "      <td>(397870, 232292, 24004)</td>\n",
       "      <td>2</td>\n",
       "      <td>((397867, 232230, 23999), (397854, 232252, 239...</td>\n",
       "      <td>good</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         neuron                                            ng_link  \\\n",
       "0  8.646911e+17  https://neuroglancer.neuvue.io/?json_url=https...   \n",
       "1           NaN                                                NaN   \n",
       "2           NaN                                                NaN   \n",
       "3           NaN                                                NaN   \n",
       "4           NaN                                                NaN   \n",
       "\n",
       "               seg_id                 pink_pts  num_endpoints  \\\n",
       "0  864691135909994000  (402188, 228684, 24029)              2   \n",
       "1  864691135247440303  (401258, 224832, 24029)              3   \n",
       "2  864691134794123793  (401314, 228366, 24424)              2   \n",
       "3  864691135772363453  (400199, 220721, 24029)              7   \n",
       "4  864691135314714227  (397870, 232292, 24004)              2   \n",
       "\n",
       "                                           endpoints comments  \\\n",
       "0  ((402584, 228856, 23991), (402985, 229235, 235...     good   \n",
       "1  ((401612, 224623, 23991), (405257, 226318, 236...     good   \n",
       "2  ((401242, 228382, 24444), (400982, 228457, 245...        m   \n",
       "3  ((401289, 218721, 23991), (399895, 216533, 235...        m   \n",
       "4  ((397867, 232230, 23999), (397854, 232252, 239...     good   \n",
       "\n",
       "           detailed_comments  \n",
       "0                        NaN  \n",
       "1                        NaN  \n",
       "2  merged to two axon pieces  \n",
       "3                        NaN  \n",
       "4                        NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating series objects for the desired columns \n",
    "segIDs = load['seg_id']\n",
    "Endpoints = load['endpoints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = endpoint_generator(load, invalidation_d= 5000, humfrey_iters= 50, decimation_factor= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/08/2022 02:49:42 PM WARNING: face_normals incorrect shape, ignoring! \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: deduplication not currently supported for this layer's variable layered draco meshes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1670/1670 [00:00<00:00, 261059.51it/s]\n"
     ]
    }
   ],
   "source": [
    "t1, skel, mesh_obj = tip_finder_decimation(\n",
    "    864691135909994000, inval_d=5600, cube_side_len=300, decimation_factor=0.5)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correctly  have 2 columns for the proposed vs gt endpoints\n",
    "load.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(402937, 229087, 23546), (402637, 228637, 23996)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions we wrote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_euclidian_distance(proposed_endpoint, gt_endponts_array):\n",
    "    diffs = gt_endponts_array - proposed_endpoint\n",
    "    diffs_distance = np.sqrt(np.sum(np.square(diffs), axis = 1))\n",
    "    min_dist_ind = np.argmin(diffs_distance)\n",
    "    min_dist = diffs_distance[min_dist_ind]\n",
    "    return min_dist\n",
    "\n",
    "    # smallest_distance = 2**30\n",
    "    # for endpoint in gt_endponts_array:\n",
    "    #     difference = proposed_endpoint - endpoint\n",
    "    #     distance = np.sqrt(np.sum(np.square(difference)))\n",
    "    #     if(distance < smallest_distance):\n",
    "    #         smallest_distance = distance \n",
    "    # return smallest_distance \n",
    "            \n",
    "    #could add endpoint to dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def endpoint_generator(load, invalidation_d, humfrey_iters, decimation_factor):\n",
    "    for idx, i in enumerate(load['seg_id']):\n",
    "        if idx == 20:\n",
    "            break\n",
    "        try:\n",
    "            t1, skel, mesh_obj = tip_finder_decimation(str(i))\n",
    "            endpoints_dict[i] = t1\n",
    "        except:\n",
    "            print(f\"\\n\\nSeg {i} returned error on get. Skipping.\\n\")\n",
    "            pass\n",
    "    load['endpoints'] = load['endpoints'].apply(\n",
    "        lambda x: list(ast.literal_eval(x)))\n",
    "    load['proposed_endpoints'] = load.seg_id.map(endpoints_dict)\n",
    "    return load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall(num_correct, array_endpoints): \n",
    "    recall = num_correct / len(array_endpoints)\n",
    "    return recall * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision(num_correct, array_proposed): \n",
    "    precision = num_correct / len(array_proposed)\n",
    "    return precision * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_metrics(load, threshold, run, output):\n",
    "    for index, row in load.iterrows():\n",
    "        if index == 20:\n",
    "            break\n",
    "        \n",
    "        endpoint_array = np.array(row[\"endpoints\"])\n",
    "        proposed_endpoints_array = np.array(row[\"proposed_endpoints\"])\n",
    "        segID = row[\"seg_id\"]\n",
    "\n",
    "        #skip anything seg_id that isn't 'good'\n",
    "        if row[\"comments\"] != 'good':\n",
    "            print(\"skipping \\n\")\n",
    "            continue\n",
    "        \n",
    "        #if both say no endpoints, then it's correct\n",
    "        elif (len(proposed_endpoints_array.shape) == 0 and len(endpoint_array.shape) == 0):\n",
    "            out_df.at[index, 'precision'] =1.0\n",
    "            out_df.at[index, 'recall'] = 1.0\n",
    "            out_df.at[index, 'f1'] = 1.0\n",
    "\n",
    "            continue\n",
    "\n",
    "        #if we propose no endpoints but there are endpoints, it's wrong\n",
    "        elif (len(proposed_endpoints_array.shape) == 0 and len(endpoint_array.shape) > 0):\n",
    "            out_df.at[index, 'precision'] = 0.0\n",
    "            out_df.at[index, 'recall'] = 0.0\n",
    "            out_df.at[index, 'f1'] = 0.0\n",
    "            continue \n",
    "\n",
    "        #if we propose endpoints but there are none, it's wrong\n",
    "        elif proposed_endpoints_array.size > 0 and endpoint_array.size == 0:\n",
    "            out_df.at[index, 'precision'] = 0.0\n",
    "            out_df.at[index, 'recall'] = 0.0\n",
    "            out_df.at[index, 'f1'] = 0.0\n",
    "            continue\n",
    "\n",
    "        #should skip for now if the endpoint array is size zero to avoid divide by zero error\n",
    "        elif endpoint_array.size == 0:\n",
    "            out_df.at[index, 'precision'] = null\n",
    "            out_df.at[index, 'recall'] = null\n",
    "            out_df.at[index, 'f1'] = null\n",
    "            continue\n",
    "\n",
    "        else:                                                                                   \n",
    "            endpoint_ids = np.arange(0, len(endpoint_array))\n",
    "            test_ids = np.arange(0, len(proposed_endpoints_array)) + len(proposed_endpoints_array)\n",
    "\n",
    "            #get precision and recall at end of each iteration \n",
    "            analysis = run_synapse_analysis(\n",
    "                endpoint_array,\n",
    "                np.array(endpoint_ids),\n",
    "                proposed_endpoints_array,\n",
    "                np.array(test_ids),\n",
    "                threshold,\n",
    "                iso_correction=10,\n",
    "                )        \n",
    "\n",
    "            output.loc[len(output.index)] = [run, analysis.precision, analysis.recall, analysis.f1, segID, len(endpoint_array), endpoint_array, row[\"comments\"], proposed_endpoints_array]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Various utility classes and functions for Confirms.\n",
    "\"\"\"\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "def calculate_precision_recall(tp, fp, fn):\n",
    "    \"\"\"\n",
    "    Calculate precision/recall from given true positives, false positives, and false negatives.\n",
    "    Parameters\n",
    "    ----------\n",
    "    tp : int\n",
    "        The number of true positives.\n",
    "    fp : int\n",
    "        The number of false positives.\n",
    "    fn : int\n",
    "        The number of false negatives.\n",
    "    Returns\n",
    "    -------\n",
    "    precision : float\n",
    "        The precision score.\n",
    "    recall : float\n",
    "        The recall score.\n",
    "    \"\"\"\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def calculate_f1(precision, recall):\n",
    "    \"\"\"\n",
    "    Calculate the F1 score from precision/recall scores.\n",
    "    Parameters\n",
    "    ----------\n",
    "    precision : float\n",
    "        The precision score.\n",
    "    recall : float\n",
    "        The recall score.\n",
    "    Returns\n",
    "    -------\n",
    "    f1 : float\n",
    "        The F1 score.\n",
    "    \"\"\"\n",
    "    return 2 * ((precision * recall) / (precision + recall))\n",
    "\n",
    "\n",
    "def get_summary_metrics(array):\n",
    "    \"\"\"\n",
    "    Calculate a number of summary metrics on an array of numbers.\n",
    "    Parameters\n",
    "    ----------\n",
    "    array : array_like\n",
    "        Array containing numbers who summary metrics is desired.\n",
    "    Returns\n",
    "    -------\n",
    "    metrics : dict\n",
    "        Dict containing the mean, median, max, min, range, standard deviation,\n",
    "        and variance of the input array.\n",
    "    \"\"\"\n",
    "    summary_object = {}\n",
    "    array = np.array(array)\n",
    "\n",
    "    summary_object[\"mean\"] = np.mean(array)\n",
    "    summary_object[\"median\"] = np.median(array)\n",
    "    summary_object[\"max\"] = np.amax(array)\n",
    "    summary_object[\"min\"] = np.amin(array)\n",
    "    summary_object[\"range\"] = np.mean(array)\n",
    "    summary_object[\"stddev\"] = np.std(array)\n",
    "    summary_object[\"variance\"] = np.var(array)\n",
    "\n",
    "    return summary_object\n",
    "\n",
    "\n",
    "def munkres_assignment(workers, jobs):\n",
    "    \"\"\"\n",
    "    Perform hungarian-munkres assignment.\n",
    "    Parameters\n",
    "    ----------\n",
    "    workers : array_like\n",
    "        Array containing the first set of points.\n",
    "    jobs : array_like\n",
    "        Array containing the second set of points.\n",
    "    Returns\n",
    "    -------\n",
    "    cost_matrix : numpy.ndarray\n",
    "        Matrix containing pairwise distances (the cost of assignment).\n",
    "    row_ind : numpy.ndarray\n",
    "        Row indices of cost_matrix for optimal assignment.\n",
    "    col_ind : numpy.ndarray\n",
    "        Column indices of cost_matrx for optimal assignment.\n",
    "    \"\"\"\n",
    "    cost_matrix = spatial.distance.cdist(workers, jobs, \"euclidean\")\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    return cost_matrix, row_ind, col_ind\n",
    "\n",
    "\n",
    "def make_isotropic(xyz, correction, dimen=2):\n",
    "    \"\"\"\n",
    "    Correct anisotropy in a collection of x, y, z coordinates.\n",
    "    This function performs a deepcopy of xyz before making the necessary modifications.\n",
    "    Parameters\n",
    "    ----------\n",
    "    xyz : numpy.ndarray or pandas.DataFrame:\n",
    "        The coordinate values.\n",
    "    correction : float\n",
    "        The value to correct anisotrophy.\n",
    "    dimen : int\n",
    "        Index of last dimension to which to apply the correction. Default is 2.\n",
    "    Returns\n",
    "    -------\n",
    "    iso_xyz : numpy.ndarray or pandas.DataFrame\n",
    "        An isotropic version of xyz.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(xyz, np.ndarray) and not isinstance(xyz, pd.DataFrame):\n",
    "        xyz = np.asarray(xyz)\n",
    "\n",
    "    shape = np.shape(xyz)\n",
    "    isotropic_xyz = xyz.copy()\n",
    "    size = shape[-1]\n",
    "    if dimen >= size or dimen < 0:\n",
    "        raise ValueError(\"improper dimen value (valid: 0 through {})\".format(size - 1))\n",
    "    if isinstance(xyz, np.ndarray):\n",
    "        isotropic_xyz[..., dimen] = isotropic_xyz[..., dimen] * correction\n",
    "    else:\n",
    "        isotropic_xyz.iloc[:, dimen] = isotropic_xyz.iloc[:, dimen] * correction\n",
    "    return isotropic_xyz\n",
    "\n",
    "\n",
    "# def get_range(center, pixel_size, radius):\n",
    "#     \"\"\"\n",
    "#     <Description here>\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     center : <type>\n",
    "#         <Description>\n",
    "#     pixel_size : <type>\n",
    "#         <Description>\n",
    "#     radius : <type>\n",
    "#         <Description>\n",
    "    \n",
    "#     Returns\n",
    "#     -------\n",
    "#     range : numpy.ndarray\n",
    "#         <Description>\n",
    "#     \"\"\"\n",
    "#     return np.asarray(\n",
    "#         [int(center - (radius / pixel_size)), int(center + (radius / pixel_size))],\n",
    "#         dtype=\"int\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Confirms synapse processing and analysis functions.\n",
    "\"\"\"\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# from . import utils\n",
    "# import utils\n",
    "\n",
    "SynapseMetrics = namedtuple(\n",
    "    \"SynapseMetrics\",\n",
    "    [\"precision\", \"recall\", \"f1\", \"tp_gt_ids\", \"tp_test_ids\", \"fp_ids\", \"fn_ids\"],\n",
    ")\n",
    "\n",
    "\n",
    "def filter_synapse_id_core(volume, xyz, ids, box_radius_nm=2500):\n",
    "    \"\"\"\n",
    "    Filter synapses to only return those within a central core.\n",
    "    Parameters\n",
    "    ----------\n",
    "    volume : dict\n",
    "        The volume to filter on.\n",
    "    xyz : array_like\n",
    "        Synapse xyz coordinates.\n",
    "    ids : array_like\n",
    "        Synapse hash (ids).\n",
    "    box_radius_nm : int\n",
    "        radius of cube, from volume core.\n",
    "    Returns\n",
    "    -------\n",
    "        xyz : numpy.ndarray\n",
    "            Synapse coordinates.\n",
    "        ids : numpy.ndarray\n",
    "            Synapse ids.\n",
    "    \"\"\"\n",
    "\n",
    "    xyz_out = []\n",
    "    id_out = []\n",
    "    center = np.asarray(volume[\"center\"], \"float\")\n",
    "    base_resolution = np.asarray(volume[\"base_resolution\"], \"float\")\n",
    "    annotation_resolution = np.asarray(volume[\"resolution\"], \"float\")\n",
    "\n",
    "    pad_vx = box_radius_nm / base_resolution[0] / (2 ** annotation_resolution)\n",
    "    pad_vy = box_radius_nm / base_resolution[1] / (2 ** annotation_resolution)\n",
    "    pad_vz = box_radius_nm / base_resolution[2]\n",
    "    xr = (center[0] - pad_vx, center[0] + pad_vx)\n",
    "    yr = (center[1] - pad_vy, center[1] + pad_vy)\n",
    "    zr = (center[2] - pad_vz, center[2] + pad_vz)\n",
    "\n",
    "    for i in range(len(xyz)):\n",
    "        x = xyz[i][0]\n",
    "        y = xyz[i][1]\n",
    "        z = xyz[i][2]\n",
    "\n",
    "        if (\n",
    "            x > xr[0]\n",
    "            and x < xr[1]\n",
    "            and y > yr[0]\n",
    "            and y < yr[1]\n",
    "            and z > zr[0]\n",
    "            and z < zr[1]\n",
    "        ):\n",
    "            xyz_out.append(xyz[i])\n",
    "            id_out.append(ids[i])\n",
    "\n",
    "    return np.array(xyz_out), np.array(id_out, dtype=np.object)\n",
    "\n",
    "\n",
    "def synapse_match(\n",
    "    xyz_truth, xyz_detect, id_truth, id_detect, thresh\n",
    "):  # pylint: disable=R0914\n",
    "    \"\"\"\n",
    "    <Description here>\n",
    "    Parameters\n",
    "    ----------\n",
    "    xyz_truth : array_like\n",
    "        <description>\n",
    "    xyz_detect : array_like\n",
    "        <description>\n",
    "    id_truth : array_like\n",
    "        <description>\n",
    "    id_detech : array_like\n",
    "        <description>\n",
    "    thresh : float\n",
    "        <description>\n",
    "    Returns\n",
    "    -------\n",
    "    id_lookup : <type>\n",
    "        <description>\n",
    "    \"\"\"\n",
    "\n",
    "    # pylint: disable=C0103\n",
    "\n",
    "    # Ensure we have numpy arrays\n",
    "    xyz_truth = np.asarray(xyz_truth)\n",
    "    xyz_detect = np.asarray(xyz_detect)\n",
    "    id_truth = np.asarray(id_truth)\n",
    "    id_detect = np.asarray(id_detect)\n",
    "\n",
    "    cost, row_ind, col_ind = munkres_assignment(xyz_truth, xyz_detect)\n",
    "    print(cost)\n",
    "    match_idx = np.where(cost[row_ind, col_ind] < thresh)\n",
    "\n",
    "    if len(match_idx) > 0:\n",
    "        # row is idx of GT TP\n",
    "        # col is idx of student TP\n",
    "        gt_tp_idx, det_tp_idx = row_ind[match_idx], col_ind[match_idx]\n",
    "        gt_tp_ids, det_tp_ids = id_truth[gt_tp_idx], id_detect[det_tp_idx]\n",
    "        # Combine into pairs\n",
    "        id_lookup = np.column_stack((gt_tp_ids, det_tp_ids))\n",
    "    else:\n",
    "        gt_tp_idx = []\n",
    "        det_tp_idx = []\n",
    "        id_lookup = np.column_stack(\n",
    "            (np.array([], dtype=\"object\"), np.array([], dtype=\"object\"))\n",
    "        )\n",
    "\n",
    "    # not in row (set diff) are FN\n",
    "    gt_syn_idx = np.arange(0, len(xyz_truth))\n",
    "    fn_idx = np.setdiff1d(gt_syn_idx, gt_tp_idx)\n",
    "    if len(fn_idx) > 0:\n",
    "        fn_ids = id_truth[fn_idx]\n",
    "        id_lookup_fn = np.column_stack((fn_ids, np.repeat(None, len(fn_ids))))\n",
    "        id_lookup = np.concatenate((id_lookup, id_lookup_fn))\n",
    "\n",
    "    # not in col (set diff) are FP\n",
    "    det_syn_idx = set(np.arange(0, len(xyz_detect)))\n",
    "    fp_idx = np.asarray(list(det_syn_idx.difference(det_tp_idx)))\n",
    "    if len(fp_idx) > 0:\n",
    "        fp_ids = id_detect[fp_idx]\n",
    "        id_lookup_fp = np.column_stack((np.repeat(None, len(fp_ids)), fp_ids))\n",
    "        id_lookup = np.concatenate((id_lookup, id_lookup_fp))\n",
    "\n",
    "    return pd.DataFrame(id_lookup, columns=[\"ground_truth\", \"detect\"])\n",
    "\n",
    "\n",
    "def run_synapse_analysis(\n",
    "    gt_xyzs,\n",
    "    gt_ids,\n",
    "    test_xyzs,\n",
    "    test_ids,\n",
    "    threshold,\n",
    "    iso_corrected=False,\n",
    "    iso_correction=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    <Description here>\n",
    "    Parameters\n",
    "    ----------\n",
    "    gt_xyzs : numpy.ndarray\n",
    "        Array of ground truth xyz coordinates\n",
    "    gt_ids : numpy.ndarray\n",
    "        Array of ids associated with ground truth xyz coordinates\n",
    "    test_xyzs : numpy.ndarray\n",
    "        Array of test xyz coordinates\n",
    "    test_ids : numpy.ndarray\n",
    "        Array of ids associated with test xyz coordinates\n",
    "    threshold : float\n",
    "        Synapse matching threshold\n",
    "    iso_corrected : boolean\n",
    "        Mark whether the data is isotropic. If not, it will be made isotropic using the\n",
    "        `iso_correction` parameter.\n",
    "    iso_correction : float\n",
    "        Value to correct anistropy.\n",
    "    Returns\n",
    "    -------\n",
    "    sm : SynapseMetrics\n",
    "        Resultant object containing precision, recall, and F1 scores, along with\n",
    "        with true positive (both ground truth and test), false positive, and false negative\n",
    "        ids.\n",
    "    \"\"\"\n",
    "    # pylint: disable=R0913,R0914\n",
    "    if not iso_corrected:\n",
    "        # gt_xyzs = utils.make_isotropic(gt_xyzs, iso_correction)\n",
    "        gt_xyzs = make_isotropic(gt_xyzs, iso_correction)\n",
    "        # test_xyzs = utils.make_isotropic(test_xyzs, iso_correction)\n",
    "        test_xyzs = make_isotropic(test_xyzs, iso_correction)\n",
    "\n",
    "    results_table = synapse_match(gt_xyzs, test_xyzs, gt_ids, test_ids, threshold)\n",
    "\n",
    "    tp = results_table.dropna()\n",
    "    fn = results_table[results_table.detect.isnull()]\n",
    "    fp = results_table[results_table.ground_truth.isnull()]\n",
    "\n",
    "    assert len(tp.ground_truth) == len(\n",
    "        tp.detect\n",
    "    ), \"true positive ground truth and test size mismatch\"\n",
    "\n",
    "    tp_count = len(tp)\n",
    "    fp_count = len(fp)\n",
    "    fn_count = len(fn)\n",
    "\n",
    "    try:\n",
    "        # precision, recall = utils.calculate_precision_recall(\n",
    "        #     tp_count, fp_count, fn_count\n",
    "        # )\n",
    "\n",
    "        precision, recall = calculate_precision_recall(\n",
    "            tp_count, fp_count, fn_count\n",
    "        )\n",
    "\n",
    "    except ZeroDivisionError:\n",
    "        precision, recall = np.nan, np.nan\n",
    "\n",
    "    try:\n",
    "        # f1 = utils.calculate_f1(precision, recall)\n",
    "        f1 = calculate_f1(precision, recall)\n",
    "    except ZeroDivisionError:\n",
    "        f1 = np.nan\n",
    "\n",
    "    sm = SynapseMetrics(\n",
    "        precision=precision,\n",
    "        recall=recall,\n",
    "        f1=f1,\n",
    "        tp_gt_ids=np.asarray(tp.ground_truth),\n",
    "        tp_test_ids=np.asarray(tp.detect),\n",
    "        fp_ids=np.asarray(fp.detect),\n",
    "        fn_ids=np.asarray(fn.ground_truth),\n",
    "    )\n",
    "    return sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING\n",
    "gt_xyzs = np.array([[100475, 143982,  21654],\n",
    "                    [100583, 144194,  21702]])\n",
    "\n",
    "gt_ids = np.array([1,2])\n",
    "\n",
    "test_xyzs = np.array([[100979, 144058,  21663],\n",
    "                      [100780, 144830,  21653]])\n",
    "\n",
    "test_ids = np.array([3,4])\n",
    "\n",
    "threshold = 500\n",
    "print(\"EGHSEGH\")\n",
    "analysis = run_synapse_analysis(\n",
    "    gt_xyzs,\n",
    "    gt_ids,\n",
    "    test_xyzs,\n",
    "    test_ids,\n",
    "    threshold,\n",
    "    iso_correction=10,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using analysis functions and helper functions to calculate recall, precision, f1 for multiple seg ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_df = load[['seg_id', 'num_endpoints', 'endpoints', 'comments', 'proposed_endpoints']]\n",
    "# empty_l = [None for _ in range(len(load))]\n",
    "# out_df.insert(0, 'run', range(len(load)))\n",
    "# out_df.insert(1, 'precision', empty_l)\n",
    "# out_df.insert(2, 'recall', empty_l)\n",
    "# out_df.insert(3, 'f1', empty_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_df.head(2)\n",
    "# #TEST 1: invalidation_d at default: 12000, WITH Humfrey_Smoothing: 50 iterations, Decimation_Factor = 0.50\n",
    "\n",
    "# #should track total amount of correct gt recovered \n",
    "\n",
    "# def testing_metrics(load, threshold, run, output):\n",
    "#     # # Create out_df\n",
    "#     # out_df = load[['seg_id', 'num_endpoints',\n",
    "#     #                'endpoints', 'comments', 'proposed_endpoints']]\n",
    "#     # empty_l = [None for _ in range(len(load))]\n",
    "#     # out_df.insert(0, 'run', range(len(load)))\n",
    "#     # out_df.insert(1, 'precision', empty_l)\n",
    "#     # out_df.insert(2, 'recall', empty_l)\n",
    "#     # out_df.insert(3, 'f1', empty_l)\n",
    "\n",
    "#     # FIGURE OUT WHETHER WE SHOULD PASS IN OUT_DF INTO THIS FUNCTION AND APPEND IN PLACE OR IF WE SHOULD RETURN OUT_DF AND CONCAT\n",
    "\n",
    "#     for index, row in load.iterrows():\n",
    "#         endpoint_array = np.array(row[\"endpoints\"])\n",
    "#         proposed_endpoints_array = np.array(row[\"proposed_endpoints\"])\n",
    "#         segID = row[\"seg_id\"]\n",
    "\n",
    "#         #skip anything seg_id that isn't 'good'\n",
    "#         if row[\"comments\"] != 'good':\n",
    "#             print(\"skipping \\n\")\n",
    "#             continue\n",
    "        \n",
    "#         #if both say no endpoints, then it's correct\n",
    "#         elif (len(proposed_endpoints_array.shape) == 0 and len(endpoint_array.shape) == 0):\n",
    "#             out_df.at[index, 'precision'] =1.0\n",
    "#             out_df.at[index, 'recall'] = 1.0\n",
    "#             out_df.at[index, 'f1'] = 1.0\n",
    "\n",
    "#             continue\n",
    "\n",
    "#         #if we propose no endpoints but there are endpoints, it's wrong\n",
    "#         elif (len(proposed_endpoints_array.shape) == 0 and len(endpoint_array.shape) > 0):\n",
    "#             out_df.at[index, 'precision'] = 0.0\n",
    "#             out_df.at[index, 'recall'] = 0.0\n",
    "#             out_df.at[index, 'f1'] = 0.0\n",
    "#             continue \n",
    "\n",
    "#         #if we propose endpoints but there are none, it's wrong\n",
    "#         elif proposed_endpoints_array.size > 0 and endpoint_array.size == 0:\n",
    "#             out_df.at[index, 'precision'] = 0.0\n",
    "#             out_df.at[index, 'recall'] = 0.0\n",
    "#             out_df.at[index, 'f1'] = 0.0\n",
    "#             continue\n",
    "\n",
    "#         #should skip for now if the endpoint array is size zero to avoid divide by zero error\n",
    "#         elif endpoint_array.size == 0:\n",
    "#             out_df.at[index, 'precision'] = null\n",
    "#             out_df.at[index, 'recall'] = null\n",
    "#             out_df.at[index, 'f1'] = null\n",
    "#             continue\n",
    "\n",
    "#         else:                                                                                   \n",
    "#             endpoint_ids = np.arange(0, len(endpoint_array))\n",
    "#             test_ids = np.arange(0, len(proposed_endpoints_array)) + len(proposed_endpoints_array)\n",
    "\n",
    "#             #get precision and recall at end of each iteration \n",
    "#             analysis = run_synapse_analysis(\n",
    "#                 endpoint_array,\n",
    "#                 np.array(endpoint_ids),\n",
    "#                 proposed_endpoints_array,\n",
    "#                 np.array(test_ids),\n",
    "#                 threshold,\n",
    "#                 iso_correction=10,\n",
    "#                 )        \n",
    "\n",
    "#             output.loc[len(output.index)] = [run, analysis.precision, analysis.recall, analysis.f1, segID, len(endpoint_array), endpoint_array, row[\"comments\"], proposed_endpoints_array]\n",
    "\n",
    "#             # out_df.at[index,'precision'] = analysis.precision\n",
    "#             # out_df.at[index,'recall'] = analysis.recall\n",
    "#             # out_df.at[index,'f1'] = analysis.f1\n",
    "            \n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TEST 1: invalidation_d at default: 12000, WITH Humfrey_Smoothing: 50 iterations, Decimation_Factor = 0.50\n",
    "\n",
    "# #should track total amount of correct gt recovered \n",
    "\n",
    "# def testing_metrics(load, threshold):\n",
    "#     for index, row in load.iterrows():\n",
    "#         #reset num correct\n",
    "#         # num_correct = 0\n",
    "#         endpoint_array = np.array(row[\"endpoints\"])\n",
    "#         proposed_endpoints_array = np.array(row[\"proposed_endpoints\"])\n",
    "#         # print(f\"Seg id: {row['seg_id']}\")\n",
    "#         segID = row[\"seg_id\"]\n",
    "#         # print(\"Endpoint Array:\")\n",
    "#         # print(endpoint_array)\n",
    "#         #print(len(endpoint_array))\n",
    "#         # print()\n",
    "#         # print(\"Proposed Endpoint Array:\")\n",
    "#         # print(proposed_endpoints_array)\n",
    "#         # print()\n",
    "#         #print(load[\"Endpoints\"])\n",
    "        \n",
    "#         #skip anything seg_id that isn't 'good'\n",
    "#         if row[\"comments\"] != 'good':\n",
    "#             print(\"skipping \\n\")\n",
    "#             continue\n",
    "\n",
    "#         #learn better pandas filtered rather than if else statements     \n",
    "#         #maybe skip v-shaped segments\n",
    "#         #add in a label for ground truth \n",
    "        \n",
    "#         #if both say no endpoints, then it's correct\n",
    "#         elif (len(proposed_endpoints_array.shape) == 0 and len(endpoint_array.shape) == 0):\n",
    "#             # print(\"correct, no endpoints\")\n",
    "#             # print(\"\\nNEXT POINTS\")\n",
    "#             out_df.at[index, 'precision'] =1.0\n",
    "#             out_df.at[index, 'recall'] = 1.0\n",
    "#             out_df.at[index, 'f1'] = 1.0\n",
    "\n",
    "#             continue\n",
    "\n",
    "#         #if we propose no endpoints but there are endpoints, it's wrong\n",
    "#         elif (len(proposed_endpoints_array.shape) == 0 and len(endpoint_array.shape) > 0):\n",
    "#             # print(\"no proposed endpoints found, but ground truth endpoints exist\")\n",
    "#             # print(\"\\nNEXT POINTS\")\n",
    "#             out_df.at[index, 'precision'] = 0.0\n",
    "#             out_df.at[index, 'recall'] = 0.0\n",
    "#             out_df.at[index, 'f1'] = 0.0\n",
    "#             continue \n",
    "\n",
    "#         #if we propose endpoints but there are none, it's wrong\n",
    "#         elif proposed_endpoints_array.size > 0 and endpoint_array.size == 0:\n",
    "#             # print(\"not correct--too many proposed endpoints but none ground truth endpoints exist\")\n",
    "#             # print(\"\\nNEXT POINTS\")\n",
    "#             out_df.at[index, 'precision'] = 0.0\n",
    "#             out_df.at[index, 'recall'] = 0.0\n",
    "#             out_df.at[index, 'f1'] = 0.0\n",
    "#             continue\n",
    "\n",
    "#         #should skip for now if the endpoint array is size zero to avoid divide by zero error\n",
    "#         elif endpoint_array.size == 0:\n",
    "#             # print(\"array size is 0\")\n",
    "#             # print(\"\\nNEXT POINTS\")\n",
    "#             out_df.at[index, 'precision'] = null\n",
    "#             out_df.at[index, 'recall'] = null\n",
    "#             out_df.at[index, 'f1'] = null\n",
    "#             continue\n",
    "\n",
    "#         else:                                                                                   \n",
    "#             # for proposed_endpoint in proposed_endpoints_array:\n",
    "#                 # smallest_distance= find_euclidian_distance(proposed_endpoint, endpoint_array)\n",
    "#                 # print(smallest_distance)\n",
    "                \n",
    "#                 #compare to threshold and return yes or no \n",
    "#                 # if smallest_distance < threshold:\n",
    "#                 #     print(\"correct\")\n",
    "#                 #     num_correct = num_correct + 1\n",
    "#                 #     print()\n",
    "#                 # else:\n",
    "#                 #     print(\"wrong\")\n",
    "#                 #     print()\n",
    "#                 # pass\n",
    "\n",
    "#         # endpoint_ids = []\n",
    "#         # test_ids = []\n",
    "#         # for i in range(0, len(endpoint_array)):\n",
    "#         #     endpoint_ids.append(segID)\n",
    "            \n",
    "#         # for i in range(0, len(proposed_endpoints_array)):\n",
    "#         #     test_ids.append(segID)\n",
    "\n",
    "#         endpoint_ids = np.arange(0, len(endpoint_array))\n",
    "#         test_ids = np.arange(0, len(proposed_endpoints_array)) + len(proposed_endpoints_array)\n",
    "\n",
    "#         #get precision and recall at end of each iteration \n",
    "#         analysis = run_synapse_analysis(\n",
    "#             endpoint_array,\n",
    "#             np.array(endpoint_ids),\n",
    "#             proposed_endpoints_array,\n",
    "#             np.array(test_ids),\n",
    "#             threshold,\n",
    "#             iso_correction=10,\n",
    "#             )        \n",
    "\n",
    "#         out_df.at[index,'precision'] = analysis.precision\n",
    "#         out_df.at[index,'recall'] = analysis.recall\n",
    "#         out_df.at[index,'f1'] = analysis.f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.to_csv('results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df = pd.DataFrame(columns=[\"run\", \"precision\", \"recall\"])\n",
    "pd_df_2 = pd.DataFrame(columns=[\"run\", \"precision\"])\n",
    "for i in range(0,7):\n",
    "    pd_df_2.loc[len(pd_df.index)] = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make threshold for each endpoint\n",
    "experiment with box (cannot overlap)\n",
    "for each endpoint see if it finds in box -- if yes true found endpont, if false --not an endpoint\n",
    "find distance from predicted to ground truth \n",
    "could still do MSE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threshold: \n",
    "-find the size of the segment (maybe connected componenets)\n",
    "-array from the skel\n",
    "-flat is edge case \n",
    "-maybe a percentage \n",
    "--a hard number \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing different combinations of params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in data\n",
    "load = pd.read_csv(\"agents/Data/endpoints_gt5.csv\")\n",
    "endpoints_dict = {}\n",
    "\n",
    "#creating series objects for the desired collumns\n",
    "segIDs = load['seg_id']\n",
    "Endpoints = load['endpoints']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate out_df\n",
    "\n",
    "out_df = pd.DataFrame(columns=[\"run\", \"precision\", \"recall\", \"f1\", \"seg_id\", \"num_endpoints\", \"endpoints\", \"comments\", \"proposed_endpoints\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = 0\n",
    "# for invalidation_d in range(4000, 6001, 200):\n",
    "for invalidation_d in range (4000, 4201, 200):\n",
    "    # for num_humfrey_iters in range(0,251,25):\n",
    "    for num_humfrey_iters in range (50,76, 25):\n",
    "        # for decimation_factor in np.arange(0.3, 0.71, 0.04):\n",
    "        for decimation_factor in np.arange(0.3, 0.35, 0.04):\n",
    "            run+=1\n",
    "            load_with_proposed_endpoints = endpoint_generator(load, invalidation_d= invalidation_d, humfrey_iters= num_humfrey_iters, decimation_factor= decimation_factor)\n",
    "            out_df = testing_metrics(load_with_proposed_endpoints, threshold = 500, run = run, output = out_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Psuedocode:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate load(loading in w/o proposed endpoints)\n",
    "generate the out_df here\n",
    "\n",
    "for (variations of invalidation_d):\n",
    "    for (variations of num_humfrey_iter):\n",
    "        for (variations of decimation_factor):\n",
    "            load_w_proposed = endpoint_generator(load, put in the invalidation_d, humfrey_iters, and decimation factor)\n",
    "            pass load with proposed into testing block, have testing block return a df\n",
    "            append the df returned to out_df\n",
    "\n",
    "\n",
    "export out_df to a csv(results.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to complie results!!!! \n",
    "\n",
    "add to dataframe all of the precision and recalls \n",
    "plot the results and check for \n",
    "params don't have to be the best \n",
    "\n",
    "\n",
    "make a sepereate dataframe \n",
    "\n",
    "look: pd.concat\n",
    "\n",
    "mention the params \n",
    "\n",
    "or have a collumn that has run number "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRECISION: #test at ground truth / # test  \n",
    "\n",
    "RECALL: # gt recovered / number ground truth \n",
    "(would be 100% if you found all the correct endponts...NO EXTRAS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
