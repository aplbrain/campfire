{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from caveclient import CAVEclient\n",
    "from intern import array\n",
    "import pickle\n",
    "import numpy as np\n",
    "from agents import data_loader\n",
    "from cloudvolume import CloudVolume\n",
    "from membrane_detection import membranes\n",
    "from agents.scripts import precompute_membrane_vectors, create_post_matrix, merge_paths, get_soma\n",
    "import agents.sensor\n",
    "from agents.run import run_agents\n",
    "import aws.sqs as sqs\n",
    "import sys\n",
    "import time\n",
    "import ast\n",
    "import pandas as pd\n",
    "import agents.scripts as scripts\n",
    "from drive import drive\n",
    "from finding_orphans import *\n",
    "from math import sqrt\n",
    "from tip_finding.tip_finding import tip_finder_decimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = pd.read_csv(\"agents/Data/endpoints_gt4.csv\")\n",
    "endpoints_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating series objects for the desired collumns \n",
    "segIDs = load['seg_id']\n",
    "Endpoints = load['endpoints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def endpoint_generator(load, invalidation_d, humfrey_iters, decimation_factor):\n",
    "    for idx, i in enumerate(load['seg_id']):\n",
    "        if idx==20:\n",
    "            break\n",
    "        try:\n",
    "            t1, skel, mesh_obj = tip_finder_decimation(str(i), invalidation_d = invalidation_d, humfrey_iters = humfrey_iters, decimation_factor = decimation_factor)\n",
    "            endpoints_dict[i] = t1\n",
    "        except:\n",
    "            print(f\"\\n\\nSeg {i} returned error on get. Skipping.\\n\")\n",
    "            pass\n",
    "\n",
    "    load['endpoints'] = load['endpoints'].apply(lambda x: list(ast.literal_eval(x)))\n",
    "    load['proposed_endpoints'] = load.seg_id.map(endpoints_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correctly  have 2 collumns for the proposed vs gt endpoints\n",
    "load.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_euclidian_distance(proposed_endpoint, gt_endponts_array):\n",
    "    smallest_distance = 2**30\n",
    "    for endpoint in gt_endponts_array:\n",
    "        difference = proposed_endpoint - endpoint\n",
    "        distance = np.sqrt(np.sum(np.square(difference)))\n",
    "        if(distance < smallest_distance):\n",
    "            smallest_distance = distance \n",
    "    return smallest_distance \n",
    "            \n",
    "    #could add endpoint to dict\n",
    "    \n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall(num_correct, array_endpoints): \n",
    "    recall = num_correct / len(array_endpoints)\n",
    "    return recall * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision(num_correct, array_proposed): \n",
    "    precision = num_correct / len(array_proposed)\n",
    "    return precision * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST 1: invalidation_d at default: 12000, WITH Humfrey_Smoothing: 50 iterations, Decimation_Factor = 0.50\n",
    "\n",
    "#should track total amount of correct gt recovered \n",
    "\n",
    "for index, row in load.iterrows():\n",
    "    #reset num correct\n",
    "    num_correct = 0\n",
    "    threshold = 500\n",
    "    endpoint_array = np.array(row[\"endpoints\"])\n",
    "    proposed_endpoints_array = np.array(row[\"proposed_endpoints\"])\n",
    "    print(f\"Seg id: %i\" %(row[\"seg_id\"]))\n",
    "    segID = row[\"seg_id\"]\n",
    "    print(\"Endpoint Array:\")\n",
    "    print(endpoint_array)\n",
    "    #print(len(endpoint_array))\n",
    "    print()\n",
    "    print(\"Proposed Endpoint Array:\")\n",
    "    print(proposed_endpoints_array)\n",
    "    print()\n",
    "    #print(load[\"Endpoints\"])\n",
    "\n",
    "    #skip sharply angled/false merged\n",
    "    if row[\"comments\"] == 'a':\n",
    "        print(\"skipping\")\n",
    "        print()\n",
    "        continue\n",
    "\n",
    "    #skip small seg ids\n",
    "    if row[\"comments\"] == 's':\n",
    "        print(\"skipping\")\n",
    "        print()\n",
    "        continue \n",
    "   \n",
    "    #skip any seg ids with false merges: \n",
    "    if row[\"comments\"] in ('m', \"mm\", \"e\"):\n",
    "        print(\"skipping\")\n",
    "        print()\n",
    "        continue\n",
    "\n",
    "    #learn better pandas filtered rather than if else statements \n",
    "\n",
    "    #maybe skip v-shaped segments\n",
    "\n",
    "    #add in a label for ground truth \n",
    "    \n",
    "    #if both say no endpoints, then it's correct\n",
    "    if (len(proposed_endpoints_array.shape) == 0 and len(endpoint_array.shape) == 0):\n",
    "        print(\"correct, no endpoints\")\n",
    "        print(\"\\nNEXT POINTS\")\n",
    "        continue\n",
    "    #if we propose no endpoints but there are endpoints, it's wrong\n",
    "    elif (len(proposed_endpoints_array.shape) == 0 and len(endpoint_array.shape) > 0):\n",
    "        print(\"no proposed endpoints found, but ground truth endpoints exist\")\n",
    "        print(\"\\nNEXT POINTS\")\n",
    "        continue \n",
    "\n",
    "    #if we propose endpoints but there are none, it's wrong\n",
    "    elif proposed_endpoints_array.size > 0 and endpoint_array.size == 0:\n",
    "        print(\"not correct--too many proposed endpoints but none ground truth endpoints exist\")\n",
    "        print(\"\\nNEXT POINTS\")\n",
    "        continue\n",
    "    #should skip for now if the endpoint array is size zero to avoid divide by zero error\n",
    "    elif endpoint_array.size == 0:\n",
    "        print(\"array size is 0\")\n",
    "        print(\"\\nNEXT POINTS\")\n",
    "        continue\n",
    "    else:                                                                                   \n",
    "        for proposed_endpoint in proposed_endpoints_array:\n",
    "            smallest_distance= find_euclidian_distance(proposed_endpoint, endpoint_array)\n",
    "            print(smallest_distance)\n",
    "            \n",
    "            \n",
    "            #compare to threshold and return yes or no \n",
    "            if smallest_distance < threshold:\n",
    "                print(\"correct\")\n",
    "                num_correct = num_correct + 1\n",
    "                print()\n",
    "            else:\n",
    "                print(\"wrong\")\n",
    "                print()\n",
    "        #get precision and recall at end of each iteration \n",
    "        analysis = run_synapse_analysis(\n",
    "            endpoint_array,\n",
    "            s,\n",
    "            test_xyzs,\n",
    "            test_ids,\n",
    "            threshold,\n",
    "            iso_correction=10,\n",
    "        )\n",
    "        recall = get_recall(num_correct, endpoint_array)\n",
    "        precision = get_precision(num_correct, proposed_endpoints_array)\n",
    "        print(\"recall = \" + str(recall))\n",
    "        print(\"precision = \" + str(precision))\n",
    "        if recall > 100.0 or precision > 100.0:\n",
    "            print(\"ERROR: RESULTS OVER 100% DETECTED\")\n",
    "        print()\n",
    "        print(\"\\nNEXT POINTS\")\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make threshold for each endpoint\n",
    "experiment with box (cannot overlap)\n",
    "for each endpoint see if it finds in box -- if yes true found endpont, if false --not an endpoint\n",
    "find distance from predicted to ground truth \n",
    "could still do MSE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threshold: \n",
    "-find the size of the segment (maybe connected componenets)\n",
    "-array from the skel\n",
    "-flat is edge case \n",
    "-maybe a percentage \n",
    "--a hard number \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(proposed_endpoints_array.shape) == 0:\n",
    "    print(\"correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Various utility classes and functions for Confirms.\n",
    "\"\"\"\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "def calculate_precision_recall(tp, fp, fn):\n",
    "    \"\"\"\n",
    "    Calculate precision/recall from given true positives, false positives, and false negatives.\n",
    "    Parameters\n",
    "    ----------\n",
    "    tp : int\n",
    "        The number of true positives.\n",
    "    fp : int\n",
    "        The number of false positives.\n",
    "    fn : int\n",
    "        The number of false negatives.\n",
    "    Returns\n",
    "    -------\n",
    "    precision : float\n",
    "        The precision score.\n",
    "    recall : float\n",
    "        The recall score.\n",
    "    \"\"\"\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def calculate_f1(precision, recall):\n",
    "    \"\"\"\n",
    "    Calculate the F1 score from precision/recall scores.\n",
    "    Parameters\n",
    "    ----------\n",
    "    precision : float\n",
    "        The precision score.\n",
    "    recall : float\n",
    "        The recall score.\n",
    "    Returns\n",
    "    -------\n",
    "    f1 : float\n",
    "        The F1 score.\n",
    "    \"\"\"\n",
    "    return 2 * ((precision * recall) / (precision + recall))\n",
    "\n",
    "\n",
    "def get_summary_metrics(array):\n",
    "    \"\"\"\n",
    "    Calculate a number of summary metrics on an array of numbers.\n",
    "    Parameters\n",
    "    ----------\n",
    "    array : array_like\n",
    "        Array containing numbers who summary metrics is desired.\n",
    "    Returns\n",
    "    -------\n",
    "    metrics : dict\n",
    "        Dict containing the mean, median, max, min, range, standard deviation,\n",
    "        and variance of the input array.\n",
    "    \"\"\"\n",
    "    summary_object = {}\n",
    "    array = np.array(array)\n",
    "\n",
    "    summary_object[\"mean\"] = np.mean(array)\n",
    "    summary_object[\"median\"] = np.median(array)\n",
    "    summary_object[\"max\"] = np.amax(array)\n",
    "    summary_object[\"min\"] = np.amin(array)\n",
    "    summary_object[\"range\"] = np.mean(array)\n",
    "    summary_object[\"stddev\"] = np.std(array)\n",
    "    summary_object[\"variance\"] = np.var(array)\n",
    "\n",
    "    return summary_object\n",
    "\n",
    "\n",
    "def munkres_assignment(workers, jobs):\n",
    "    \"\"\"\n",
    "    Perform hungarian-munkres assignment.\n",
    "    Parameters\n",
    "    ----------\n",
    "    workers : array_like\n",
    "        Array containing the first set of points.\n",
    "    jobs : array_like\n",
    "        Array containing the second set of points.\n",
    "    Returns\n",
    "    -------\n",
    "    cost_matrix : numpy.ndarray\n",
    "        Matrix containing pairwise distances (the cost of assignment).\n",
    "    row_ind : numpy.ndarray\n",
    "        Row indices of cost_matrix for optimal assignment.\n",
    "    col_ind : numpy.ndarray\n",
    "        Column indices of cost_matrx for optimal assignment.\n",
    "    \"\"\"\n",
    "    cost_matrix = spatial.distance.cdist(workers, jobs, \"euclidean\")\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    return cost_matrix, row_ind, col_ind\n",
    "\n",
    "\n",
    "def make_isotropic(xyz, correction, dimen=2):\n",
    "    \"\"\"\n",
    "    Correct anisotropy in a collection of x, y, z coordinates.\n",
    "    This function performs a deepcopy of xyz before making the necessary modifications.\n",
    "    Parameters\n",
    "    ----------\n",
    "    xyz : numpy.ndarray or pandas.DataFrame:\n",
    "        The coordinate values.\n",
    "    correction : float\n",
    "        The value to correct anisotrophy.\n",
    "    dimen : int\n",
    "        Index of last dimension to which to apply the correction. Default is 2.\n",
    "    Returns\n",
    "    -------\n",
    "    iso_xyz : numpy.ndarray or pandas.DataFrame\n",
    "        An isotropic version of xyz.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(xyz, np.ndarray) and not isinstance(xyz, pd.DataFrame):\n",
    "        xyz = np.asarray(xyz)\n",
    "\n",
    "    shape = np.shape(xyz)\n",
    "    isotropic_xyz = xyz.copy()\n",
    "    size = shape[-1]\n",
    "    if dimen >= size or dimen < 0:\n",
    "        raise ValueError(\"improper dimen value (valid: 0 through {})\".format(size - 1))\n",
    "    if isinstance(xyz, np.ndarray):\n",
    "        isotropic_xyz[..., dimen] = isotropic_xyz[..., dimen] * correction\n",
    "    else:\n",
    "        isotropic_xyz.iloc[:, dimen] = isotropic_xyz.iloc[:, dimen] * correction\n",
    "    return isotropic_xyz\n",
    "\n",
    "\n",
    "# def get_range(center, pixel_size, radius):\n",
    "#     \"\"\"\n",
    "#     <Description here>\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     center : <type>\n",
    "#         <Description>\n",
    "#     pixel_size : <type>\n",
    "#         <Description>\n",
    "#     radius : <type>\n",
    "#         <Description>\n",
    "    \n",
    "#     Returns\n",
    "#     -------\n",
    "#     range : numpy.ndarray\n",
    "#         <Description>\n",
    "#     \"\"\"\n",
    "#     return np.asarray(\n",
    "#         [int(center - (radius / pixel_size)), int(center + (radius / pixel_size))],\n",
    "#         dtype=\"int\","
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to complie results!!!! \n",
    "\n",
    "add to dataframe all of the precision and recallls \n",
    "plot the results and check for \n",
    "params don't have to be the best \n",
    "\n",
    "\n",
    "make a sepereate dataframe \n",
    "\n",
    "look: pd.concat\n",
    "\n",
    "mention the params \n",
    "\n",
    "or have a collumn that has run number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Confirms synapse processing and analysis functions.\n",
    "\"\"\"\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# from . import utils\n",
    "# import utils\n",
    "\n",
    "SynapseMetrics = namedtuple(\n",
    "    \"SynapseMetrics\",\n",
    "    [\"precision\", \"recall\", \"f1\", \"tp_gt_ids\", \"tp_test_ids\", \"fp_ids\", \"fn_ids\"],\n",
    ")\n",
    "\n",
    "\n",
    "def filter_synapse_id_core(volume, xyz, ids, box_radius_nm=2500):\n",
    "    \"\"\"\n",
    "    Filter synapses to only return those within a central core.\n",
    "    Parameters\n",
    "    ----------\n",
    "    volume : dict\n",
    "        The volume to filter on.\n",
    "    xyz : array_like\n",
    "        Synapse xyz coordinates.\n",
    "    ids : array_like\n",
    "        Synapse hash (ids).\n",
    "    box_radius_nm : int\n",
    "        radius of cube, from volume core.\n",
    "    Returns\n",
    "    -------\n",
    "        xyz : numpy.ndarray\n",
    "            Synapse coordinates.\n",
    "        ids : numpy.ndarray\n",
    "            Synapse ids.\n",
    "    \"\"\"\n",
    "\n",
    "    xyz_out = []\n",
    "    id_out = []\n",
    "    center = np.asarray(volume[\"center\"], \"float\")\n",
    "    base_resolution = np.asarray(volume[\"base_resolution\"], \"float\")\n",
    "    annotation_resolution = np.asarray(volume[\"resolution\"], \"float\")\n",
    "\n",
    "    pad_vx = box_radius_nm / base_resolution[0] / (2 ** annotation_resolution)\n",
    "    pad_vy = box_radius_nm / base_resolution[1] / (2 ** annotation_resolution)\n",
    "    pad_vz = box_radius_nm / base_resolution[2]\n",
    "    xr = (center[0] - pad_vx, center[0] + pad_vx)\n",
    "    yr = (center[1] - pad_vy, center[1] + pad_vy)\n",
    "    zr = (center[2] - pad_vz, center[2] + pad_vz)\n",
    "\n",
    "    for i in range(len(xyz)):\n",
    "        x = xyz[i][0]\n",
    "        y = xyz[i][1]\n",
    "        z = xyz[i][2]\n",
    "\n",
    "        if (\n",
    "            x > xr[0]\n",
    "            and x < xr[1]\n",
    "            and y > yr[0]\n",
    "            and y < yr[1]\n",
    "            and z > zr[0]\n",
    "            and z < zr[1]\n",
    "        ):\n",
    "            xyz_out.append(xyz[i])\n",
    "            id_out.append(ids[i])\n",
    "\n",
    "    return np.array(xyz_out), np.array(id_out, dtype=np.object)\n",
    "\n",
    "\n",
    "def synapse_match(\n",
    "    xyz_truth, xyz_detect, id_truth, id_detect, thresh\n",
    "):  # pylint: disable=R0914\n",
    "    \"\"\"\n",
    "    <Description here>\n",
    "    Parameters\n",
    "    ----------\n",
    "    xyz_truth : array_like\n",
    "        <description>\n",
    "    xyz_detect : array_like\n",
    "        <description>\n",
    "    id_truth : array_like\n",
    "        <description>\n",
    "    id_detech : array_like\n",
    "        <description>\n",
    "    thresh : float\n",
    "        <description>\n",
    "    Returns\n",
    "    -------\n",
    "    id_lookup : <type>\n",
    "        <description>\n",
    "    \"\"\"\n",
    "\n",
    "    # pylint: disable=C0103\n",
    "\n",
    "    # Ensure we have numpy arrays\n",
    "    xyz_truth = np.asarray(xyz_truth)\n",
    "    xyz_detect = np.asarray(xyz_detect)\n",
    "    id_truth = np.asarray(id_truth)\n",
    "    id_detect = np.asarray(id_detect)\n",
    "\n",
    "    cost, row_ind, col_ind = munkres_assignment(xyz_truth, xyz_detect)\n",
    "    match_idx = np.where(cost[row_ind, col_ind] < thresh)\n",
    "\n",
    "    if len(match_idx) > 0:\n",
    "        # row is idx of GT TP\n",
    "        # col is idx of student TP\n",
    "        gt_tp_idx, det_tp_idx = row_ind[match_idx], col_ind[match_idx]\n",
    "        gt_tp_ids, det_tp_ids = id_truth[gt_tp_idx], id_detect[det_tp_idx]\n",
    "        # Combine into pairs\n",
    "        id_lookup = np.column_stack((gt_tp_ids, det_tp_ids))\n",
    "    else:\n",
    "        gt_tp_idx = []\n",
    "        det_tp_idx = []\n",
    "        id_lookup = np.column_stack(\n",
    "            (np.array([], dtype=\"object\"), np.array([], dtype=\"object\"))\n",
    "        )\n",
    "\n",
    "    # not in row (set diff) are FN\n",
    "    gt_syn_idx = np.arange(0, len(xyz_truth))\n",
    "    fn_idx = np.setdiff1d(gt_syn_idx, gt_tp_idx)\n",
    "    if len(fn_idx) > 0:\n",
    "        fn_ids = id_truth[fn_idx]\n",
    "        id_lookup_fn = np.column_stack((fn_ids, np.repeat(None, len(fn_ids))))\n",
    "        id_lookup = np.concatenate((id_lookup, id_lookup_fn))\n",
    "\n",
    "    # not in col (set diff) are FP\n",
    "    det_syn_idx = set(np.arange(0, len(xyz_detect)))\n",
    "    fp_idx = np.asarray(list(det_syn_idx.difference(det_tp_idx)))\n",
    "    if len(fp_idx) > 0:\n",
    "        fp_ids = id_detect[fp_idx]\n",
    "        id_lookup_fp = np.column_stack((np.repeat(None, len(fp_ids)), fp_ids))\n",
    "        id_lookup = np.concatenate((id_lookup, id_lookup_fp))\n",
    "\n",
    "    return pd.DataFrame(id_lookup, columns=[\"ground_truth\", \"detect\"])\n",
    "\n",
    "\n",
    "def run_synapse_analysis(\n",
    "    gt_xyzs,\n",
    "    gt_ids,\n",
    "    test_xyzs,\n",
    "    test_ids,\n",
    "    threshold,\n",
    "    iso_corrected=False,\n",
    "    iso_correction=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    <Description here>\n",
    "    Parameters\n",
    "    ----------\n",
    "    gt_xyzs : numpy.ndarray\n",
    "        Array of ground truth xyz coordinates\n",
    "    gt_ids : numpy.ndarray\n",
    "        Array of ids associated with ground truth xyz coordinates\n",
    "    test_xyzs : numpy.ndarray\n",
    "        Array of test xyz coordinates\n",
    "    test_ids : numpy.ndarray\n",
    "        Array of ids associated with test xyz coordinates\n",
    "    threshold : float\n",
    "        Synapse matching threshold\n",
    "    iso_corrected : boolean\n",
    "        Mark whether the data is isotropic. If not, it will be made isotropic using the\n",
    "        `iso_correction` parameter.\n",
    "    iso_correction : float\n",
    "        Value to correct anistropy.\n",
    "    Returns\n",
    "    -------\n",
    "    sm : SynapseMetrics\n",
    "        Resultant object containing precision, recall, and F1 scores, along with\n",
    "        with true positive (both ground truth and test), false positive, and false negative\n",
    "        ids.\n",
    "    \"\"\"\n",
    "    # pylint: disable=R0913,R0914\n",
    "    if not iso_corrected:\n",
    "        # gt_xyzs = utils.make_isotropic(gt_xyzs, iso_correction)\n",
    "        gt_xyzs = make_isotropic(gt_xyzs, iso_correction)\n",
    "        # test_xyzs = utils.make_isotropic(test_xyzs, iso_correction)\n",
    "        test_xyzs = make_isotropic(test_xyzs, iso_correction)\n",
    "\n",
    "    results_table = synapse_match(gt_xyzs, test_xyzs, gt_ids, test_ids, threshold)\n",
    "\n",
    "    tp = results_table.dropna()\n",
    "    fn = results_table[results_table.detect.isnull()]\n",
    "    fp = results_table[results_table.ground_truth.isnull()]\n",
    "\n",
    "    assert len(tp.ground_truth) == len(\n",
    "        tp.detect\n",
    "    ), \"true positive ground truth and test size mismatch\"\n",
    "\n",
    "    tp_count = len(tp)\n",
    "    fp_count = len(fp)\n",
    "    fn_count = len(fn)\n",
    "\n",
    "    try:\n",
    "        # precision, recall = utils.calculate_precision_recall(\n",
    "        #     tp_count, fp_count, fn_count\n",
    "        # )\n",
    "\n",
    "        precision, recall = calculate_precision_recall(\n",
    "            tp_count, fp_count, fn_count\n",
    "        )\n",
    "\n",
    "    except ZeroDivisionError:\n",
    "        precision, recall = np.nan, np.nan\n",
    "\n",
    "    try:\n",
    "        # f1 = utils.calculate_f1(precision, recall)\n",
    "        f1 = calculate_f1(precision, recall)\n",
    "    except ZeroDivisionError:\n",
    "        f1 = np.nan\n",
    "\n",
    "    sm = SynapseMetrics(\n",
    "        precision=precision,\n",
    "        recall=recall,\n",
    "        f1=f1,\n",
    "        tp_gt_ids=np.asarray(tp.ground_truth),\n",
    "        tp_test_ids=np.asarray(tp.detect),\n",
    "        fp_ids=np.asarray(fp.detect),\n",
    "        fn_ids=np.asarray(fn.ground_truth),\n",
    "    )\n",
    "    return sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_xyzs = np.array([[167695,211664,20725],\n",
    "                    [145921,225039,22976]])\n",
    "\n",
    "gt_ids = np.array([864691135537360703, 864691135537360703])\n",
    "\n",
    "test_xyzs = np.array([[145718,224993,22957],\n",
    "                      [147843,224023,22885],\n",
    "                      [152766,222078,22418],\n",
    "                      [166663,211491,20783]])\n",
    "\n",
    "test_ids = np.array(\n",
    "    [864691135537360703, 864691135537360703, 864691135537360703, 864691135537360703])\n",
    "\n",
    "threshold = 500\n",
    "\n",
    "analysis = run_synapse_analysis(\n",
    "    gt_xyzs,\n",
    "    gt_ids,\n",
    "    test_xyzs,\n",
    "    test_ids,\n",
    "    threshold,\n",
    "    iso_correction=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRECISION: #test at ground truth / # test  \n",
    "\n",
    "RECALL: # gt recovered / number ground truth \n",
    "(would be 100% if you found all the correct endponts...NO EXTRAS)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
