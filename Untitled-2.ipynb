{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections.abc import Iterable\n",
    "from agents import data_loader\n",
    "from caveclient import CAVEclient\n",
    "from cloudvolume import CloudVolume, VolumeCutout\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from orphan_extension.utils.cast_to_bounds import cast_points_within_bounds\n",
    "from orphan_extension.utils.multi_loader import multi_proc_type, multi_soma_count\n",
    "from tip_finding import tip_finding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrphanError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Orphans:\n",
    "    def __init__(self, bounds_list: list):\n",
    "        self.bounds_list = bounds_list\n",
    "\n",
    "    # def __init__(self, x_min, x_max, y_min, y_max, z_min, z_max):\n",
    "    #     self.x_min = x_min\n",
    "    #     self.x_max = x_max\n",
    "    #     self.y_min = y_min\n",
    "    #     self.y_max = y_max\n",
    "    #     self.z_min = z_min\n",
    "    #     self.z_max = z_max\n",
    "\n",
    "    def get_unique_seg_ids_em(self, coords=None) -> list:\n",
    "        \"\"\"\n",
    "        Gets all the seg ids within a given subvolume and organizes by size of process. Returns list of tuples: (seg_id, size)\n",
    "        \"\"\"\n",
    "        if (coords != None and len(coords) != 6):  # CHANGE THE ERROR THROWN!\n",
    "            raise OrphanError(\"get_unique_seg_ids_em needs 6 coordinates!!\")\n",
    "\n",
    "        x_min, x_max, y_min, y_max, z_min, z_max = coords if coords else self.bounds_list\n",
    "\n",
    "        seg_ids_sv = data_loader.get_seg(x_min, x_max, y_min,\n",
    "                                         y_max, z_min, z_max)\n",
    "        # Get entire EM data - uncomment after testing\n",
    "        # em = CloudVolume('s3://bossdb-open-data/iarpa_microns/minnie/minnie65/seg', use_https=True, mip=0, parallel=True, fill_missing=True, progress=True)\n",
    "\n",
    "        # Get seg ids in the specified subvolume\n",
    "\n",
    "        # Get rid of the 4th dimension since its magnitude is 1\n",
    "        seg_ids_sv = np.squeeze(seg_ids_sv)\n",
    "\n",
    "        # List of all unique seg ids in the 3d subvolume\n",
    "        unique_seg_ids_sv = np.unique(seg_ids_sv)\n",
    "\n",
    "        # Removing the first element (artifacts) of unique_seg_ids_sv\n",
    "        \"\"\"\n",
    "        Drop all zeros in array instead of only first, handles edge cases\n",
    "        unique_seg_ids_sv = np.delete(unique_seg_ids_sv, 0)\n",
    "        \"\"\"\n",
    "\n",
    "        unique_seg_ids_sv = unique_seg_ids_sv[unique_seg_ids_sv != 0]\n",
    "        unique_seg_ids_sv = unique_seg_ids_sv[unique_seg_ids_sv != None]\n",
    "\n",
    "        # Organizing seg ids in subvolume by size\n",
    "        seg_ids_by_size = {}\n",
    "        for seg_id in (pbar := tqdm(unique_seg_ids_sv)):\n",
    "            pbar.set_description('Organizing seg_ids by size')\n",
    "            # seg_ids_by_size[seg_id] = int(em[em == seg_id].sum()) # Uncomment after testing to organize seg ids by size considering whole data\n",
    "            seg_ids_by_size[seg_id] = [int(\n",
    "                np.sum(seg_ids_sv == seg_id))]\n",
    "\n",
    "        seg_ids_by_size = sorted(seg_ids_by_size.items(),\n",
    "                                 key=lambda x: x[1], reverse=True)\n",
    "        # Returns a list of tuples with first element as seg id, second elmenet of tuple is a list containing size\n",
    "        return seg_ids_by_size  # Sorted in descending order\n",
    "\n",
    "    def get_orphans(self, coords=None) -> dict:\n",
    "        \"\"\"\n",
    "        Get the list of orphans within a given subvolume organized by largest orphan in subvolume first\n",
    "        \"\"\"\n",
    "        unique_seg_ids = self.get_unique_seg_ids_em(coords)\n",
    "        unique_seg_ids_l = [i[0] for i in unique_seg_ids]\n",
    "\n",
    "        # Getting all the orphans\n",
    "        seg_soma_count = multi_soma_count(unique_seg_ids_l)\n",
    "        # orphans = list({k:v for k, v in orphans.items() if v==0}.keys())\n",
    "\n",
    "        orphan_dict = {}\n",
    "\n",
    "        for seg_id_and_size in (pbar := tqdm(unique_seg_ids)):\n",
    "            pbar.set_description('Labeling orphans')\n",
    "\n",
    "            if (seg_soma_count[seg_id_and_size[0]] == 0):\n",
    "                orphan_dict[seg_id_and_size[0]] = seg_id_and_size[1]\n",
    "\n",
    "            # if seg_id_and_size[0] in orphans:\n",
    "            #     orphan_dict[seg_id_and_size[0]] = seg_id_and_size[1]\n",
    "\n",
    "        return orphan_dict, seg_soma_count  # dict of seg_ids that are orphans in given subvolume\n",
    "\n",
    "    def get_process_type(self, processes: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Input: processes is a dictionary with key = seg_id, value = list of attributes\n",
    "        Returns: updated processes so that value also includes the type of the process\n",
    "        \"\"\"\n",
    "        proc_dict = multi_proc_type(list(processes.keys()))\n",
    "        for seg_id, attributes in (pbar := tqdm(processes.items())):\n",
    "            pbar.set_description('Adding process type')\n",
    "            attributes.append(proc_dict[seg_id])\n",
    "\n",
    "        return processes\n",
    "\n",
    "    def get_pot_extensions(self, endpoint_coords):\n",
    "\n",
    "        if (len(endpoint_coords) != 3):\n",
    "            # FIX THIS - SHOULD BE DIFF TYPE OF ERROR\n",
    "            raise OrphanError(\n",
    "                \"get_pot_extension needs all 3 coordinates of endpoint to extend!\")\n",
    "\n",
    "        # Get the coordinates of the bounding box around the endpoint\n",
    "        endpoint_bounding_box_coords = bounding_box_coords(endpoint_coords)\n",
    "\n",
    "        # Get a preliminary list of all seg ids within bounding box\n",
    "        # pot_ex = self.get_unique_seg_ids_em(*endpoint_bounding_box_coords)\n",
    "        pot_ex = self.get_unique_seg_ids_em(endpoint_bounding_box_coords)\n",
    "\n",
    "        # Get seg id of current fragment\n",
    "        curr_process_seg_id = data_loader.get_seg(\n",
    "            endpoint_coords[0], endpoint_coords[0], endpoint_coords[1], endpoint_coords[1], endpoint_coords[2], endpoint_coords[2])\n",
    "\n",
    "        # Get type of all processes\n",
    "        pot_ex = dict(pot_ex)\n",
    "        pot_ex = self.get_process_type(pot_ex)\n",
    "\n",
    "        # Get type of current process\n",
    "        curr_process_type = pot_ex[curr_process_seg_id][1]\n",
    "\n",
    "        # Remove current seg id from the list of potential extensions - REWORK TO USE DELETE\n",
    "        # pot_ex = pot_ex[str(pot_ex) != str(curr_process_seg_id)]\n",
    "        del pot_ex[curr_process_seg_id]\n",
    "\n",
    "        # Filter out all other processes whose type!= current process type\n",
    "        pot_ex = remove_diff_types(curr_process_type, pot_ex)\n",
    "\n",
    "        return pot_ex  # Return all potential extensions after removing confirmed other types\n",
    "\n",
    "\n",
    "def bounding_box_coords(point: Iterable, boxrad: Iterable = [100, 100, 10]) -> list:\n",
    "    # Data bounds not validated\n",
    "    data_bounds = [26000, 220608, 30304, 161376, 14825, 27881]\n",
    "\n",
    "    # Confirm that entry is 3dim\n",
    "    if len(point) != 3:\n",
    "        raise OrphanError(\n",
    "            \"Point passed to func bounding_box_coords() must be an iterable of length 3.\")\n",
    "    if len(boxrad) != 3:\n",
    "        raise OrphanError(\n",
    "            \"Box dimensions passed to func bounding_box_coords() must be 3 dimensional\")\n",
    "\n",
    "    # Check bound validity and cast to new bounds\n",
    "    casted_bounds = cast_points_within_bounds(point, data_bounds, boxrad)\n",
    "\n",
    "    return casted_bounds\n",
    "\n",
    "\n",
    "def remove_diff_types(process_type, pot_ex):\n",
    "    for seg_id, attributes in pot_ex.items():\n",
    "        if (process_type not in attributes or \"unconfirmed\" not in attributes):\n",
    "            del pot_ex[seg_id]\n",
    "\n",
    "    return pot_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shard Indices: 100%|██████████| 2/2 [00:00<00:00,  4.02it/s]\n",
      "Minishard Indices: 100%|██████████| 2/2 [00:00<00:00,  5.42it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 32.50it/s]\n",
      "Decompressing: 100%|██████████| 18/18 [00:00<00:00, 4304.06it/s]\n",
      "Organizing seg_ids by size: 100%|██████████| 39/39 [00:00<00:00, 271.70it/s]\n",
      "Labeling orphans: 100%|██████████| 39/39 [00:00<00:00, 392.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of orphans: 36\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{864691135631953092: 1,\n",
       " 864691135615954025: 1,\n",
       " 864691135715218970: 1,\n",
       " 864691136909215598: 0,\n",
       " 864691135521264882: 0,\n",
       " 864691135368930546: 0,\n",
       " 864691135918483376: 0,\n",
       " 864691135804594461: 0,\n",
       " 864691136914365806: 0,\n",
       " 864691135395943378: 0,\n",
       " 864691135478343235: 0,\n",
       " 864691135648168388: 0,\n",
       " 864691136000419720: 0,\n",
       " 864691135449037042: 0,\n",
       " 864691136181973462: 0,\n",
       " 864691135582201586: 0,\n",
       " 864691133035107425: 0,\n",
       " 864691136913731182: 0,\n",
       " 864691135407650258: 0,\n",
       " 864691135968211902: 0,\n",
       " 864691132647778343: 0,\n",
       " 864691135401901778: 0,\n",
       " 864691135104027483: 0,\n",
       " 864691133716301031: 0,\n",
       " 864691132647775271: 0,\n",
       " 864691132647776807: 0,\n",
       " 864691135648134852: 0,\n",
       " 864691135407637970: 0,\n",
       " 864691133035106657: 0,\n",
       " 864691135648947396: 0,\n",
       " 864691133456842377: 0,\n",
       " 864691132647776295: 0,\n",
       " 864691133716301287: 0,\n",
       " 864691133035107169: 0,\n",
       " 864691132647777575: 0,\n",
       " 864691135804092189: 0,\n",
       " 864691135793272349: 0,\n",
       " 864691133456842633: 0,\n",
       " 864691132647777063: 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds = bounding_box_coords([115267, 91839, 21305], boxrad=[100, 100, 10])\n",
    "orphanclass = Orphans(bounds)\n",
    "orphans, seg_soma_count = orphanclass.get_orphans()\n",
    "print(\"Number of orphans:\", len(orphans))\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "seg_soma_count\n",
    "# proc_types = orphanclass.get_process_type(orphans)\n",
    "# print(proc_types)\n",
    "\n",
    "# print()\n",
    "# total_size = orphans.keys()\n",
    "# total_size = list(total_size)\n",
    "# print(total_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
